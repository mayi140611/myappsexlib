{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp math.func.loss\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss function | cost function | objective function\n",
    "这些概念的定义有一些模糊，一般情况下：\n",
    "\n",
    "## loss function\n",
    "衡量的是在单个样本上的表现；\n",
    "\n",
    "## cost function | objective function\n",
    "cost function等同于objective function  \n",
    "衡量的是在全体样本上的表现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回归问题常见的loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SquareLoss\n",
    "$$l(y,\\hat y)=0.5*(y-\\hat y)^2$$\n",
    "\n",
    "### 一般用于回归问题。\n",
    "\n",
    "举例:\n",
    "对于3个样本, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def square_loss(y_true, y_hat):\n",
    "    return 0.5 * np.power((y_true - y_hat), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([0.2,0.6,0.8])\n",
    "y_hat = np.array([0.18, 0.5, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0002, 0.005 , 0.005 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square_loss(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对应的cost function\n",
    "假设它们属于一个批次，那么这个批次的mean_square_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0034000000000000024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_square_loss = np.sum(square_loss(y, y_hat))/len(y)\n",
    "mean_square_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类问题常见的loss function\n",
    "分类问题的分类一般用one-hot，假设  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [(1, 0, 0), (0, 1, 0), (0, 0, 1),]\n",
    "# 一般是通过softmax输出\n",
    "y_hat = [(0.8, 0.1, 0.1), (0.3, 0.4, 0.3), (0.1, 0.4, 0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何来定义loss？是否可以用SquareLoss？如果非要使用SquareLoss的话，有两种方案：\n",
    "1. 把y使用np.argmax还原成数值编码[0, 1, 2]  \n",
    "对于上述而言例子而言，loss结果是0，完美  \n",
    "\n",
    "但是假设y_hat的数值编码是[0, 1, 0]和另一种[0, 1, 1]的loss应该是一样的(都预测错1个)，但是使用SquareLoss的话，[0, 1, 0]的loss大于[0, 1, 1]的loss。所以，这种方案不合适\n",
    "\n",
    "1. 第二种方案是\n",
    "如计算第一个样本的$loss=0.5*((1-0.8)^2+(0-0.1)^2+(0-0.1)^2)/3$  \n",
    "这样做有显得太苛刻了(__from李沐__)，必须要使得y_hat和y完全相同，loss才等于0。而实际上，例子中的结果的loss就应该是0！\n",
    "\n",
    "__综上，使用SquareLoss作为分类问题的loss并不合适！__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogLoss\n",
    "https://medium.com/datadriveninvestor/understanding-the-log-loss-function-of-xgboost-8842e99d975d\n",
    "\n",
    "Log loss, short for logarithmic loss is a loss function for classification that quantifies the price paid for the inaccuracy of predictions in classification problems. Log loss penalizes false classifications by taking into account the probability of classification. \n",
    "### LogLossBinary\n",
    "\n",
    "主要用作2分类问题的损失函数，如作为逻辑回归的loss function。\n",
    "$$l(y,\\hat y)=-yln(\\hat y)-(1-y)ln(1-\\hat y)$$\n",
    "\n",
    "其中$y和\\hat y都是标量。\\hat y$表示预测为1的概率，那么$1-\\hat y$就是预测为0的概率。\n",
    "\n",
    "因为y的取值只有0或1，那么实际中表示式的两部分只有一部分会生效：当y=1时，前半部分生效;当y=0时，后半部分生效\n",
    "\n",
    "上式可以拆解为条件表达式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_cond(actual, predict_prob):\n",
    "    if actual == 1:  \n",
    "        # use natural logarithm\n",
    "        return -log(predict_prob) \n",
    "    else:\n",
    "        return -log(1 - predict_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y, p):\n",
    "    # Avoid division by zero\n",
    "    p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "    return - y * np.log(p) - (1 - y) * np.log(1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(0, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEntropyLoss(交叉熵损失)\n",
    "Cross-entropy is the more generic form of logarithmic loss when it comes to machine learning algorithms. \n",
    "\n",
    "While log loss is used for binary classification algorithms, cross-entropy serves the same purpose for multiclass classification problems. In other words, log loss is used when there are 2 possible outcomes and cross-entropy is used when there are more than 2 possible outcomes.\n",
    "\n",
    "主要用作多分类问题的损失函数。  \n",
    "\n",
    "logistic变换，其中x为标量\n",
    "$$sigmoid(x)=\\frac{1}{1+exp(x)}$$\n",
    "扩展到多分类的\n",
    "$$softmax(\\vec x)=\\frac{exp(\\vec x)}{\\sum exp(\\vec x)}$$\n",
    "LogLoss对应到多分类的结果为CrossEntropyLoss  \n",
    "$$l(\\vec y, \\vec{\\hat y})=-\\sum_{i=1}^n{y_iln(\\hat y_i)}$$\n",
    "\n",
    "理解：因为采用one-hot, $\\vec y$中只有一个为1，\n",
    "* 假设$y_i=1$，如果对应的预测值$\\hat y_i$为1，那么loss值为0；\n",
    "* 如果对应的预测值$\\hat y_i$接近0，那么loss值接近正无穷大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_hat):\n",
    "    \"\"\"\n",
    "    multi分类loss function. 每一个样本的预测值用一个one-hot vector表示\n",
    "    :param y_true: np.array([(1, 0, 0), (1, 0, 0), (1, 0, 0)])\n",
    "    :param y_hat: np.array([(0.18, 0.5, 0.7), (0.18, 0.5, 0.7),(0.18, 0.5, 0.7),])\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Avoid division by zero\n",
    "    y_hat = np.clip(y_hat, 1e-15, 1 - 1e-15)\n",
    "    # 注意计算是由于-y_true * np.log(y_hat) element-wise，\n",
    "    # 每一行只有一个值非0，所以按照cross_entropy_loss有两个\\sum，可以合并成1个\n",
    "    return np.sum(-y_true * np.log(y_hat))/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([(1, 0, 0), (1, 0, 0), (1, 0, 0)])\n",
    "y_hat = np.array([(0.48, 0.51, 0.01), (0.48, 0.51, 0.01),(0.48, 0.51, 0.01),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7339691750802005"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nb_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted engineering_nbdev.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No notebooks were modified\r\n",
      "converting /Users/luoyonggui/PycharmProjects/nbdevlib/index.ipynb to README.md\r\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "277.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
