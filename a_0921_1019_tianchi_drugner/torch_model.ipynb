{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思路\n",
    "先把transformers官方版本复现一遍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "from loguru import logger\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from filelock import FileLock\n",
    "from code.config import args\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "# from transformers import PreTrainedTokenizer#, is_torch_available\n",
    "\n",
    "from transformers import Trainer, AutoModelForTokenClassification, AutoConfig\n",
    "from transformers.modeling_bert import BertPreTrainedModel, BertModel\n",
    "from datetime import datetime\n",
    "\n",
    "from seqeval.metrics import f1_score, precision_score, accuracy_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 250\n",
    "\n",
    "# model_name_or_path = 'bert-base-cased'\n",
    "model_name_or_path = 'clue/roberta_chinese_base'\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InputExample | InputFeatures | Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InputExample:\n",
    "    \"\"\"\n",
    "    A single training/test example for token classification.\n",
    "    Args:\n",
    "        guid: Unique id for the example.\n",
    "        words: list. The words of the sequence.\n",
    "        labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "\n",
    "    guid: str\n",
    "    words: List[str]\n",
    "    labels: Optional[List[str]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InputFeatures:\n",
    "    \"\"\"\n",
    "    A single set of features of data.\n",
    "    Property names are the same names as the corresponding inputs to a model.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids: List[int]\n",
    "    attention_mask: List[int]\n",
    "    token_type_ids: Optional[List[int]] = None\n",
    "    label_ids: Optional[List[int]] = None\n",
    "\n",
    "\n",
    "class Split(Enum):\n",
    "    train = \"train\"\n",
    "    dev = \"val\"\n",
    "    test = \"test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_examples_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples_from_file(fp) -> List[InputExample]:\n",
    "    \"\"\"\n",
    "    :fp: corpus路径\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(fp)\n",
    "    guid_index = 1\n",
    "    examples = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                if words:\n",
    "                    examples.append(InputExample(guid=f\"{guid_index}\", words=words, labels=labels))\n",
    "                    guid_index += 1\n",
    "                    words = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                splits = line.split(\" \")\n",
    "                words.append(splits[0])\n",
    "                if len(splits) > 1:\n",
    "                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
    "                else:\n",
    "                    # Examples could have no label for mode = \"test\"\n",
    "                    labels.append(\"O\")\n",
    "        if words:\n",
    "            examples.append(InputExample(guid=f\"{mode}-{guid_index}\", words=words, labels=labels))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InputExample(guid='1', words=['7', '.', '服', '药', '1', '个', '月', '症', '状', '无', '缓', '解', '，', '应', '去', '医', '院', '就', '诊', '。', '8', '.', '对', '本', '品', '过', '敏', '者', '禁', '用', '，', '过', '敏', '体', '质', '者', '慎', '用', '。', '9', '.', '本', '品', '性', '状', '发', '生', '改', '变', '时', '禁', '止', '使', '用', '。', '1', '0', '.', '请', '将', '本', '品', '放', '在', '儿', '童', '不', '能', '接', '触', '的', '地', '方', '。', '1', '1', '.', '如', '正', '在', '使', '用', '其', '他', '药', '品', '，', '使', '用', '本', '品', '前', '请', '咨', '询', '医', '师', '或', '药', '师', '。', '丸', '剂', '(', '水', '蜜', '丸', ')', '镀', '铝', '复', '合', '膜', '，', '每', '袋', '装', '6', '克', '，', '每', '盒', '装', '1', '0', '袋', '。', '补', '气', '养', '血', '，', '调', '经', '止', '带', '。', '用', '于', '气', '血', '两', '虚', '，', '身', '体', '瘦', '弱', '，', '腰', '膝', '酸', '软', '，', '月', '经', '量', '少', '、', '后', '错', '，', '带', '下', '补', '气', '养', '血', '，', '调', '经', '止', '带', '6', 'g', '*', '1', '0', '袋', '孕', '妇', '禁', '用', '。', '补', '气', '养', '血', '、', '调', '经', '止', '带', '，', '用', '于', '月', '经', '不', '调', '、', '经', '期', '腹', '痛', '尚', '不', '明', '确', '。'], labels=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'O']),\n",
       " InputExample(guid='2', words=['尚', '不', '明', '确', '。', '云', '南', '龙', '海', '天', '然', '植', '物', '药', '业', '有', '限', '公', '司', '尚', '不', '明', '确', '。', '本', '品', '为', '胶', '囊', '剂', '，', '内', '容', '物', '为', '黄', '褐', '色', '至', '棕', '褐', '色', '颗', '粒', '及', '粉', '末', '，', '气', '微', '，', '味', '涩', '，', '微', '苦', '。', '每', '粒', '装', '0', '.', '4', 'g', '。', '铝', '塑', '，', '1', '0', '粒', '/', '板', '*', '3', '板', '/', '盒', '。', '孕', '妇', '忌', '服', '。', '彝', '医', '：', '补', '知', '凯', '扎', '诺', '，', '且', '凯', '色', '土', '。', '<', 'b', 'r', '/', '>', '中', '医', '：', '益', '肾', '。', '活', '血', '，', '软', '坚', '散', '结', '。', '用', '于', '肾', '阳', '不', '足', '、', '气', '滞', '血', '瘀', '所', '致', '的', '乳', '腺', '增', '生', '益', '肾', '。', '活', '血', '，', '软', '坚', '散', '结', '。', '用', '于', '肾', '阳', '不', '足', '、', '气', '滞', '血', '瘀', '所', '致', '的', '乳', '腺', '增', '生', '。', '口', '服', '，', '一', '次', '3', '-', '5', '粒', '，', '一', '日', '3', '次', '；', '饭', '后', '服', '用', '。', '月', '经', '前', '1', '5', '天', '开', '始', '服', '，', '至', '月', '经', '来', '时', '停', '药', '。'], labels=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DRUG_DOSAGE', 'I-DRUG_DOSAGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DRUG_TASTE', 'I-DRUG_TASTE', 'O', 'B-DRUG_TASTE', 'I-DRUG_TASTE', 'O', 'B-DRUG_TASTE', 'I-DRUG_TASTE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'O', 'O', 'B-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'O', 'O', 'B-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'O', 'B-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = read_examples_from_file(f'{args.DATA_GEN}train_val.txt')\n",
    "examples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1957"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(path: str) -> List[str]:\n",
    "    if path:\n",
    "        with open(path, \"r\") as f:\n",
    "            labels = f.read().splitlines()\n",
    "        if \"O\" not in labels:\n",
    "            labels = [\"O\"] + labels\n",
    "        return labels\n",
    "    else:\n",
    "        return [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-DRUG',\n",
       " 'I-DRUG',\n",
       " 'B-DRUG_INGREDIENT',\n",
       " 'I-DRUG_INGREDIENT',\n",
       " 'B-DISEASE',\n",
       " 'I-DISEASE',\n",
       " 'B-SYMPTOM',\n",
       " 'I-SYMPTOM',\n",
       " 'B-SYNDROME',\n",
       " 'I-SYNDROME',\n",
       " 'B-DISEASE_GROUP',\n",
       " 'I-DISEASE_GROUP',\n",
       " 'B-FOOD',\n",
       " 'I-FOOD',\n",
       " 'B-FOOD_GROUP',\n",
       " 'I-FOOD_GROUP',\n",
       " 'B-PERSON_GROUP',\n",
       " 'I-PERSON_GROUP',\n",
       " 'B-DRUG_GROUP',\n",
       " 'I-DRUG_GROUP',\n",
       " 'B-DRUG_DOSAGE',\n",
       " 'I-DRUG_DOSAGE',\n",
       " 'B-DRUG_TASTE',\n",
       " 'I-DRUG_TASTE',\n",
       " 'B-DRUG_EFFICACY',\n",
       " 'I-DRUG_EFFICACY']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = get_labels(f'{args.DATA_GEN}torch_ner_data/labels.txt')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {i: label for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(\n",
    "    examples: List[InputExample],\n",
    "    label_list: List[str],\n",
    "    max_seq_length: int,\n",
    "    tokenizer,#: PreTrainedTokenizer,\n",
    "    cls_token_at_end=False,\n",
    "    cls_token=\"[CLS]\",\n",
    "    cls_token_segment_id=0,\n",
    "    sep_token=\"[SEP]\",\n",
    "    sep_token_extra=False,\n",
    "    pad_on_left=False,\n",
    "    pad_token=0,\n",
    "    pad_token_segment_id=0,\n",
    "    pad_token_label_id=-100,\n",
    "    sequence_a_segment_id=0,\n",
    "    mask_padding_with_zero=True,\n",
    ") -> List[InputFeatures]:\n",
    "    \"\"\" Loads a data file into a list of `InputFeatures`\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "    # TODO clean up all this to leverage built-in features of tokenizers\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10_000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n",
    "\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "            # bert-base-multilingual-cased sometimes output \"nothing ([]) when calling tokenize with just a space.\n",
    "            if len(word_tokens) > 0:\n",
    "                tokens.extend(word_tokens)\n",
    "                # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "                label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = tokenizer.num_special_tokens_to_add()\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids:   0   0   0   0  0     0   0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        if sep_token_extra:\n",
    "            # roberta uses an extra separator b/w pairs of sentences\n",
    "            tokens += [sep_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens += [cls_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "            segment_ids += [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            label_ids = [pad_token_label_id] + label_ids\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n",
    "        else:\n",
    "            input_ids += [pad_token] * padding_length\n",
    "            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
    "            segment_ids += [pad_token_segment_id] * padding_length\n",
    "            label_ids += [pad_token_label_id] * padding_length\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "#         if ex_index < 5:\n",
    "#             logger.info(\"*** Example ***\")\n",
    "#             logger.info(\"guid: %s\", example.guid)\n",
    "#             logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n",
    "#             logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
    "#             logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
    "#             logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
    "#             logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n",
    "\n",
    "        if \"token_type_ids\" not in tokenizer.model_input_names:\n",
    "            segment_ids = None\n",
    "\n",
    "            \n",
    "            \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=input_mask, token_type_ids=segment_ids, label_ids=label_ids\n",
    "            )\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-29 17:25:37.340 | INFO     | __main__:convert_examples_to_features:31 - Writing example %d of %d\n",
      "2020-09-29 17:25:37.346 | INFO     | __main__:convert_examples_to_features:110 - *** Example ***\n",
      "2020-09-29 17:25:37.346 | INFO     | __main__:convert_examples_to_features:111 - guid: %s\n",
      "2020-09-29 17:25:37.346 | INFO     | __main__:convert_examples_to_features:112 - tokens: %s\n",
      "2020-09-29 17:25:37.347 | INFO     | __main__:convert_examples_to_features:113 - input_ids: %s\n",
      "2020-09-29 17:25:37.347 | INFO     | __main__:convert_examples_to_features:114 - input_mask: %s\n",
      "2020-09-29 17:25:37.348 | INFO     | __main__:convert_examples_to_features:115 - segment_ids: %s\n",
      "2020-09-29 17:25:37.348 | INFO     | __main__:convert_examples_to_features:116 - label_ids: %s\n",
      "2020-09-29 17:25:37.353 | INFO     | __main__:convert_examples_to_features:110 - *** Example ***\n",
      "2020-09-29 17:25:37.353 | INFO     | __main__:convert_examples_to_features:111 - guid: %s\n",
      "2020-09-29 17:25:37.353 | INFO     | __main__:convert_examples_to_features:112 - tokens: %s\n",
      "2020-09-29 17:25:37.353 | INFO     | __main__:convert_examples_to_features:113 - input_ids: %s\n",
      "2020-09-29 17:25:37.354 | INFO     | __main__:convert_examples_to_features:114 - input_mask: %s\n",
      "2020-09-29 17:25:37.354 | INFO     | __main__:convert_examples_to_features:115 - segment_ids: %s\n",
      "2020-09-29 17:25:37.354 | INFO     | __main__:convert_examples_to_features:116 - label_ids: %s\n",
      "2020-09-29 17:25:37.360 | INFO     | __main__:convert_examples_to_features:110 - *** Example ***\n",
      "2020-09-29 17:25:37.360 | INFO     | __main__:convert_examples_to_features:111 - guid: %s\n",
      "2020-09-29 17:25:37.361 | INFO     | __main__:convert_examples_to_features:112 - tokens: %s\n",
      "2020-09-29 17:25:37.361 | INFO     | __main__:convert_examples_to_features:113 - input_ids: %s\n",
      "2020-09-29 17:25:37.361 | INFO     | __main__:convert_examples_to_features:114 - input_mask: %s\n",
      "2020-09-29 17:25:37.362 | INFO     | __main__:convert_examples_to_features:115 - segment_ids: %s\n",
      "2020-09-29 17:25:37.362 | INFO     | __main__:convert_examples_to_features:116 - label_ids: %s\n",
      "2020-09-29 17:25:37.363 | INFO     | __main__:convert_examples_to_features:110 - *** Example ***\n",
      "2020-09-29 17:25:37.364 | INFO     | __main__:convert_examples_to_features:111 - guid: %s\n",
      "2020-09-29 17:25:37.364 | INFO     | __main__:convert_examples_to_features:112 - tokens: %s\n",
      "2020-09-29 17:25:37.364 | INFO     | __main__:convert_examples_to_features:113 - input_ids: %s\n",
      "2020-09-29 17:25:37.365 | INFO     | __main__:convert_examples_to_features:114 - input_mask: %s\n",
      "2020-09-29 17:25:37.365 | INFO     | __main__:convert_examples_to_features:115 - segment_ids: %s\n",
      "2020-09-29 17:25:37.365 | INFO     | __main__:convert_examples_to_features:116 - label_ids: %s\n",
      "2020-09-29 17:25:37.370 | INFO     | __main__:convert_examples_to_features:110 - *** Example ***\n",
      "2020-09-29 17:25:37.370 | INFO     | __main__:convert_examples_to_features:111 - guid: %s\n",
      "2020-09-29 17:25:37.371 | INFO     | __main__:convert_examples_to_features:112 - tokens: %s\n",
      "2020-09-29 17:25:37.371 | INFO     | __main__:convert_examples_to_features:113 - input_ids: %s\n",
      "2020-09-29 17:25:37.371 | INFO     | __main__:convert_examples_to_features:114 - input_mask: %s\n",
      "2020-09-29 17:25:37.372 | INFO     | __main__:convert_examples_to_features:115 - segment_ids: %s\n",
      "2020-09-29 17:25:37.372 | INFO     | __main__:convert_examples_to_features:116 - label_ids: %s\n"
     ]
    }
   ],
   "source": [
    "features = convert_examples_to_features(examples, labels, max_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputFeatures(input_ids=[101, 1020, 100, 1034, 100, 886, 100, 995, 100, 1052, 1010, 100, 100, 100, 100, 100, 100, 100, 991, 999, 1020, 100, 1034, 100, 886, 1039, 100, 100, 100, 100, 100, 1099, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 1099, 100, 100, 1099, 100, 100, 1099, 100, 100, 886, 100, 100, 100, 121, 119, 125, 176, 886, 100, 100, 1099, 122, 121, 100, 120, 100, 115, 124, 100, 120, 100, 886, 100, 100, 100, 100, 886, 100, 100, 1102, 100, 100, 100, 100, 100, 1099, 100, 100, 100, 1006, 886, 133, 171, 187, 120, 135, 980, 100, 1102, 100, 100, 886, 100, 100, 1099, 100, 100, 100, 100, 886, 100, 100, 100, 100, 100, 100, 885, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 1056, 100, 100, 886, 100, 100, 1099, 100, 100, 100, 100, 886, 100, 100, 100, 100, 100, 100, 885, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 1056, 886, 997, 100, 1099, 976, 100, 124, 118, 126, 100, 1099, 976, 1033, 124, 100, 100, 100, 100, 100, 100, 886, 1037, 100, 100, 122, 126, 1010, 100, 100, 100, 1099, 100, 1037, 100, 100, 100, 100, 100, 886, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 24, 0, 23, 24, 0, 23, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 26, 0, 25, 26, 26, 26, 0, 0, 0, 9, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0, 25, 26, 0, 25, 26, 26, 26, 0, 0, 0, 9, 10, 10, 10, 0, 9, 10, 10, 10, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    features: List[InputFeatures]\n",
    "    pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index\n",
    "    # Use cross entropy ignore_index as padding label id so that only\n",
    "    # real label ids contribute to the loss later.\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        examples,\n",
    "        tokenizer, #: PreTrainedTokenizer,\n",
    "        labels: List[str],\n",
    "        model_type: str,\n",
    "        max_seq_length: Optional[int] = None,\n",
    "    ):\n",
    "        \n",
    "        # TODO clean up all this to leverage built-in features of tokenizers\n",
    "        self.features = convert_examples_to_features(\n",
    "            examples,\n",
    "            labels,\n",
    "            max_seq_length,\n",
    "            tokenizer,\n",
    "            cls_token_at_end=bool(model_type in [\"xlnet\"]),\n",
    "            # xlnet has a cls token at the end\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            cls_token_segment_id=2 if model_type in [\"xlnet\"] else 0,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            sep_token_extra=False,\n",
    "            # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "            pad_on_left=bool(tokenizer.padding_side == \"left\"),\n",
    "            pad_token=tokenizer.pad_token_id,\n",
    "            pad_token_segment_id=tokenizer.pad_token_type_id,\n",
    "            pad_token_label_id=self.pad_token_label_id,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> InputFeatures:\n",
    "        return self.features[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e898693747aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_data_collator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_loader = DataLoader(features, batch_size=6, shuffle=True,collate_fn=default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_loader)  # size of features / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'labels': tensor([[-100,    0,    0,  ...,    0,    0, -100],\n",
      "        [-100,    0,    0,  ..., -100, -100, -100],\n",
      "        [-100,    0,    0,  ...,    0,    0, -100],\n",
      "        [-100,    0,    0,  ..., -100, -100, -100],\n",
      "        [-100,    0,    0,  ...,    0,    0, -100],\n",
      "        [-100,    0,    0,  ...,    0,    0, -100]]), 'input_ids': tensor([[ 101,  125,  119,  ...,  100,  100,  102],\n",
      "        [ 101,  100,  100,  ...,    0,    0,    0],\n",
      "        [ 101,  100,  100,  ...,  176,  115,  102],\n",
      "        [ 101,  100,  100,  ...,    0,    0,    0],\n",
      "        [ 101, 1039,  100,  ...,  100,  100,  102],\n",
      "        [ 101,  100,  100,  ...,  100,  100,  102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# for step, inputs in enumerate(train_data_loader):\n",
    "#     print(step)\n",
    "#     print(inputs)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import nested_concat, nested_numpify, EvalPrediction\n",
    "from typing import Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n",
    "#     print(preds_list)\n",
    "#     print(out_label_list)\n",
    "    return {\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    batch_size, seq_len = preds.shape\n",
    "\n",
    "    out_label_list = [[] for _ in range(batch_size)]\n",
    "    preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "#         print(len(out_label_list[i]), len(preds_list[i]))\n",
    "    return preds_list, out_label_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470b6da4ee464633b63ea65adbce80aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=621.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=len(labels),\n",
    "        id2label= label_map,\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-DRUG\",\n",
       "    \"2\": \"I-DRUG\",\n",
       "    \"3\": \"B-DRUG_INGREDIENT\",\n",
       "    \"4\": \"I-DRUG_INGREDIENT\",\n",
       "    \"5\": \"B-DISEASE\",\n",
       "    \"6\": \"I-DISEASE\",\n",
       "    \"7\": \"B-SYMPTOM\",\n",
       "    \"8\": \"I-SYMPTOM\",\n",
       "    \"9\": \"B-SYNDROME\",\n",
       "    \"10\": \"I-SYNDROME\",\n",
       "    \"11\": \"B-DISEASE_GROUP\",\n",
       "    \"12\": \"I-DISEASE_GROUP\",\n",
       "    \"13\": \"B-FOOD\",\n",
       "    \"14\": \"I-FOOD\",\n",
       "    \"15\": \"B-FOOD_GROUP\",\n",
       "    \"16\": \"I-FOOD_GROUP\",\n",
       "    \"17\": \"B-PERSON_GROUP\",\n",
       "    \"18\": \"I-PERSON_GROUP\",\n",
       "    \"19\": \"B-DRUG_GROUP\",\n",
       "    \"20\": \"I-DRUG_GROUP\",\n",
       "    \"21\": \"B-DRUG_DOSAGE\",\n",
       "    \"22\": \"I-DRUG_DOSAGE\",\n",
       "    \"23\": \"B-DRUG_TASTE\",\n",
       "    \"24\": \"I-DRUG_TASTE\",\n",
       "    \"25\": \"B-DRUG_EFFICACY\",\n",
       "    \"26\": \"I-DRUG_EFFICACY\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"B-DISEASE\": 5,\n",
       "    \"B-DISEASE_GROUP\": 11,\n",
       "    \"B-DRUG\": 1,\n",
       "    \"B-DRUG_DOSAGE\": 21,\n",
       "    \"B-DRUG_EFFICACY\": 25,\n",
       "    \"B-DRUG_GROUP\": 19,\n",
       "    \"B-DRUG_INGREDIENT\": 3,\n",
       "    \"B-DRUG_TASTE\": 23,\n",
       "    \"B-FOOD\": 13,\n",
       "    \"B-FOOD_GROUP\": 15,\n",
       "    \"B-PERSON_GROUP\": 17,\n",
       "    \"B-SYMPTOM\": 7,\n",
       "    \"B-SYNDROME\": 9,\n",
       "    \"I-DISEASE\": 6,\n",
       "    \"I-DISEASE_GROUP\": 12,\n",
       "    \"I-DRUG\": 2,\n",
       "    \"I-DRUG_DOSAGE\": 22,\n",
       "    \"I-DRUG_EFFICACY\": 26,\n",
       "    \"I-DRUG_GROUP\": 20,\n",
       "    \"I-DRUG_INGREDIENT\": 4,\n",
       "    \"I-DRUG_TASTE\": 24,\n",
       "    \"I-FOOD\": 14,\n",
       "    \"I-FOOD_GROUP\": 16,\n",
       "    \"I-PERSON_GROUP\": 18,\n",
       "    \"I-SYMPTOM\": 8,\n",
       "    \"I-SYNDROME\": 10,\n",
       "    \"O\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyBertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertForTokenClassification(BertPreTrainedModel):\n",
    "\n",
    "    authorized_unexpected_keys = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "#         return outputs, logits\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyBertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyBertForTokenClassification(config)\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, inputs in enumerate(train_data_loader):\n",
    "#     print(step)\n",
    "#     print(inputs)\n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            inputs[k] = v.to('cuda')\n",
    "    outputs, logits = model(**inputs)\n",
    "#     print(outputs[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 200, 27])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 200])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['attention_mask'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_loss = inputs['attention_mask'].view(-1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_logits = logits.view(-1, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1200, 27])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " (tensor([[[ 0.9684,  2.7308,  0.0000,  ..., -0.6839, -2.0568,  0.1047],\n",
       "           [-0.4525,  0.7580,  1.0821,  ..., -0.9978, -1.4021,  0.0256],\n",
       "           [-0.1976,  0.0000,  1.2650,  ..., -2.1700, -2.6246, -0.7749],\n",
       "           ...,\n",
       "           [ 0.4136, -0.3217, -0.2158,  ..., -1.3179, -1.3585, -0.5520],\n",
       "           [ 0.9265, -0.1301, -0.5426,  ..., -0.3856, -1.7954, -1.3358],\n",
       "           [ 0.5968, -0.1118,  0.0000,  ..., -1.6809, -1.4990,  0.1104]],\n",
       "  \n",
       "          [[ 0.9684,  2.7308, -0.0140,  ..., -0.6839, -2.0568,  0.1047],\n",
       "           [ 1.4905,  2.9093,  2.0309,  ..., -0.8984, -1.8409,  0.7107],\n",
       "           [-1.2150,  1.2836,  1.4268,  ..., -0.6602, -2.2829,  0.0000],\n",
       "           ...,\n",
       "           [ 0.3063,  1.0189,  0.3453,  ..., -1.9695, -2.3599, -0.3869],\n",
       "           [-0.0974, -0.5618,  0.7042,  ...,  0.6677, -1.9689,  0.3352],\n",
       "           [-0.2496,  0.0063,  0.5047,  ..., -1.9593, -0.9733,  0.0000]],\n",
       "  \n",
       "          [[ 0.9684,  2.7308, -0.0140,  ..., -0.6839, -2.0568,  0.1047],\n",
       "           [ 0.5409,  2.8065,  0.2995,  ..., -1.8346, -3.0500, -0.6517],\n",
       "           [-0.7853,  1.2820,  0.4147,  ...,  0.0000, -4.2944, -0.8459],\n",
       "           ...,\n",
       "           [ 0.6110,  1.3844, -0.4616,  ...,  0.0000,  0.3207, -0.2442],\n",
       "           [ 0.1474,  0.6566,  1.3517,  ...,  0.7127, -2.2180,  0.6763],\n",
       "           [-0.2496,  0.0063,  0.5047,  ..., -1.9593, -0.9733,  0.9034]],\n",
       "  \n",
       "          [[ 0.9684,  2.7308, -0.0140,  ..., -0.6839,  0.0000,  0.1047],\n",
       "           [ 0.5409,  2.8065,  0.2995,  ..., -1.8346, -3.0500, -0.6517],\n",
       "           [-0.7853,  1.2820,  0.4147,  ..., -1.9875, -4.2944, -0.8459],\n",
       "           ...,\n",
       "           [ 0.4136, -0.3217, -0.2158,  ..., -1.3179, -1.3585, -0.5520],\n",
       "           [ 0.9265, -0.1301,  0.0000,  ...,  0.0000, -1.7954, -1.3358],\n",
       "           [ 0.5968, -0.1118, -0.4248,  ..., -1.6809, -1.4990,  0.1104]],\n",
       "  \n",
       "          [[ 0.9684,  2.7308, -0.0140,  ..., -0.6839, -2.0568,  0.0000],\n",
       "           [-0.5320,  3.1923,  0.9599,  ...,  0.0000, -1.9422,  0.0000],\n",
       "           [-1.6285, -0.4395,  0.0000,  ..., -0.3217, -3.5866,  0.5805],\n",
       "           ...,\n",
       "           [ 0.0000,  1.7784,  0.9119,  ..., -0.9478, -0.6698,  0.8852],\n",
       "           [ 0.7902,  1.1955,  0.0286,  ..., -1.0173, -2.7247, -1.1290],\n",
       "           [-0.2496,  0.0063,  0.5047,  ..., -1.9593, -0.9733,  0.9034]],\n",
       "  \n",
       "          [[ 0.9684,  2.7308, -0.0140,  ..., -0.6839, -2.0568,  0.1047],\n",
       "           [-0.0767,  3.5652,  0.8543,  ..., -0.8308,  0.0000,  0.5933],\n",
       "           [-0.7853,  1.2820,  0.0000,  ..., -1.9875, -4.2944, -0.8459],\n",
       "           ...,\n",
       "           [ 0.3063,  1.0189,  0.3453,  ..., -1.9695,  0.0000, -0.3869],\n",
       "           [ 0.7902,  1.1955,  0.0000,  ..., -1.0173, -2.7247, -1.1290],\n",
       "           [-0.2496,  0.0063,  0.5047,  ..., -1.9593, -0.9733,  0.9034]]],\n",
       "         device='cuda:0', grad_fn=<FusedDropoutBackward>),\n",
       "  tensor([[[ 1.1838,  2.3396,  1.0208,  ..., -0.9217, -1.5448,  0.5546],\n",
       "           [-0.7964,  0.6041,  1.9381,  ..., -0.9315, -0.8640,  0.5019],\n",
       "           [-0.1170,  0.0083,  1.3252,  ..., -1.5568, -1.7613, -0.5446],\n",
       "           ...,\n",
       "           [ 0.2204, -0.3187,  0.4528,  ..., -1.6026, -0.8335,  0.1143],\n",
       "           [ 1.0583,  0.1096, -0.4671,  ..., -0.8592, -1.5268, -0.9095],\n",
       "           [ 0.2587, -0.0471,  0.5113,  ..., -1.4484, -1.1030,  0.5321]],\n",
       "  \n",
       "          [[ 1.2819,  2.3757,  0.7278,  ..., -0.7798, -1.6824, -0.0907],\n",
       "           [ 1.3696,  2.4415,  2.4534,  ..., -1.1397, -1.3575,  0.7202],\n",
       "           [-0.9358,  1.1126,  1.5134,  ..., -0.3453, -1.5472,  0.2477],\n",
       "           ...,\n",
       "           [-0.0423,  0.7835,  1.0529,  ..., -2.0983, -1.3828, -0.2732],\n",
       "           [ 0.1280, -0.5196,  1.2001,  ...,  0.0212, -1.2935,  0.3240],\n",
       "           [-0.0824,  0.0407,  1.5048,  ..., -2.1533, -0.1542, -0.0126]],\n",
       "  \n",
       "          [[ 0.8924,  2.2884,  1.0066,  ..., -0.7009, -1.6064,  0.5598],\n",
       "           [-0.0447,  2.5150,  0.5165,  ..., -1.7930, -2.4909, -0.3684],\n",
       "           [-0.8552,  1.1213,  1.0320,  ..., -0.2996, -3.0962, -0.6407],\n",
       "           ...,\n",
       "           [ 0.6130,  1.0842, -0.1800,  ..., -0.3506,  0.7846,  0.0101],\n",
       "           [ 0.3630,  0.3666,  1.6398,  ..., -0.3031, -1.4366,  0.5935],\n",
       "           [-0.2222, -0.2788,  1.5880,  ..., -1.7867, -0.0477,  0.7348]],\n",
       "  \n",
       "          [[ 1.3130,  2.2532,  0.9478,  ..., -0.9388,  0.2110,  0.2890],\n",
       "           [ 0.0955,  2.8767,  1.0651,  ..., -1.4020, -2.6087, -0.2058],\n",
       "           [-0.9925,  1.1452,  1.0856,  ..., -1.8762, -3.3764, -0.6508],\n",
       "           ...,\n",
       "           [ 0.0456, -0.5162, -0.1295,  ..., -1.4114, -0.7955, -0.0404],\n",
       "           [ 0.9624,  0.0853, -0.0179,  ..., -0.1272, -1.6393, -0.9464],\n",
       "           [ 0.5740, -0.0151,  0.2436,  ..., -1.4297, -0.8815,  0.6709]],\n",
       "  \n",
       "          [[ 0.8391,  2.5011,  0.9665,  ..., -0.6294, -1.7140,  0.0707],\n",
       "           [-0.6095,  2.7524,  1.6358,  ..., -0.2243, -1.2105,  0.4765],\n",
       "           [-1.6590, -0.3553,  0.5296,  ..., -0.6845, -2.8263,  0.8358],\n",
       "           ...,\n",
       "           [-0.0848,  1.6336,  1.0368,  ..., -1.6647,  0.0506,  0.7461],\n",
       "           [ 0.5860,  1.1210,  0.3896,  ..., -1.8498, -1.9220, -1.1918],\n",
       "           [-0.1136,  0.0082,  1.7003,  ..., -2.2505, -0.1330,  0.8153]],\n",
       "  \n",
       "          [[ 1.1244,  2.3946,  0.7114,  ..., -0.7781, -1.6228,  0.3594],\n",
       "           [-0.1909,  3.3080,  0.9611,  ..., -0.8092,  0.1475,  0.6960],\n",
       "           [-0.7802,  1.1446,  0.8253,  ..., -1.9282, -3.1580, -0.7503],\n",
       "           ...,\n",
       "           [-0.1462,  1.0718,  0.7753,  ..., -2.0677,  0.7316, -0.2105],\n",
       "           [ 0.5964,  1.2360,  0.5155,  ..., -1.6642, -1.8271, -0.8140],\n",
       "           [-0.0701, -0.3826,  1.3476,  ..., -1.9615, -0.1255,  0.9360]]],\n",
       "         device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 1.4655e+00,  1.9391e+00,  1.3514e+00,  ..., -1.1576e+00,\n",
       "            -4.1724e-02,  1.1002e+00],\n",
       "           [-5.8021e-01,  1.6827e-01,  1.8225e+00,  ..., -3.1901e-01,\n",
       "            -2.8817e-01,  8.4986e-01],\n",
       "           [-2.4439e-01, -1.3392e-01,  1.3998e+00,  ..., -1.5482e+00,\n",
       "            -1.0678e+00,  4.5163e-01],\n",
       "           ...,\n",
       "           [ 4.0259e-01, -1.8485e-01,  8.4534e-01,  ..., -1.7993e+00,\n",
       "            -6.8398e-01,  1.0751e+00],\n",
       "           [ 1.5950e+00,  2.0093e-01, -5.2718e-01,  ..., -1.0388e+00,\n",
       "            -1.0763e+00, -3.5722e-03],\n",
       "           [ 4.3594e-01, -6.4119e-02,  7.2886e-01,  ..., -1.2140e+00,\n",
       "            -3.1440e-01,  1.2083e+00]],\n",
       "  \n",
       "          [[ 1.2412e+00,  1.9445e+00,  5.5718e-01,  ..., -1.1878e+00,\n",
       "            -2.3062e-01,  6.9910e-01],\n",
       "           [ 1.2744e+00,  2.7335e+00,  2.4721e+00,  ..., -7.1092e-01,\n",
       "            -1.0620e+00,  9.2274e-01],\n",
       "           [-7.7175e-01,  1.0868e+00,  1.1785e+00,  ..., -3.2024e-01,\n",
       "            -1.6276e+00,  7.1951e-01],\n",
       "           ...,\n",
       "           [-3.0428e-01,  6.3406e-01,  8.4743e-01,  ..., -2.6295e+00,\n",
       "            -7.5690e-01,  8.6666e-01],\n",
       "           [ 4.9214e-01, -6.6268e-01,  5.3351e-01,  ..., -1.5406e-01,\n",
       "            -1.1876e+00,  1.1993e+00],\n",
       "           [-1.1095e-01,  1.0824e-01,  1.1507e+00,  ..., -1.9305e+00,\n",
       "            -1.2148e-01,  6.8962e-01]],\n",
       "  \n",
       "          [[ 1.1935e+00,  2.1161e+00,  1.2319e+00,  ..., -1.0638e+00,\n",
       "            -1.7853e-01,  1.0234e+00],\n",
       "           [ 5.6248e-02,  2.1071e+00,  6.3826e-01,  ..., -1.6500e+00,\n",
       "            -1.9243e+00,  5.8630e-01],\n",
       "           [-7.7893e-01,  5.5238e-01,  5.7709e-01,  ..., -8.5525e-01,\n",
       "            -2.2932e+00,  3.5356e-01],\n",
       "           ...,\n",
       "           [ 6.0151e-01,  8.9715e-01, -2.0738e-01,  ..., -8.0825e-01,\n",
       "             1.1230e+00,  6.0541e-01],\n",
       "           [ 8.0677e-01,  7.7186e-01,  1.7224e+00,  ..., -4.5251e-01,\n",
       "            -8.3939e-01,  1.1686e+00],\n",
       "           [-3.8220e-01, -2.3188e-01,  1.1682e+00,  ..., -1.7830e+00,\n",
       "             5.0952e-01,  1.4975e+00]],\n",
       "  \n",
       "          [[ 1.2261e+00,  2.0582e+00,  9.9535e-01,  ..., -1.0642e+00,\n",
       "             1.4642e+00,  7.0317e-01],\n",
       "           [-4.7963e-02,  2.3378e+00,  1.0091e+00,  ..., -1.4053e+00,\n",
       "            -1.9979e+00,  8.3804e-01],\n",
       "           [-1.2287e+00,  8.3430e-01,  7.6778e-01,  ..., -2.1077e+00,\n",
       "            -2.7819e+00,  4.7781e-01],\n",
       "           ...,\n",
       "           [ 1.6101e-01, -2.5169e-01,  1.0959e-01,  ..., -1.2673e+00,\n",
       "            -3.6997e-01,  6.7164e-01],\n",
       "           [ 1.5534e+00,  4.3448e-01, -1.0680e-01,  ..., -2.8433e-01,\n",
       "            -8.4018e-01, -2.3019e-01],\n",
       "           [ 5.4689e-01, -1.8602e-01,  4.1189e-01,  ..., -1.2516e+00,\n",
       "            -2.6893e-01,  1.1857e+00]],\n",
       "  \n",
       "          [[ 1.3721e+00,  2.1779e+00,  7.4836e-01,  ..., -1.0100e+00,\n",
       "            -9.0322e-01,  6.6584e-01],\n",
       "           [-4.7687e-01,  2.4209e+00,  1.9451e+00,  ..., -3.1642e-01,\n",
       "            -4.3791e-01,  8.6886e-01],\n",
       "           [-1.4202e+00, -7.9918e-01,  2.2853e-02,  ..., -7.5630e-01,\n",
       "            -2.5785e+00,  1.8029e+00],\n",
       "           ...,\n",
       "           [ 3.0211e-01,  1.4561e+00,  1.3435e+00,  ..., -1.8775e+00,\n",
       "             4.3515e-01,  1.2355e+00],\n",
       "           [ 5.4945e-01,  9.0604e-01,  2.0813e-02,  ..., -2.1477e+00,\n",
       "            -1.4866e+00, -2.3393e-01],\n",
       "           [ 1.3627e-03,  2.4892e-01,  1.2083e+00,  ..., -2.1894e+00,\n",
       "             5.3963e-01,  1.3865e+00]],\n",
       "  \n",
       "          [[ 1.5827e+00,  2.0169e+00,  6.2939e-01,  ..., -1.0147e+00,\n",
       "            -4.6120e-01,  9.2811e-01],\n",
       "           [ 3.6583e-01,  2.9738e+00,  1.1642e+00,  ..., -6.7172e-01,\n",
       "             8.2309e-01,  1.1934e+00],\n",
       "           [-6.9646e-01,  5.1490e-01,  7.4949e-01,  ..., -1.9771e+00,\n",
       "            -2.8585e+00,  4.3933e-01],\n",
       "           ...,\n",
       "           [-2.9625e-02,  5.9238e-01,  8.6867e-01,  ..., -1.9927e+00,\n",
       "             1.1145e+00,  7.8639e-01],\n",
       "           [ 6.6344e-01,  9.6509e-01,  3.9972e-01,  ..., -1.9607e+00,\n",
       "            -1.3192e+00,  5.1200e-02],\n",
       "           [-1.3320e-01, -4.6279e-01,  1.1166e+00,  ..., -2.0227e+00,\n",
       "             6.4016e-01,  1.8281e+00]]], device='cuda:0',\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 1.4198e+00,  2.1764e+00,  1.0062e+00,  ..., -1.1252e+00,\n",
       "             5.4758e-01,  1.3218e+00],\n",
       "           [ 5.1328e-02, -1.9620e-01,  1.5637e+00,  ..., -5.5503e-01,\n",
       "             3.0726e-01,  1.0946e+00],\n",
       "           [-1.1632e-01, -1.6542e-01,  1.2157e+00,  ..., -1.4130e+00,\n",
       "            -9.1807e-01,  6.8634e-01],\n",
       "           ...,\n",
       "           [ 7.0878e-01, -1.5674e-01,  1.3773e+00,  ..., -1.1488e+00,\n",
       "            -8.3482e-01,  1.0562e+00],\n",
       "           [ 1.9633e+00,  9.4159e-01,  6.8026e-05,  ..., -5.3382e-01,\n",
       "            -1.0516e+00,  3.5089e-01],\n",
       "           [ 1.0836e+00,  4.6810e-01,  5.9002e-01,  ..., -1.1645e+00,\n",
       "            -3.3773e-01,  1.3112e+00]],\n",
       "  \n",
       "          [[ 1.5173e+00,  2.2710e+00,  4.9340e-01,  ..., -1.4563e+00,\n",
       "             4.5674e-01,  7.4733e-01],\n",
       "           [ 1.4900e+00,  2.4908e+00,  2.3528e+00,  ..., -4.8266e-01,\n",
       "            -6.7718e-01,  1.1142e+00],\n",
       "           [-3.2404e-01,  1.0605e+00,  7.2764e-01,  ...,  7.3604e-04,\n",
       "            -9.9532e-01,  7.7106e-01],\n",
       "           ...,\n",
       "           [-1.8445e-01,  5.6985e-01,  8.3638e-01,  ..., -1.7683e+00,\n",
       "            -6.8458e-01,  1.5834e+00],\n",
       "           [ 1.0793e+00, -4.6588e-01,  7.9519e-01,  ..., -1.8028e-01,\n",
       "            -1.0495e+00,  1.7718e+00],\n",
       "           [ 3.0521e-01,  1.1116e-01,  1.0788e+00,  ..., -1.3789e+00,\n",
       "             2.3580e-01,  5.9500e-01]],\n",
       "  \n",
       "          [[ 1.4240e+00,  1.9821e+00,  8.5094e-01,  ..., -1.1431e+00,\n",
       "             4.5084e-01,  1.1127e+00],\n",
       "           [ 5.0790e-01,  1.8267e+00,  3.4277e-01,  ..., -1.6791e+00,\n",
       "            -1.5863e+00,  1.2174e+00],\n",
       "           [-1.2934e-01,  1.9322e-01,  8.0823e-02,  ..., -7.9075e-01,\n",
       "            -2.2818e+00,  1.0101e+00],\n",
       "           ...,\n",
       "           [ 1.0985e+00,  1.0709e+00, -1.3164e-01,  ..., -3.7670e-01,\n",
       "             8.4312e-01,  5.7850e-01],\n",
       "           [ 1.7618e+00,  1.0777e+00,  1.8537e+00,  ...,  2.8652e-01,\n",
       "            -4.4708e-01,  1.2001e+00],\n",
       "           [ 4.0981e-01, -9.6567e-02,  1.2007e+00,  ..., -1.2663e+00,\n",
       "             6.5753e-01,  1.5154e+00]],\n",
       "  \n",
       "          [[ 1.2182e+00,  2.4490e+00,  9.5726e-01,  ..., -1.2534e+00,\n",
       "             2.0851e+00,  7.1415e-01],\n",
       "           [ 4.9005e-02,  2.2321e+00,  4.2906e-01,  ..., -1.1438e+00,\n",
       "            -1.5543e+00,  1.6703e+00],\n",
       "           [-6.5033e-01,  2.9049e-01,  2.5242e-01,  ..., -1.9820e+00,\n",
       "            -2.5124e+00,  1.2603e+00],\n",
       "           ...,\n",
       "           [ 7.2389e-01, -2.1605e-03,  2.9293e-01,  ..., -7.2510e-01,\n",
       "            -5.9209e-01,  4.7998e-01],\n",
       "           [ 2.2031e+00,  4.8469e-01, -1.5176e-01,  ...,  1.1377e-02,\n",
       "            -8.3783e-01,  2.9121e-01],\n",
       "           [ 1.1821e+00,  4.5549e-01,  2.8367e-01,  ..., -1.1302e+00,\n",
       "             2.7470e-02,  1.6491e+00]],\n",
       "  \n",
       "          [[ 1.5525e+00,  2.4802e+00,  5.4620e-01,  ..., -1.0108e+00,\n",
       "             5.7994e-02,  7.9077e-01],\n",
       "           [ 1.3290e-01,  2.2802e+00,  1.8573e+00,  ...,  4.2562e-02,\n",
       "             1.3367e-02,  8.1791e-01],\n",
       "           [-7.8873e-01, -1.0525e+00, -7.2865e-02,  ..., -3.8075e-01,\n",
       "            -1.7465e+00,  1.7656e+00],\n",
       "           ...,\n",
       "           [ 1.0766e+00,  1.3842e+00,  1.0955e+00,  ..., -8.8934e-01,\n",
       "             4.7324e-01,  1.1417e+00],\n",
       "           [ 1.1241e+00,  8.3297e-01,  3.5875e-01,  ..., -1.8714e+00,\n",
       "            -1.4066e+00, -2.0010e-01],\n",
       "           [ 6.7670e-01,  2.9121e-01,  1.0391e+00,  ..., -1.6930e+00,\n",
       "             6.6901e-01,  1.2499e+00]],\n",
       "  \n",
       "          [[ 1.8170e+00,  2.2385e+00,  3.2496e-01,  ..., -1.1558e+00,\n",
       "             1.1790e-01,  8.8589e-01],\n",
       "           [ 1.4000e+00,  2.7362e+00,  5.5286e-01,  ..., -2.9965e-01,\n",
       "             1.5034e+00,  1.0379e+00],\n",
       "           [-2.7425e-01,  7.4775e-02,  2.1283e-02,  ..., -1.6365e+00,\n",
       "            -2.6598e+00,  1.2066e+00],\n",
       "           ...,\n",
       "           [ 5.3685e-01,  6.3626e-01,  8.2038e-01,  ..., -1.4317e+00,\n",
       "             7.4317e-01,  1.2584e+00],\n",
       "           [ 1.4031e+00,  1.0380e+00,  8.1170e-01,  ..., -1.6490e+00,\n",
       "            -1.3086e+00,  8.1546e-01],\n",
       "           [ 5.7497e-01, -2.4089e-01,  1.0932e+00,  ..., -1.7110e+00,\n",
       "             5.6475e-01,  1.6591e+00]]], device='cuda:0',\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 2.1369e+00,  2.0252e+00,  9.0568e-01,  ..., -1.1973e+00,\n",
       "             7.0886e-01,  1.6728e+00],\n",
       "           [ 7.6901e-01, -2.4726e-01,  1.4762e+00,  ..., -3.6803e-01,\n",
       "             6.1784e-01,  5.7649e-01],\n",
       "           [ 5.9208e-01, -7.3927e-01,  6.9927e-01,  ..., -1.5219e+00,\n",
       "            -6.8161e-01,  8.7115e-01],\n",
       "           ...,\n",
       "           [ 9.2239e-01, -9.9142e-02,  9.4702e-01,  ..., -1.0762e+00,\n",
       "            -8.1101e-01,  7.4549e-01],\n",
       "           [ 2.1466e+00,  4.5837e-01, -2.0692e-01,  ..., -1.0107e+00,\n",
       "            -1.0080e+00,  9.4543e-01],\n",
       "           [ 1.3704e+00,  4.1589e-01,  7.0134e-01,  ..., -1.0190e+00,\n",
       "            -3.1887e-01,  1.2661e+00]],\n",
       "  \n",
       "          [[ 2.3000e+00,  2.0636e+00,  1.3787e-01,  ..., -1.7290e+00,\n",
       "             6.1121e-01,  6.5918e-01],\n",
       "           [ 1.7261e+00,  2.3505e+00,  1.6885e+00,  ..., -2.9738e-01,\n",
       "            -1.7837e-01,  1.0037e+00],\n",
       "           [ 4.9595e-01,  8.7361e-01, -2.4545e-01,  ...,  1.2822e-01,\n",
       "            -8.4531e-01,  1.1517e+00],\n",
       "           ...,\n",
       "           [ 3.1137e-01,  7.7756e-01,  6.0857e-01,  ..., -1.4930e+00,\n",
       "            -7.2713e-01,  1.4178e+00],\n",
       "           [ 1.3466e+00, -4.3922e-01, -3.9027e-04,  ..., -3.7205e-01,\n",
       "            -1.1292e+00,  1.8022e+00],\n",
       "           [ 4.7698e-01,  7.0181e-02,  6.8636e-01,  ..., -1.2949e+00,\n",
       "             1.0759e-01,  1.0785e+00]],\n",
       "  \n",
       "          [[ 2.2525e+00,  1.7807e+00,  9.7526e-01,  ..., -1.1504e+00,\n",
       "             3.8944e-01,  1.5364e+00],\n",
       "           [ 1.1945e+00,  1.6015e+00,  5.2017e-01,  ..., -1.4276e+00,\n",
       "            -1.0396e+00,  1.2987e+00],\n",
       "           [ 5.0179e-01,  5.8039e-02, -1.8435e-01,  ..., -1.0302e+00,\n",
       "            -2.0657e+00,  1.5644e+00],\n",
       "           ...,\n",
       "           [ 1.6102e+00,  1.2759e+00, -7.3075e-01,  ..., -3.9121e-01,\n",
       "             4.8967e-01,  4.6207e-01],\n",
       "           [ 1.9434e+00,  1.3280e+00,  1.3882e+00,  ..., -4.5052e-02,\n",
       "            -4.9444e-01,  1.3778e+00],\n",
       "           [ 4.8585e-01,  4.6835e-02,  9.2873e-01,  ..., -1.2323e+00,\n",
       "             4.1837e-01,  1.8045e+00]],\n",
       "  \n",
       "          [[ 1.4798e+00,  1.9284e+00,  7.6642e-01,  ..., -1.3402e+00,\n",
       "             1.8571e+00,  1.3325e+00],\n",
       "           [ 3.2074e-01,  2.2819e+00,  8.1999e-01,  ..., -1.1774e+00,\n",
       "            -1.2113e+00,  1.5055e+00],\n",
       "           [-1.8639e-02,  2.0931e-01, -6.9611e-02,  ..., -1.8366e+00,\n",
       "            -1.8842e+00,  1.4109e+00],\n",
       "           ...,\n",
       "           [ 1.1135e+00, -7.1440e-02,  4.2339e-01,  ..., -8.4457e-01,\n",
       "            -6.7325e-01,  3.1064e-01],\n",
       "           [ 2.3993e+00,  3.8651e-01, -3.4954e-01,  ..., -1.8412e-01,\n",
       "            -8.4151e-01,  5.9935e-01],\n",
       "           [ 1.3274e+00,  4.2499e-01,  7.3393e-02,  ..., -1.1458e+00,\n",
       "            -1.4613e-01,  1.4815e+00]],\n",
       "  \n",
       "          [[ 2.2711e+00,  2.2885e+00,  4.0576e-01,  ..., -1.1605e+00,\n",
       "            -1.5727e-02,  1.2864e+00],\n",
       "           [ 4.9159e-01,  1.8284e+00,  1.4490e+00,  ..., -2.1687e-01,\n",
       "             2.1121e-02,  8.5173e-01],\n",
       "           [ 2.9058e-01, -1.0606e+00, -9.3596e-01,  ..., -1.6554e-01,\n",
       "            -1.2887e+00,  1.6355e+00],\n",
       "           ...,\n",
       "           [ 1.7520e+00,  1.5340e+00,  4.2382e-01,  ..., -6.6650e-01,\n",
       "             3.2404e-01,  6.6488e-01],\n",
       "           [ 1.0696e+00,  5.4674e-01,  1.5703e-01,  ..., -1.6799e+00,\n",
       "            -1.2353e+00,  1.5428e-01],\n",
       "           [ 7.8011e-01,  6.5453e-01,  5.2512e-01,  ..., -1.3655e+00,\n",
       "             3.0113e-01,  1.6273e+00]],\n",
       "  \n",
       "          [[ 2.5930e+00,  2.0412e+00,  9.8014e-02,  ..., -1.3600e+00,\n",
       "             5.6133e-01,  1.3773e+00],\n",
       "           [ 1.6710e+00,  2.8003e+00,  4.1284e-01,  ..., -3.4334e-01,\n",
       "             1.3030e+00,  8.8805e-01],\n",
       "           [ 6.5501e-01, -3.7194e-01, -1.4254e-01,  ..., -1.5482e+00,\n",
       "            -2.1451e+00,  1.0480e+00],\n",
       "           ...,\n",
       "           [ 8.1573e-01,  6.9135e-01,  3.8691e-01,  ..., -1.3212e+00,\n",
       "             6.6345e-01,  1.2272e+00],\n",
       "           [ 1.3868e+00,  7.0764e-01,  7.9496e-01,  ..., -1.7957e+00,\n",
       "            -1.1403e+00,  1.1355e+00],\n",
       "           [ 5.8388e-01, -1.3952e-01,  9.3646e-01,  ..., -1.5659e+00,\n",
       "             3.8399e-01,  2.0654e+00]]], device='cuda:0',\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 1.7772e+00,  2.1976e+00,  1.1142e+00,  ..., -2.0979e+00,\n",
       "             6.5287e-02,  1.1962e+00],\n",
       "           [ 2.9679e-01,  2.3653e-02,  1.1080e+00,  ..., -6.3337e-01,\n",
       "             6.2014e-01,  5.9435e-01],\n",
       "           [ 2.7362e-01, -8.5613e-01,  5.5549e-01,  ..., -1.6631e+00,\n",
       "            -1.0444e+00,  2.0496e-01],\n",
       "           ...,\n",
       "           [ 8.7644e-01,  2.1693e-01,  3.8771e-01,  ..., -8.5065e-01,\n",
       "            -7.4188e-01,  1.7800e-01],\n",
       "           [ 1.6772e+00,  8.1689e-01, -3.2556e-01,  ..., -1.5985e+00,\n",
       "            -8.4172e-01,  8.0794e-01],\n",
       "           [ 9.7078e-01,  6.4738e-01,  6.1992e-01,  ..., -1.3021e+00,\n",
       "            -3.8084e-01,  8.2440e-01]],\n",
       "  \n",
       "          [[ 1.7936e+00,  1.9358e+00,  1.7836e-01,  ..., -2.3107e+00,\n",
       "             2.1806e-01,  2.1569e-01],\n",
       "           [ 1.3002e+00,  2.4033e+00,  1.4422e+00,  ..., -1.1462e+00,\n",
       "            -6.9703e-01,  1.1015e+00],\n",
       "           [ 4.6740e-01,  1.2371e+00, -8.2567e-01,  ..., -9.6904e-02,\n",
       "            -7.3246e-01,  4.5702e-01],\n",
       "           ...,\n",
       "           [-5.5140e-02,  4.3101e-01, -1.9192e-01,  ..., -2.1932e+00,\n",
       "            -9.7604e-01,  1.0496e+00],\n",
       "           [ 1.2080e+00, -1.0225e-01, -4.0723e-01,  ..., -1.0782e+00,\n",
       "            -1.2239e+00,  1.4239e+00],\n",
       "           [ 4.6543e-01,  3.3865e-01,  7.0961e-01,  ..., -1.3821e+00,\n",
       "            -3.2806e-01,  2.8856e-01]],\n",
       "  \n",
       "          [[ 1.7433e+00,  1.7020e+00,  1.1608e+00,  ..., -1.9034e+00,\n",
       "            -2.9601e-01,  8.3516e-01],\n",
       "           [ 8.7158e-01,  1.3360e+00,  2.1304e-01,  ..., -1.9606e+00,\n",
       "            -1.6805e+00,  1.2093e+00],\n",
       "           [-1.6383e-01,  1.0046e-01, -6.9450e-01,  ..., -9.6610e-01,\n",
       "            -2.4089e+00,  8.4840e-01],\n",
       "           ...,\n",
       "           [ 1.1657e+00,  1.2992e+00, -1.3233e+00,  ..., -6.7775e-01,\n",
       "             1.2804e-01,  3.5929e-02],\n",
       "           [ 1.6095e+00,  1.1944e+00,  9.7430e-01,  ..., -6.5789e-01,\n",
       "            -9.2312e-01,  9.4751e-01],\n",
       "           [ 3.3607e-01, -7.4315e-03,  5.2002e-01,  ..., -1.8254e+00,\n",
       "             1.6515e-01,  1.0923e+00]],\n",
       "  \n",
       "          [[ 1.0276e+00,  2.0903e+00,  9.0356e-01,  ..., -2.0366e+00,\n",
       "             1.2407e+00,  8.9662e-01],\n",
       "           [ 1.9592e-01,  1.9489e+00,  6.5917e-01,  ..., -1.5860e+00,\n",
       "            -1.7079e+00,  1.4553e+00],\n",
       "           [-3.3606e-01,  6.0648e-01, -6.8658e-02,  ..., -1.9156e+00,\n",
       "            -2.4677e+00,  9.4882e-01],\n",
       "           ...,\n",
       "           [ 8.8452e-01,  1.7961e-01,  2.4974e-01,  ..., -1.2050e+00,\n",
       "            -4.5544e-01, -2.8202e-01],\n",
       "           [ 2.0703e+00,  4.1550e-01, -4.8825e-01,  ..., -7.3392e-01,\n",
       "            -6.7529e-01,  2.5041e-01],\n",
       "           [ 1.1704e+00,  8.2616e-01,  4.1037e-04,  ..., -1.0539e+00,\n",
       "            -8.5799e-02,  1.2831e+00]],\n",
       "  \n",
       "          [[ 1.8617e+00,  2.3767e+00,  5.6666e-01,  ..., -1.5141e+00,\n",
       "            -6.5159e-01,  5.6103e-01],\n",
       "           [ 6.2183e-01,  1.7890e+00,  1.1038e+00,  ..., -6.8022e-01,\n",
       "            -7.3030e-02,  6.6551e-01],\n",
       "           [ 3.0931e-01, -7.3638e-01, -1.0211e+00,  ..., -5.8118e-01,\n",
       "            -1.0168e+00,  1.8068e+00],\n",
       "           ...,\n",
       "           [ 1.6112e+00,  1.7385e+00,  4.3594e-01,  ..., -1.2432e+00,\n",
       "             3.1993e-01,  2.5105e-01],\n",
       "           [ 6.5404e-01,  1.5995e-02, -1.2489e-01,  ..., -1.9754e+00,\n",
       "            -1.6300e+00,  3.6798e-01],\n",
       "           [ 7.6847e-01,  7.4923e-01,  3.7123e-01,  ..., -1.8849e+00,\n",
       "             2.1989e-01,  7.0483e-01]],\n",
       "  \n",
       "          [[ 2.1763e+00,  2.1608e+00,  4.1510e-02,  ..., -2.0540e+00,\n",
       "             1.6706e-01,  9.0569e-01],\n",
       "           [ 1.6372e+00,  2.8435e+00,  3.6520e-01,  ..., -6.9545e-01,\n",
       "             1.1028e+00,  1.3222e+00],\n",
       "           [ 6.3485e-02, -2.4805e-01, -7.9987e-01,  ..., -1.5576e+00,\n",
       "            -2.4578e+00,  2.0038e-01],\n",
       "           ...,\n",
       "           [ 8.4509e-01,  8.7737e-01, -2.0328e-01,  ..., -1.7655e+00,\n",
       "             3.9895e-01,  1.3789e+00],\n",
       "           [ 1.2904e+00,  5.2396e-01,  2.6812e-01,  ..., -2.3817e+00,\n",
       "            -1.5140e+00,  7.2793e-01],\n",
       "           [ 5.8851e-01,  2.2572e-02,  5.4096e-01,  ..., -1.8608e+00,\n",
       "             1.2124e-01,  1.5683e+00]]], device='cuda:0',\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 5.7185e-01,  2.2292e+00,  1.9521e+00,  ..., -1.4349e+00,\n",
       "            -1.7076e-01,  1.0808e+00],\n",
       "           [-2.7617e-01,  6.6252e-01,  1.3280e+00,  ..., -6.4237e-02,\n",
       "             3.1277e-01,  9.1885e-01],\n",
       "           [-1.7153e-01, -2.8476e-01,  6.5574e-01,  ..., -6.8770e-01,\n",
       "            -7.2688e-01, -7.8768e-02],\n",
       "           ...,\n",
       "           [ 7.5372e-01,  4.0451e-01,  8.3132e-01,  ..., -2.9972e-01,\n",
       "            -6.6384e-01,  2.1803e-01],\n",
       "           [ 1.2657e+00,  1.3444e+00,  1.4674e-02,  ..., -1.0348e+00,\n",
       "            -1.0162e+00,  6.3488e-01],\n",
       "           [ 2.1388e-01,  7.4887e-01,  8.4101e-01,  ..., -8.9196e-01,\n",
       "            -3.2487e-01,  1.0190e+00]],\n",
       "  \n",
       "          [[ 4.7622e-01,  2.6566e+00,  1.2263e+00,  ..., -1.5826e+00,\n",
       "            -1.0348e-01,  2.7041e-01],\n",
       "           [ 8.2012e-01,  2.4847e+00,  1.9520e+00,  ..., -3.0825e-01,\n",
       "            -4.7426e-01,  1.3062e+00],\n",
       "           [ 4.6000e-01,  7.8852e-01, -5.1971e-01,  ...,  3.8765e-01,\n",
       "            -4.2396e-01,  7.7648e-01],\n",
       "           ...,\n",
       "           [ 5.1972e-04,  4.2480e-01,  3.3419e-01,  ..., -1.5116e+00,\n",
       "            -1.0768e+00,  9.9751e-01],\n",
       "           [ 1.0809e+00,  3.3182e-01, -2.5464e-01,  ..., -6.8667e-01,\n",
       "            -9.6225e-01,  1.2456e+00],\n",
       "           [ 1.3126e-01,  5.0498e-01,  1.1874e+00,  ..., -1.2522e+00,\n",
       "            -2.9710e-01,  1.5569e-01]],\n",
       "  \n",
       "          [[ 6.1219e-01,  2.3672e+00,  1.8770e+00,  ..., -1.2326e+00,\n",
       "            -1.7212e-01,  8.6005e-01],\n",
       "           [-1.2038e-01,  1.5176e+00,  3.6503e-01,  ..., -1.3481e+00,\n",
       "            -1.5517e+00,  1.4696e+00],\n",
       "           [-4.2645e-01,  4.2317e-01, -2.8424e-01,  ..., -5.6367e-01,\n",
       "            -2.2301e+00,  8.4919e-01],\n",
       "           ...,\n",
       "           [ 9.6012e-01,  1.7885e+00, -6.4686e-01,  ..., -1.8979e-01,\n",
       "             2.3383e-01,  2.5804e-01],\n",
       "           [ 8.6660e-01,  1.5739e+00,  1.1431e+00,  ..., -7.3501e-02,\n",
       "            -5.1608e-01,  8.1047e-01],\n",
       "           [ 1.0834e-01,  9.6636e-02,  7.4766e-01,  ..., -1.4603e+00,\n",
       "            -6.1248e-02,  1.0903e+00]],\n",
       "  \n",
       "          [[ 1.9136e-01,  2.5880e+00,  1.3662e+00,  ..., -9.5935e-01,\n",
       "             1.0179e+00,  8.7290e-01],\n",
       "           [-3.1950e-01,  1.7495e+00,  7.4075e-01,  ..., -9.3988e-01,\n",
       "            -1.3828e+00,  1.6253e+00],\n",
       "           [-5.9377e-01,  2.1865e-01, -3.2497e-01,  ..., -1.1280e+00,\n",
       "            -2.3324e+00,  9.2655e-01],\n",
       "           ...,\n",
       "           [ 8.0798e-01,  7.4899e-01,  4.2727e-01,  ..., -6.6498e-01,\n",
       "            -3.7850e-01, -3.7452e-01],\n",
       "           [ 1.5682e+00,  9.4370e-01, -4.2560e-01,  ..., -4.2197e-01,\n",
       "            -9.2501e-01,  5.3118e-01],\n",
       "           [ 5.5153e-01,  1.0255e+00,  1.8649e-01,  ..., -6.4096e-01,\n",
       "             1.4727e-01,  1.1933e+00]],\n",
       "  \n",
       "          [[ 1.5782e+00,  2.7136e+00,  1.1675e+00,  ..., -8.9967e-01,\n",
       "            -5.1926e-01,  6.2037e-01],\n",
       "           [-3.2429e-01,  2.3814e+00,  1.2983e+00,  ..., -9.0750e-02,\n",
       "            -3.1241e-01,  7.1287e-01],\n",
       "           [ 5.2454e-01, -3.6786e-01, -9.3487e-01,  ...,  2.0428e-01,\n",
       "            -5.0121e-01,  1.5656e+00],\n",
       "           ...,\n",
       "           [ 9.4608e-01,  2.1928e+00,  5.7399e-01,  ..., -1.1535e+00,\n",
       "             2.3950e-01,  3.3314e-01],\n",
       "           [ 2.7446e-01,  5.7467e-01,  5.9761e-01,  ..., -1.6404e+00,\n",
       "            -1.4704e+00,  8.1253e-01],\n",
       "           [ 1.7462e-01,  9.8462e-01,  4.9234e-01,  ..., -1.5079e+00,\n",
       "            -1.6305e-01,  8.9251e-01]],\n",
       "  \n",
       "          [[ 9.6476e-01,  2.7568e+00,  5.4365e-01,  ..., -1.2366e+00,\n",
       "             1.6000e-01,  8.9073e-01],\n",
       "           [ 8.7521e-01,  2.7825e+00,  5.7279e-01,  ..., -5.7778e-02,\n",
       "             1.2596e+00,  1.7624e+00],\n",
       "           [-3.4881e-02, -2.7721e-01, -3.1832e-01,  ..., -5.7794e-01,\n",
       "            -2.0273e+00,  2.4836e-01],\n",
       "           ...,\n",
       "           [ 5.9789e-01,  9.2031e-01,  5.4073e-01,  ..., -1.5940e+00,\n",
       "             1.4851e-01,  1.1807e+00],\n",
       "           [ 5.5886e-01,  9.7997e-01,  4.9877e-01,  ..., -1.8384e+00,\n",
       "            -1.5331e+00,  8.6174e-01],\n",
       "           [ 1.0047e-01, -3.3841e-01,  8.0532e-01,  ..., -1.6024e+00,\n",
       "             2.1039e-02,  1.6171e+00]]], device='cuda:0',\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 4.0858e-01,  1.4024e+00,  2.1010e+00,  ..., -4.1161e-01,\n",
       "            -6.5053e-01,  7.7294e-01],\n",
       "           [-4.9812e-01,  3.8844e-01,  1.5265e+00,  ...,  2.4117e-01,\n",
       "            -6.5189e-01,  5.8355e-01],\n",
       "           [-4.8111e-02, -9.2462e-01,  6.2893e-01,  ...,  2.6628e-01,\n",
       "            -6.4856e-01, -2.1288e-01],\n",
       "           ...,\n",
       "           [ 6.6203e-01, -2.0626e-01,  9.4858e-01,  ...,  2.2880e-01,\n",
       "            -1.5956e+00, -2.4776e-01],\n",
       "           [ 7.5879e-01,  7.9814e-01, -3.6934e-01,  ..., -6.5073e-01,\n",
       "            -1.2929e+00,  7.5500e-01],\n",
       "           [-4.1245e-02,  1.9343e-01,  8.9285e-01,  ..., -7.7003e-01,\n",
       "            -6.0739e-01,  6.7434e-01]],\n",
       "  \n",
       "          [[ 2.3099e-01,  1.7104e+00,  1.2836e+00,  ..., -7.0329e-01,\n",
       "            -1.7436e-01, -6.7089e-02],\n",
       "           [ 4.0112e-01,  2.2630e+00,  2.0214e+00,  ..., -4.9738e-01,\n",
       "            -4.9419e-01,  8.5485e-01],\n",
       "           [ 3.5476e-02,  1.0433e-01, -4.8919e-01,  ...,  7.5247e-01,\n",
       "            -6.9974e-01,  3.0944e-01],\n",
       "           ...,\n",
       "           [-1.3916e-03,  2.7350e-01,  3.3546e-01,  ..., -8.7128e-01,\n",
       "            -9.3203e-01,  1.7126e-01],\n",
       "           [ 3.8986e-01, -5.8217e-01, -3.6467e-01,  ..., -6.5649e-01,\n",
       "            -1.7291e+00,  6.4316e-01],\n",
       "           [-4.8950e-01,  4.2392e-01,  1.1072e+00,  ..., -7.1995e-01,\n",
       "            -3.3938e-01, -5.9588e-01]],\n",
       "  \n",
       "          [[ 3.2091e-01,  1.3364e+00,  2.0204e+00,  ..., -5.9269e-01,\n",
       "            -5.9327e-01,  5.3245e-01],\n",
       "           [-4.6862e-01,  1.1850e+00,  4.8787e-01,  ..., -6.2198e-01,\n",
       "            -2.3356e+00,  4.9405e-01],\n",
       "           [-9.7335e-01,  1.2688e-01, -6.0626e-01,  ...,  2.4261e-01,\n",
       "            -2.6185e+00, -1.6671e-01],\n",
       "           ...,\n",
       "           [ 8.0165e-01,  1.3171e+00, -4.6990e-01,  ...,  2.0791e-01,\n",
       "            -5.1849e-01,  1.2763e-01],\n",
       "           [ 3.4748e-01,  8.2978e-01,  1.0556e+00,  ...,  1.1529e-01,\n",
       "            -7.4651e-01,  4.3674e-01],\n",
       "           [-3.8620e-01,  5.0713e-02,  6.9966e-01,  ..., -1.2248e+00,\n",
       "            -2.9015e-01,  5.4672e-01]],\n",
       "  \n",
       "          [[-6.0505e-02,  1.6493e+00,  1.2011e+00,  ...,  2.0263e-01,\n",
       "             9.2353e-01,  5.6885e-01],\n",
       "           [-6.7120e-01,  1.4559e+00,  1.1710e+00,  ..., -6.9426e-01,\n",
       "            -1.6876e+00,  7.5758e-01],\n",
       "           [-1.4494e+00,  8.8766e-02, -5.5155e-01,  ..., -4.3901e-01,\n",
       "            -2.0878e+00,  6.1996e-01],\n",
       "           ...,\n",
       "           [ 6.1091e-01, -1.9852e-01,  4.0767e-01,  ...,  2.5783e-02,\n",
       "            -1.1286e+00, -7.2570e-01],\n",
       "           [ 1.3043e+00,  6.5132e-01, -3.8344e-01,  ..., -3.3942e-01,\n",
       "            -8.2097e-01,  1.9525e-01],\n",
       "           [ 1.5783e-01,  1.7809e-01,  2.8271e-01,  ..., -5.9960e-01,\n",
       "            -2.9236e-01,  6.5437e-01]],\n",
       "  \n",
       "          [[ 1.3759e+00,  1.9758e+00,  1.4088e+00,  ..., -7.9151e-01,\n",
       "            -4.4899e-01,  3.8819e-01],\n",
       "           [-3.6709e-01,  1.4714e+00,  1.6323e+00,  ..., -4.3215e-01,\n",
       "            -5.7887e-01,  1.9459e-01],\n",
       "           [ 1.8573e-02, -1.1922e+00, -1.0952e+00,  ...,  5.8447e-01,\n",
       "            -1.1257e+00,  6.4477e-01],\n",
       "           ...,\n",
       "           [ 1.0098e+00,  1.3460e+00,  6.8527e-01,  ..., -5.8634e-01,\n",
       "            -8.8754e-02,  7.5140e-03],\n",
       "           [-1.9688e-01,  3.5962e-02,  9.0444e-01,  ..., -1.5170e+00,\n",
       "            -1.6362e+00,  4.1579e-01],\n",
       "           [-1.8355e-01,  8.4443e-01,  7.4587e-01,  ..., -1.2849e+00,\n",
       "            -4.6406e-01,  3.0874e-02]],\n",
       "  \n",
       "          [[ 7.7682e-01,  1.8749e+00,  6.2427e-01,  ..., -2.4814e-01,\n",
       "            -1.3813e-01,  9.1706e-01],\n",
       "           [ 6.8160e-01,  2.3314e+00,  1.0503e+00,  ..., -8.1182e-02,\n",
       "             9.2804e-01,  1.3596e+00],\n",
       "           [-5.3071e-01, -7.3624e-01, -2.6109e-01,  ...,  3.3669e-01,\n",
       "            -2.1749e+00, -7.4902e-01],\n",
       "           ...,\n",
       "           [ 4.5759e-01,  2.3724e-01,  5.7196e-01,  ..., -8.1035e-01,\n",
       "            -6.3558e-01,  3.6746e-01],\n",
       "           [ 4.6035e-02,  5.1413e-01,  6.6563e-01,  ..., -1.5007e+00,\n",
       "            -1.8590e+00, -2.7797e-02],\n",
       "           [-3.7249e-01, -1.0292e+00,  7.2798e-01,  ..., -1.2397e+00,\n",
       "            -2.9566e-01,  6.3882e-01]]], device='cuda:0',\n",
       "         grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 0.5036,  1.3343,  2.2915,  ...,  0.4897, -0.8637,  0.7505],\n",
       "           [-0.3369,  0.3760,  1.2563,  ...,  0.7439, -0.5111,  1.6586],\n",
       "           [ 0.3377, -0.8933,  0.4343,  ...,  0.3909, -1.2088,  0.5846],\n",
       "           ...,\n",
       "           [ 0.7677,  0.1726,  0.9055,  ...,  0.5196, -2.0359,  0.8450],\n",
       "           [ 1.3713,  0.8064, -0.7423,  ..., -0.8071, -1.1332,  1.7233],\n",
       "           [ 0.2532,  0.1298,  0.7612,  ..., -0.4669, -0.9195,  1.8101]],\n",
       "  \n",
       "          [[ 0.3298,  2.0529,  1.7444,  ...,  0.2364, -0.5095,  0.0549],\n",
       "           [ 0.8451,  2.2659,  1.7635,  ..., -0.1310, -0.8547,  0.8820],\n",
       "           [-0.0701,  0.1153, -0.6650,  ...,  1.0040, -0.5173,  0.7346],\n",
       "           ...,\n",
       "           [ 0.2139,  0.5025,  0.7337,  ..., -0.1111, -1.0306,  1.3725],\n",
       "           [ 0.5549, -0.5413, -0.5822,  ..., -0.2772, -2.0171,  1.1121],\n",
       "           [-0.2433,  0.4643,  1.2190,  ..., -0.3340, -0.0047,  0.3567]],\n",
       "  \n",
       "          [[ 0.5206,  1.3050,  2.0373,  ..., -0.1001, -0.8107,  1.2656],\n",
       "           [ 0.1883,  1.6863,  0.4184,  ...,  0.0664, -2.0936,  0.5543],\n",
       "           [-0.1370,  0.4713, -1.1164,  ...,  0.2254, -2.5610,  0.4230],\n",
       "           ...,\n",
       "           [ 0.7331,  1.5654, -0.0541,  ...,  0.4368, -0.3897,  0.9320],\n",
       "           [ 0.5099,  0.7659,  0.9963,  ...,  0.3767, -1.0626,  1.0800],\n",
       "           [-0.1957,  0.7748,  0.7021,  ..., -0.3875, -0.1284,  1.3096]],\n",
       "  \n",
       "          [[ 0.1509,  1.7578,  1.4016,  ...,  0.4142,  0.6711,  0.9249],\n",
       "           [ 0.1292,  1.4564,  0.9850,  ...,  0.1624, -1.7835,  1.2420],\n",
       "           [-0.9954,  0.5593, -0.6362,  ..., -0.4966, -1.7810,  1.1744],\n",
       "           ...,\n",
       "           [ 0.6177, -0.0275,  0.4167,  ...,  0.6382, -1.1735, -0.6282],\n",
       "           [ 1.5144,  0.4823, -0.4387,  ..., -0.5402, -1.3781,  1.1199],\n",
       "           [ 0.6414,  0.2853, -0.1515,  ..., -0.2260, -0.3170,  1.4188]],\n",
       "  \n",
       "          [[ 1.2430,  1.9783,  1.8446,  ...,  0.1852, -0.6077,  0.4615],\n",
       "           [ 0.7172,  1.3740,  1.4441,  ...,  0.1653, -1.0543,  0.8717],\n",
       "           [ 0.3141, -0.6867, -1.4908,  ...,  0.2504, -1.6017,  1.0800],\n",
       "           ...,\n",
       "           [ 1.1879,  1.5636,  1.1555,  ...,  0.3195, -0.1666,  0.7630],\n",
       "           [-0.0098,  0.1541,  0.7836,  ..., -1.0469, -2.0582,  1.2124],\n",
       "           [ 0.2631,  0.8773,  0.7178,  ..., -0.6827, -0.2647,  0.8990]],\n",
       "  \n",
       "          [[ 1.0816,  2.0355,  1.0764,  ...,  0.6582, -0.3940,  1.0747],\n",
       "           [ 0.7677,  2.2258,  1.1641,  ...,  0.7377,  0.9285,  2.0472],\n",
       "           [-0.1488, -0.2939, -0.2365,  ...,  0.3672, -1.9431, -0.0534],\n",
       "           ...,\n",
       "           [ 0.7874,  0.7159,  0.8167,  ..., -0.3176, -0.5067,  1.5597],\n",
       "           [ 0.6156,  0.7867,  0.4541,  ..., -0.9061, -2.0164,  1.0416],\n",
       "           [ 0.2042, -0.3247,  0.5998,  ..., -0.6528, -0.0935,  1.6813]]],\n",
       "         device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 1.0408,  1.3304,  1.5828,  ...,  1.0208, -1.1174,  0.6957],\n",
       "           [-0.2638,  0.2906,  0.6199,  ...,  0.9296, -0.7833,  1.2857],\n",
       "           [ 0.4494, -0.3240,  0.2555,  ...,  0.7085, -1.6253,  0.3557],\n",
       "           ...,\n",
       "           [ 1.2180, -0.2411,  0.3943,  ...,  1.0293, -1.6531,  0.8970],\n",
       "           [ 1.3161,  0.9134, -1.0017,  ..., -0.2305, -0.8387,  1.3638],\n",
       "           [ 0.3755, -0.1931,  0.1778,  ...,  0.3285, -1.3082,  1.2689]],\n",
       "  \n",
       "          [[ 0.5420,  2.4424,  0.9394,  ...,  0.8352, -0.8092, -0.0745],\n",
       "           [ 0.7989,  2.0993,  1.2753,  ..., -0.0339, -0.9009,  1.1105],\n",
       "           [ 0.0442,  0.3135, -0.8819,  ...,  1.4329, -1.0009,  0.4853],\n",
       "           ...,\n",
       "           [ 0.2742,  0.2061,  0.1519,  ...,  0.4428, -1.1466,  0.8393],\n",
       "           [ 0.8188, -0.5789, -1.2393,  ...,  0.1719, -1.6927,  0.9001],\n",
       "           [-0.1646,  0.3580,  0.6001,  ...,  0.1817, -0.1957, -0.2794]],\n",
       "  \n",
       "          [[ 0.8523,  1.4036,  1.3577,  ...,  0.4645, -1.0662,  1.0709],\n",
       "           [ 0.3970,  1.2536,  0.2948,  ...,  0.2880, -2.0708,  0.1486],\n",
       "           [ 0.1902,  0.2035, -1.0802,  ...,  0.3257, -2.7268,  0.0891],\n",
       "           ...,\n",
       "           [ 1.2255,  1.1369, -0.7140,  ...,  0.9780, -0.5535,  0.7838],\n",
       "           [ 0.8356,  0.4380,  0.8110,  ...,  0.5075, -0.7995,  1.2092],\n",
       "           [ 0.2310,  0.5983,  0.0261,  ...,  0.0653, -0.2957,  0.7140]],\n",
       "  \n",
       "          [[ 0.4803,  1.7241,  0.5684,  ...,  1.1686,  0.0362,  0.9066],\n",
       "           [ 0.5077,  1.0586,  0.1483,  ...,  0.4702, -1.5077,  1.1574],\n",
       "           [-0.7994,  0.5614, -1.1157,  ..., -0.6289, -2.3639,  0.8857],\n",
       "           ...,\n",
       "           [ 1.0736, -0.3196, -0.1023,  ...,  0.9753, -0.9172, -0.6815],\n",
       "           [ 1.6575,  0.5181, -0.5190,  ..., -0.0122, -1.2031,  1.3001],\n",
       "           [ 0.6054,  0.2530, -0.7713,  ...,  0.3320, -0.5078,  1.1452]],\n",
       "  \n",
       "          [[ 1.5102,  2.1498,  1.5685,  ...,  0.8092, -0.9029,  0.5306],\n",
       "           [ 0.6963,  1.2963,  0.5388,  ...,  0.4987, -0.7725,  0.6974],\n",
       "           [ 0.6337, -0.1944, -1.4330,  ...,  0.4492, -1.7104,  0.8529],\n",
       "           ...,\n",
       "           [ 1.4733,  1.2626,  0.2504,  ...,  0.4988, -0.3665,  0.2408],\n",
       "           [ 0.4916, -0.0647,  0.5978,  ..., -0.8604, -2.1540,  0.6512],\n",
       "           [ 0.3427,  0.8539, -0.4601,  ..., -0.0839, -0.2792,  0.5028]],\n",
       "  \n",
       "          [[ 0.9662,  2.1613,  0.3198,  ...,  1.0638, -0.1967,  1.1206],\n",
       "           [ 1.2418,  2.0670,  0.9868,  ...,  0.8475,  0.6459,  1.5404],\n",
       "           [ 0.0500, -0.0332, -0.3712,  ...,  0.3884, -1.5580, -0.5517],\n",
       "           ...,\n",
       "           [ 0.9367,  0.3671,  0.0292,  ..., -0.0813, -1.0181,  0.9923],\n",
       "           [ 1.0050,  0.5316, -0.1667,  ..., -0.4775, -2.2413,  0.5298],\n",
       "           [ 0.2996, -0.6661,  0.4672,  ...,  0.0553, -0.2825,  0.9234]]],\n",
       "         device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 1.1879,  1.6956,  1.5295,  ...,  1.1645, -1.2939, -0.5420],\n",
       "           [-0.1938,  0.3272,  0.5689,  ...,  0.6618, -1.1720, -0.1478],\n",
       "           [ 0.3814, -0.3212,  0.4163,  ...,  1.3240, -2.1392, -0.6307],\n",
       "           ...,\n",
       "           [ 1.7400, -0.1088,  0.1196,  ...,  0.9462, -1.7047, -0.2953],\n",
       "           [ 0.7037,  1.1332, -0.8778,  ..., -0.0358, -1.5961,  0.6225],\n",
       "           [ 0.9709, -0.0574,  0.1168,  ...,  0.6803, -1.4066,  0.3754]],\n",
       "  \n",
       "          [[ 0.4026,  3.1115,  0.5018,  ...,  0.8575, -1.1842, -0.0874],\n",
       "           [ 0.8678,  1.9321,  0.9425,  ..., -0.4090, -0.9240,  0.9872],\n",
       "           [-0.0477,  0.3775, -0.4963,  ...,  1.6423, -1.6146, -0.4779],\n",
       "           ...,\n",
       "           [ 0.4464,  0.1677,  0.1662,  ...,  0.7701, -1.1878,  0.0250],\n",
       "           [ 0.7927, -0.3658, -0.7922,  ...,  0.0656, -1.9280, -0.0962],\n",
       "           [-0.1237,  0.7126,  0.5812,  ...,  0.6309,  0.0880, -0.1709]],\n",
       "  \n",
       "          [[ 0.8913,  1.5031,  0.8613,  ...,  0.8619, -0.9360, -0.1701],\n",
       "           [ 0.2968,  1.1607,  0.2531,  ...,  0.3738, -2.1780, -0.7763],\n",
       "           [ 0.0289,  0.5228, -0.9512,  ...,  0.4116, -2.8004, -0.9638],\n",
       "           ...,\n",
       "           [ 1.4046,  1.2170, -0.6353,  ...,  0.8725, -1.1634, -0.3659],\n",
       "           [ 0.5075,  0.3546,  0.8646,  ...,  0.7858, -1.3375,  0.0259],\n",
       "           [ 0.4270,  0.5436, -0.1888,  ...,  0.3990,  0.1038, -0.2607]],\n",
       "  \n",
       "          [[ 0.3896,  2.0768,  0.3360,  ...,  1.3203, -0.4131, -0.2732],\n",
       "           [ 0.4141,  0.9144, -0.0258,  ...,  0.4476, -1.7128, -0.1366],\n",
       "           [-0.6490,  0.6810, -1.2959,  ..., -0.2691, -2.3660,  0.0964],\n",
       "           ...,\n",
       "           [ 0.9900, -0.2817, -0.3933,  ...,  0.4753, -1.3152, -0.5388],\n",
       "           [ 1.2204,  0.3925, -0.6986,  ..., -0.1138, -1.6272,  1.2981],\n",
       "           [ 0.5785,  0.6347, -0.8538,  ...,  0.4902, -0.7069,  0.5390]],\n",
       "  \n",
       "          [[ 1.5984,  2.4402,  1.1508,  ...,  1.2984, -1.0718, -0.7709],\n",
       "           [ 0.4529,  1.1762,  0.0567,  ...,  0.7556, -1.1210, -0.2609],\n",
       "           [ 0.6889, -0.1873, -1.5039,  ...,  0.4410, -1.7521, -0.1058],\n",
       "           ...,\n",
       "           [ 1.5881,  0.9504,  0.1087,  ...,  0.2000, -0.1714, -0.7359],\n",
       "           [ 0.3169, -0.4401,  0.2416,  ..., -0.2970, -2.3652,  0.0862],\n",
       "           [ 0.3088,  1.0316, -0.3082,  ...,  0.0890, -0.1803, -0.3235]],\n",
       "  \n",
       "          [[ 0.9418,  2.9326,  0.1034,  ...,  1.4368, -0.7340, -0.0348],\n",
       "           [ 1.0851,  1.9091,  0.8440,  ...,  0.8334,  0.2262,  0.2973],\n",
       "           [-0.4619,  0.0167, -0.4277,  ...,  0.6027, -1.9888, -0.4948],\n",
       "           ...,\n",
       "           [ 1.1779,  0.1783, -0.3085,  ..., -0.6450, -0.9755, -0.1550],\n",
       "           [ 0.4660,  0.5920, -0.5210,  ..., -0.3394, -2.6411, -0.1758],\n",
       "           [ 0.1453, -0.4666,  0.5650,  ...,  0.2087, -0.0824, -0.0495]]],\n",
       "         device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 1.5430,  1.1094,  1.7393,  ...,  0.7603, -0.8234, -0.2571],\n",
       "           [ 0.7165,  0.0416,  0.5609,  ...,  0.8484, -0.6784,  0.1543],\n",
       "           [ 1.0735, -0.1853,  1.0854,  ...,  1.6248, -1.7859, -0.2453],\n",
       "           ...,\n",
       "           [ 2.2818, -0.6062,  0.2214,  ...,  1.6209, -1.2331, -0.1835],\n",
       "           [ 1.0426,  1.0509, -1.0078,  ...,  0.3215, -1.3648,  0.6993],\n",
       "           [ 1.3890, -0.6916,  0.1171,  ...,  1.4655, -0.7520,  0.1676]],\n",
       "  \n",
       "          [[ 0.6149,  2.7440,  0.5957,  ...,  0.9790, -1.0326,  0.0794],\n",
       "           [ 1.9932,  1.8717,  0.6404,  ..., -0.5860, -0.2786,  1.2113],\n",
       "           [-0.0920, -0.1331, -0.4965,  ...,  1.2226, -1.4776, -0.5317],\n",
       "           ...,\n",
       "           [ 0.8267, -0.2954,  0.2240,  ...,  1.1308, -0.9153,  0.1107],\n",
       "           [ 1.6447, -0.4718, -0.8312,  ...,  0.4876, -1.2188, -0.4454],\n",
       "           [ 0.4210,  0.5444,  0.4581,  ...,  0.9123,  0.5203, -0.3152]],\n",
       "  \n",
       "          [[ 1.1335,  1.3812,  1.3272,  ...,  1.0768, -0.8110,  0.2521],\n",
       "           [ 1.1036,  1.0855,  0.0154,  ...,  0.4446, -0.8369, -0.4852],\n",
       "           [ 0.3963,  0.3663, -0.8050,  ...,  0.1887, -2.3464, -0.9993],\n",
       "           ...,\n",
       "           [ 1.9610,  0.6399, -0.5509,  ...,  0.8910, -0.4715, -0.5389],\n",
       "           [ 1.4970,  0.2443,  0.7182,  ...,  1.1502, -1.1430,  0.0138],\n",
       "           [ 1.0006,  0.4544, -0.4032,  ...,  0.6157,  0.9880, -0.4901]],\n",
       "  \n",
       "          [[ 0.8554,  1.8916,  0.9622,  ...,  1.4552,  0.3254, -0.2156],\n",
       "           [ 0.9708,  0.9395,  0.1425,  ...,  0.4041, -0.9256,  0.5918],\n",
       "           [ 0.0337,  0.7260, -1.2639,  ...,  0.1496, -1.7144, -0.0172],\n",
       "           ...,\n",
       "           [ 1.7884, -0.6378, -0.1098,  ...,  1.1591, -1.3753, -0.4253],\n",
       "           [ 1.8448,  0.0871, -0.4917,  ...,  0.3025, -1.1427,  1.2050],\n",
       "           [ 1.2012, -0.0418, -1.1601,  ...,  1.4011, -0.6198,  0.5808]],\n",
       "  \n",
       "          [[ 1.9935,  2.1129,  1.6006,  ...,  1.4099, -0.4085, -0.5090],\n",
       "           [ 1.4106,  0.8822, -0.1706,  ...,  0.7518, -0.6812,  0.3046],\n",
       "           [ 1.1452, -0.6678, -1.2493,  ...,  0.3666, -1.3061, -0.0875],\n",
       "           ...,\n",
       "           [ 2.0916,  0.7123, -0.2862,  ...,  0.3475, -0.1886, -0.7202],\n",
       "           [ 0.5338,  0.0285,  0.3606,  ...,  0.0537, -1.4115, -0.2926],\n",
       "           [ 0.4647,  0.6512, -0.6636,  ...,  0.3164,  0.3411, -0.3465]],\n",
       "  \n",
       "          [[ 1.3860,  2.7017,  0.5813,  ...,  1.2934, -0.2403,  0.1883],\n",
       "           [ 1.8514,  1.8710,  0.3052,  ...,  0.8383,  0.8739,  0.3703],\n",
       "           [-0.0774,  0.0031, -0.1155,  ...,  0.6603, -1.7829, -0.4477],\n",
       "           ...,\n",
       "           [ 1.4544,  0.1095, -0.4491,  ..., -0.5504, -0.6708, -0.0050],\n",
       "           [ 0.6869,  0.5423, -0.8880,  ...,  0.0179, -1.6786, -0.3599],\n",
       "           [ 0.5070, -0.7132,  0.3331,  ...,  0.5841, -0.0498,  0.0234]]],\n",
       "         device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       "  tensor([[[ 1.7437,  0.9291,  1.2363,  ...,  1.0962,  0.7286, -0.3223],\n",
       "           [ 1.2185,  0.3670,  0.5582,  ...,  0.8326,  0.1728, -0.2502],\n",
       "           [ 1.6449,  0.0837,  0.7441,  ...,  2.2655, -1.0283, -0.6514],\n",
       "           ...,\n",
       "           [ 2.3095, -0.4218, -0.2199,  ...,  2.2289, -0.5987, -0.3178],\n",
       "           [ 1.4366,  1.5790, -1.5793,  ...,  0.5487, -0.8128,  0.2379],\n",
       "           [ 1.3789, -0.1577, -0.0923,  ...,  1.7618, -0.0452, -0.0863]],\n",
       "  \n",
       "          [[ 1.5035,  3.0063,  0.0811,  ...,  1.2835, -0.4432, -0.2661],\n",
       "           [ 2.1502,  1.5924,  0.1841,  ..., -0.1408,  0.2129,  0.5527],\n",
       "           [ 0.2781,  0.0848, -0.6194,  ...,  1.3902, -0.7994, -0.9657],\n",
       "           ...,\n",
       "           [ 1.5841, -0.5406, -0.3822,  ...,  1.6574, -0.0868, -0.0596],\n",
       "           [ 2.2570, -0.5259, -1.0676,  ...,  0.9329, -0.8196, -0.1864],\n",
       "           [ 1.2071,  0.7652,  0.2261,  ...,  0.9018,  1.1688,  0.2020]],\n",
       "  \n",
       "          [[ 1.1645,  1.2092,  0.8128,  ...,  1.3815,  0.6917,  0.1494],\n",
       "           [ 1.5661,  0.8735, -0.2360,  ...,  0.4084,  0.0637, -0.7019],\n",
       "           [ 0.9183,  0.1204, -1.1793,  ...,  0.3396, -1.1864, -1.3715],\n",
       "           ...,\n",
       "           [ 2.0713,  0.5496, -0.5515,  ...,  1.0898, -0.0632, -0.6474],\n",
       "           [ 1.9098,  0.4989,  0.3510,  ...,  1.1181, -0.3002,  0.1333],\n",
       "           [ 1.2730,  0.5083, -0.5589,  ...,  1.0833,  1.5501, -0.2063]],\n",
       "  \n",
       "          [[ 1.5644,  1.6201,  0.3086,  ...,  1.1380,  0.6821, -0.5561],\n",
       "           [ 1.4111,  1.0047, -0.1343,  ...,  0.8617, -0.4372, -0.2143],\n",
       "           [ 0.7011,  0.6653, -1.2887,  ...,  0.2640, -1.1604, -0.6495],\n",
       "           ...,\n",
       "           [ 2.0072, -0.4379, -0.7296,  ...,  1.6291, -0.6176, -0.3139],\n",
       "           [ 2.1090,  0.3966, -0.6725,  ...,  0.5035, -0.7350,  0.7530],\n",
       "           [ 1.4192,  0.2387, -0.9620,  ...,  1.5604, -0.0473,  0.4950]],\n",
       "  \n",
       "          [[ 2.5398,  1.8408,  1.0391,  ...,  1.3020,  0.3097, -0.7528],\n",
       "           [ 1.4975,  0.9765, -0.3492,  ...,  0.5072, -0.0930,  0.1684],\n",
       "           [ 1.4687, -0.1963, -1.6261,  ...,  0.6827, -0.9800, -0.1431],\n",
       "           ...,\n",
       "           [ 2.2167,  0.3349, -0.9923,  ...,  0.8021,  0.7962, -0.9635],\n",
       "           [ 1.1162,  0.1600,  0.0384,  ...,  0.2665, -0.8221, -0.4178],\n",
       "           [ 0.8010,  0.7481, -0.8811,  ...,  0.6862,  1.0965,  0.1976]],\n",
       "  \n",
       "          [[ 1.7403,  2.2043,  0.1103,  ...,  1.0970,  1.0819,  0.3170],\n",
       "           [ 2.3592,  2.0170,  0.1662,  ...,  1.0238,  1.2599, -0.2754],\n",
       "           [ 0.8341, -0.2357, -0.6144,  ...,  0.9599, -1.1360, -0.7352],\n",
       "           ...,\n",
       "           [ 2.1485,  0.3350, -0.7321,  ...,  0.0061,  0.2252, -0.6812],\n",
       "           [ 1.5797,  0.3730, -1.0719,  ..., -0.1745, -0.6588, -0.8224],\n",
       "           [ 0.8804, -0.3427, -0.0752,  ...,  0.8811,  0.3933,  0.1118]]],\n",
       "         device='cuda:0', grad_fn=<NativeLayerNormBackward>)))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[1]),outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 200, 768]),\n",
       " tensor([[[ 1.7437,  0.9291,  1.2363,  ...,  1.0962,  0.7286, -0.3223],\n",
       "          [ 1.2185,  0.3670,  0.5582,  ...,  0.8326,  0.1728, -0.2502],\n",
       "          [ 1.6449,  0.0837,  0.7441,  ...,  2.2655, -1.0283, -0.6514],\n",
       "          ...,\n",
       "          [ 2.3095, -0.4218, -0.2199,  ...,  2.2289, -0.5987, -0.3178],\n",
       "          [ 1.4366,  1.5790, -1.5793,  ...,  0.5487, -0.8128,  0.2379],\n",
       "          [ 1.3789, -0.1577, -0.0923,  ...,  1.7618, -0.0452, -0.0863]],\n",
       " \n",
       "         [[ 1.5035,  3.0063,  0.0811,  ...,  1.2835, -0.4432, -0.2661],\n",
       "          [ 2.1502,  1.5924,  0.1841,  ..., -0.1408,  0.2129,  0.5527],\n",
       "          [ 0.2781,  0.0848, -0.6194,  ...,  1.3902, -0.7994, -0.9657],\n",
       "          ...,\n",
       "          [ 1.5841, -0.5406, -0.3822,  ...,  1.6574, -0.0868, -0.0596],\n",
       "          [ 2.2570, -0.5259, -1.0676,  ...,  0.9329, -0.8196, -0.1864],\n",
       "          [ 1.2071,  0.7652,  0.2261,  ...,  0.9018,  1.1688,  0.2020]],\n",
       " \n",
       "         [[ 1.1645,  1.2092,  0.8128,  ...,  1.3815,  0.6917,  0.1494],\n",
       "          [ 1.5661,  0.8735, -0.2360,  ...,  0.4084,  0.0637, -0.7019],\n",
       "          [ 0.9183,  0.1204, -1.1793,  ...,  0.3396, -1.1864, -1.3715],\n",
       "          ...,\n",
       "          [ 2.0713,  0.5496, -0.5515,  ...,  1.0898, -0.0632, -0.6474],\n",
       "          [ 1.9098,  0.4989,  0.3510,  ...,  1.1181, -0.3002,  0.1333],\n",
       "          [ 1.2730,  0.5083, -0.5589,  ...,  1.0833,  1.5501, -0.2063]],\n",
       " \n",
       "         [[ 1.5644,  1.6201,  0.3086,  ...,  1.1380,  0.6821, -0.5561],\n",
       "          [ 1.4111,  1.0047, -0.1343,  ...,  0.8617, -0.4372, -0.2143],\n",
       "          [ 0.7011,  0.6653, -1.2887,  ...,  0.2640, -1.1604, -0.6495],\n",
       "          ...,\n",
       "          [ 2.0072, -0.4379, -0.7296,  ...,  1.6291, -0.6176, -0.3139],\n",
       "          [ 2.1090,  0.3966, -0.6725,  ...,  0.5035, -0.7350,  0.7530],\n",
       "          [ 1.4192,  0.2387, -0.9620,  ...,  1.5604, -0.0473,  0.4950]],\n",
       " \n",
       "         [[ 2.5398,  1.8408,  1.0391,  ...,  1.3020,  0.3097, -0.7528],\n",
       "          [ 1.4975,  0.9765, -0.3492,  ...,  0.5072, -0.0930,  0.1684],\n",
       "          [ 1.4687, -0.1963, -1.6261,  ...,  0.6827, -0.9800, -0.1431],\n",
       "          ...,\n",
       "          [ 2.2167,  0.3349, -0.9923,  ...,  0.8021,  0.7962, -0.9635],\n",
       "          [ 1.1162,  0.1600,  0.0384,  ...,  0.2665, -0.8221, -0.4178],\n",
       "          [ 0.8010,  0.7481, -0.8811,  ...,  0.6862,  1.0965,  0.1976]],\n",
       " \n",
       "         [[ 1.7403,  2.2043,  0.1103,  ...,  1.0970,  1.0819,  0.3170],\n",
       "          [ 2.3592,  2.0170,  0.1662,  ...,  1.0238,  1.2599, -0.2754],\n",
       "          [ 0.8341, -0.2357, -0.6144,  ...,  0.9599, -1.1360, -0.7352],\n",
       "          ...,\n",
       "          [ 2.1485,  0.3350, -0.7321,  ...,  0.0061,  0.2252, -0.6812],\n",
       "          [ 1.5797,  0.3730, -1.0719,  ..., -0.1745, -0.6588, -0.8224],\n",
       "          [ 0.8804, -0.3427, -0.0752,  ...,  0.8811,  0.3933,  0.1118]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].size(),outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(5, shuffle=True, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==3.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045933d9b2384801bc765240b395def7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=647.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e9558b4c3c43e1b4dfd43520bb899b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=109540.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a16048570584b58b7dca0211cff33bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593cb2ade86d42f8b510486a5058848c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1815f18af64ffeb760e0b4b22c979e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=19.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-bert-wwm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'hfl/chinese-bert-wwm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-30 17:20:16.027 | INFO     | __main__:convert_examples_to_features:31 - Writing example %d of %d\n",
      "2020-09-30 17:20:21.302 | INFO     | __main__:convert_examples_to_features:31 - Writing example %d of %d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b37edd57fcf46a88ba707870141505d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411578458.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm were not used when initializing RobertaForTokenClassification: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-30 17:21:18 34s epoch: 0; train loss: 194.565673828125; val loss: 36.21962356567383\n",
      "metrics:{'precision': 0.07949977668602054, 'recall': 0.099302649930265, 'f1': 0.08830460126503782}\n",
      "2020-09-30 17:21:53 35s epoch: 1; train loss: 138.8235321044922; val loss: 33.32533264160156\n",
      "metrics:{'precision': 0.10346611484738748, 'recall': 0.16736401673640167, 'f1': 0.12787723785166238}\n",
      "2020-09-30 17:22:29 35s epoch: 2; train loss: 124.23117065429688; val loss: 31.883255004882812\n",
      "metrics:{'precision': 0.09140182310914018, 'recall': 0.10348675034867504, 'f1': 0.09706959706959707}\n",
      "2020-09-30 17:23:04 35s epoch: 3; train loss: 116.52886962890625; val loss: 30.178844451904297\n",
      "metrics:{'precision': 0.11733258300506472, 'recall': 0.23263598326359833, 'f1': 0.15598989993453663}\n"
     ]
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(examples)):\n",
    "#     print(trn_idx, val_idx)\n",
    "    examples_train = pd.Series(examples).iloc[trn_idx].tolist()\n",
    "    examples_val = pd.Series(examples).iloc[val_idx].tolist()\n",
    "    trainset = NerDataset(examples_train, tokenizer, labels, config.model_type, max_seq_length)\n",
    "    train_data_loader = DataLoader(trainset, batch_size=6, shuffle=True,collate_fn=default_data_collator)\n",
    "    valset = NerDataset(examples_val, tokenizer, labels, config.model_type, max_seq_length)\n",
    "    val_data_loader = DataLoader(valset, batch_size=6, shuffle=True,collate_fn=default_data_collator)\n",
    "    \n",
    "#     model = MyBertForTokenClassification(config)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name_or_path, \n",
    "                                                            config = config)\n",
    "    model = model.to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \n",
    "    time_start = datetime.now()\n",
    "    \n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        losst = 0\n",
    "        for step, inputs in enumerate(train_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "        #     print(step)\n",
    "        #     print(inputs)\n",
    "            for k, v in inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to('cuda')\n",
    "#             print(inputs)\n",
    "#             break\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            losst += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             if step % 100 == 0:\n",
    "#                 print(epoch, step, loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            lossv = 0\n",
    "            preds: torch.Tensor = None\n",
    "            label_ids: torch.Tensor = None\n",
    "            for stepv, inputsv in enumerate(val_data_loader):\n",
    "                for k, v in inputsv.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        inputsv[k] = v.to('cuda')\n",
    "                outputs = model(**inputsv)\n",
    "                lossv += outputs[0] \n",
    "                logits = outputs[1]\n",
    "                preds = logits if preds is None else nested_concat(preds, logits, dim=0)\n",
    "                label_ids = inputsv['labels'] if label_ids is None else nested_concat(label_ids, inputsv['labels'], dim=0)\n",
    "            preds = nested_numpify(preds)\n",
    "            label_ids = nested_numpify(label_ids)\n",
    "            metrics = compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\n",
    "            now = datetime.now()\n",
    "            print(f'{now.strftime(\"%Y-%m-%d %H:%M:%S\")} {int((now-time_start).total_seconds())}s epoch: {epoch}; train loss: {losst}; val loss: {lossv}\\nmetrics:{metrics}')\n",
    "        model.train()\n",
    "        time_start = now\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-30 16:58:08.923 | INFO     | __main__:convert_examples_to_features:31 - Writing example %d of %d\n",
      "2020-09-30 16:58:13.903 | INFO     | __main__:convert_examples_to_features:31 - Writing example %d of %d\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MyBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing MyBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-30 16:58:53 35s epoch: 0; train loss: 233.0487336218357; val loss: 46.893863677978516\n",
      "metrics:{'precision': 0.15670969678775143, 'recall': 0.14560669456066946, 'f1': 0.15095430884904568}\n",
      "2020-09-30 16:59:28 35s epoch: 1; train loss: 166.47629860043526; val loss: 39.51923751831055\n",
      "metrics:{'precision': 0.2821650399290151, 'recall': 0.26610878661087867, 'f1': 0.2739018087855297}\n",
      "2020-09-30 17:00:04 35s epoch: 2; train loss: 138.5117626786232; val loss: 33.81781768798828\n",
      "metrics:{'precision': 0.33513206475433116, 'recall': 0.3291492329149233, 'f1': 0.3321137067267098}\n",
      "2020-09-30 17:00:39 35s epoch: 3; train loss: 124.96450391411781; val loss: 32.58758544921875\n",
      "metrics:{'precision': 0.3378901212277534, 'recall': 0.36541143654114366, 'f1': 0.35111230233181456}\n",
      "2020-09-30 17:01:15 35s epoch: 4; train loss: 115.32277190685272; val loss: 31.041973114013672\n",
      "metrics:{'precision': 0.3672627235213205, 'recall': 0.3723849372384937, 'f1': 0.3698060941828255}\n",
      "2020-09-30 17:01:51 35s epoch: 5; train loss: 108.80313205718994; val loss: 29.711118698120117\n",
      "metrics:{'precision': 0.38501144164759726, 'recall': 0.37545327754532776, 'f1': 0.3801722920491456}\n",
      "2020-09-30 17:02:26 35s epoch: 6; train loss: 103.03407727181911; val loss: 29.864734649658203\n",
      "metrics:{'precision': 0.38737151248164464, 'recall': 0.3679218967921897, 'f1': 0.3773962804005722}\n",
      "2020-09-30 17:03:02 35s epoch: 7; train loss: 99.50326013565063; val loss: 29.451154708862305\n",
      "metrics:{'precision': 0.3717723740285786, 'recall': 0.4136680613668061, 'f1': 0.3916028518616319}\n",
      "2020-09-30 17:03:38 35s epoch: 8; train loss: 94.82470512390137; val loss: 29.462955474853516\n",
      "metrics:{'precision': 0.3889325990034094, 'recall': 0.4136680613668061, 'f1': 0.400919167342525}\n",
      "2020-09-30 17:04:13 35s epoch: 9; train loss: 90.71812663972378; val loss: 29.79369354248047\n",
      "metrics:{'precision': 0.3476592478894858, 'recall': 0.3790794979079498, 'f1': 0.36269015212169736}\n",
      "2020-09-30 17:04:49 35s epoch: 10; train loss: 87.38411067426205; val loss: 29.479978561401367\n",
      "metrics:{'precision': 0.37698986975397974, 'recall': 0.43598326359832634, 'f1': 0.4043461389212263}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-eac0c90bfe4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_numpify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mlabel_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_numpify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{now.strftime(\"%Y-%m-%d %H:%M:%S\")} {int((now-time_start).total_seconds())}s epoch: {epoch}; train loss: {losst}; val loss: {lossv}\\nmetrics:{metrics}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-eb0ec762a128>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEvalPrediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpreds_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_label_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#     print(preds_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     print(out_label_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     return {\n",
      "\u001b[0;32m<ipython-input-13-b5def7813cdb>\u001b[0m in \u001b[0;36malign_predictions\u001b[0;34m(predictions, label_ids)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mout_label_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mpreds_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m    941\u001b[0m     def __init__(self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,\n\u001b[1;32m    942\u001b[0m                  reduce=None, reduction: str = 'mean') -> None:\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(examples)):\n",
    "#     print(trn_idx, val_idx)\n",
    "    examples_train = pd.Series(examples).iloc[trn_idx].tolist()\n",
    "    examples_val = pd.Series(examples).iloc[val_idx].tolist()\n",
    "    trainset = NerDataset(examples_train, tokenizer, labels, config.model_type, max_seq_length)\n",
    "    train_data_loader = DataLoader(trainset, batch_size=6, shuffle=True,collate_fn=default_data_collator)\n",
    "    valset = NerDataset(examples_val, tokenizer, labels, config.model_type, max_seq_length)\n",
    "    val_data_loader = DataLoader(valset, batch_size=6, shuffle=True,collate_fn=default_data_collator)\n",
    "    \n",
    "    model = MyBertForTokenClassification.from_pretrained(model_name_or_path, config=config)\n",
    "    \n",
    "    model = model.to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \n",
    "    time_start = datetime.now()\n",
    "    \n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        losst = 0\n",
    "        model.train()\n",
    "        for step, inputs in enumerate(train_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "        #     print(step)\n",
    "        #     print(inputs)\n",
    "            for k, v in inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to('cuda')\n",
    "#             print(inputs)\n",
    "#             break\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            losst += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             if step % 100 == 0:\n",
    "#                 print(epoch, step, loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            lossv = 0\n",
    "            preds: torch.Tensor = None\n",
    "            label_ids: torch.Tensor = None\n",
    "            for stepv, inputsv in enumerate(val_data_loader):\n",
    "                for k, v in inputsv.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        inputsv[k] = v.to('cuda')\n",
    "                outputs = model(**inputsv)\n",
    "                lossv += outputs[0] \n",
    "                logits = outputs[1]\n",
    "                preds = logits if preds is None else nested_concat(preds, logits, dim=0)\n",
    "                label_ids = inputsv['labels'] if label_ids is None else nested_concat(label_ids, inputsv['labels'], dim=0)\n",
    "            preds = nested_numpify(preds)\n",
    "            label_ids = nested_numpify(label_ids)\n",
    "            metrics = compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\n",
    "            now = datetime.now()\n",
    "            print(f'{now.strftime(\"%Y-%m-%d %H:%M:%S\")} {int((now-time_start).total_seconds())}s epoch: {epoch}; train loss: {losst}; val loss: {lossv}\\nmetrics:{metrics}')\n",
    "        \n",
    "        time_start = now\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-30 16:14:24.209 | INFO     | __main__:convert_examples_to_features:31 - Writing example %d of %d\n",
      "2020-09-30 16:14:29.097 | INFO     | __main__:convert_examples_to_features:31 - Writing example %d of %d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-30 16:15:05 35s epoch: 0; train loss: 199.09524354338646; val loss: 52.30827713012695\n",
      "metrics:{'precision': 0.0, 'recall': 0.0, 'f1': 0}\n",
      "2020-09-30 16:15:40 35s epoch: 1; train loss: 197.51129357516766; val loss: 52.45339584350586\n",
      "metrics:{'precision': 0.0, 'recall': 0.0, 'f1': 0}\n",
      "2020-09-30 16:16:16 35s epoch: 2; train loss: 195.32498371601105; val loss: 52.4373664855957\n",
      "metrics:{'precision': 0.01778496362166532, 'recall': 0.006136680613668061, 'f1': 0.009124844462878473}\n",
      "2020-09-30 16:16:51 35s epoch: 3; train loss: 193.34185588359833; val loss: 51.69658279418945\n",
      "metrics:{'precision': 0.012589928057553957, 'recall': 0.003905160390516039, 'f1': 0.005961251862891207}\n",
      "2020-09-30 16:17:27 35s epoch: 4; train loss: 190.9053521603346; val loss: 51.945770263671875\n",
      "metrics:{'precision': 0.029885057471264367, 'recall': 0.010878661087866108, 'f1': 0.01595092024539877}\n",
      "2020-09-30 16:18:03 35s epoch: 5; train loss: 189.11482363939285; val loss: 52.24580764770508\n",
      "metrics:{'precision': 0.018038331454340473, 'recall': 0.004463040446304045, 'f1': 0.007155635062611808}\n",
      "2020-09-30 16:18:38 35s epoch: 6; train loss: 188.58023875951767; val loss: 51.35285186767578\n",
      "metrics:{'precision': 0.011483253588516746, 'recall': 0.0033472803347280333, 'f1': 0.005183585313174946}\n",
      "2020-09-30 16:19:14 35s epoch: 7; train loss: 186.14440089464188; val loss: 52.1237678527832\n",
      "metrics:{'precision': 0.006896551724137931, 'recall': 0.0019525801952580196, 'f1': 0.003043478260869565}\n",
      "2020-09-30 16:19:49 35s epoch: 8; train loss: 183.82936826348305; val loss: 51.42243194580078\n",
      "metrics:{'precision': 0.008684863523573201, 'recall': 0.0019525801952580196, 'f1': 0.0031883397859257577}\n",
      "2020-09-30 16:20:25 35s epoch: 9; train loss: 181.33844411373138; val loss: 51.99251937866211\n",
      "metrics:{'precision': 0.019400352733686066, 'recall': 0.006136680613668061, 'f1': 0.009324009324009324}\n",
      "2020-09-30 16:21:01 35s epoch: 10; train loss: 178.6709751188755; val loss: 52.06770706176758\n",
      "metrics:{'precision': 0.030658250676284943, 'recall': 0.009483960948396096, 'f1': 0.014486578610992758}\n",
      "2020-09-30 16:21:36 35s epoch: 11; train loss: 176.64670464396477; val loss: 51.957176208496094\n",
      "metrics:{'precision': 0.02408256880733945, 'recall': 0.005857740585774059, 'f1': 0.009423378954453668}\n",
      "2020-09-30 16:22:12 35s epoch: 12; train loss: 173.80307285487652; val loss: 52.20600509643555\n",
      "metrics:{'precision': 0.04694323144104803, 'recall': 0.01199442119944212, 'f1': 0.019106865141079763}\n",
      "2020-09-30 16:22:48 35s epoch: 13; train loss: 170.70791205763817; val loss: 51.929290771484375\n",
      "metrics:{'precision': 0.037937743190661476, 'recall': 0.010878661087866108, 'f1': 0.01690873618035985}\n",
      "2020-09-30 16:23:23 35s epoch: 14; train loss: 167.2434120774269; val loss: 52.751502990722656\n",
      "metrics:{'precision': 0.06365503080082136, 'recall': 0.017294281729428172, 'f1': 0.027198947137530155}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-852147b92a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_numpify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mlabel_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_numpify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{now.strftime(\"%Y-%m-%d %H:%M:%S\")} {int((now-time_start).total_seconds())}s epoch: {epoch}; train loss: {losst}; val loss: {lossv}\\nmetrics:{metrics}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-eb0ec762a128>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m      5\u001b[0m     return {\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_label_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;34m\"recall\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_label_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_label_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     }\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mrecall_score\u001b[0;34m(y_true, y_pred, average, suffix)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;36m0.50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \"\"\"\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mtrue_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0mpred_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mget_entities\u001b[0;34m(seq, suffix)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtype_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mend_of_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mchunks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_of_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mend_of_chunk\u001b[0;34m(prev_tag, tag, prev_type, type_)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprev_tag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'E'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchunk_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mprev_tag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'S'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchunk_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprev_tag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'B'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchunk_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(examples)):\n",
    "#     print(trn_idx, val_idx)\n",
    "    examples_train = pd.Series(examples).iloc[trn_idx].tolist()\n",
    "    examples_val = pd.Series(examples).iloc[val_idx].tolist()\n",
    "    trainset = NerDataset(examples_train, tokenizer, labels, config.model_type, max_seq_length)\n",
    "    train_data_loader = DataLoader(trainset, batch_size=6, shuffle=True,collate_fn=default_data_collator)\n",
    "    valset = NerDataset(examples_val, tokenizer, labels, config.model_type, max_seq_length)\n",
    "    val_data_loader = DataLoader(valset, batch_size=6, shuffle=True,collate_fn=default_data_collator)\n",
    "    \n",
    "#     model = MyBertForTokenClassification(config)\n",
    "\n",
    "#     model = model.to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \n",
    "    time_start = datetime.now()\n",
    "    \n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        losst = 0\n",
    "        for step, inputs in enumerate(train_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "        #     print(step)\n",
    "        #     print(inputs)\n",
    "            for k, v in inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to('cuda')\n",
    "#             print(inputs)\n",
    "#             break\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            losst += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             if step % 100 == 0:\n",
    "#                 print(epoch, step, loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            lossv = 0\n",
    "            preds: torch.Tensor = None\n",
    "            label_ids: torch.Tensor = None\n",
    "            for stepv, inputsv in enumerate(val_data_loader):\n",
    "                for k, v in inputsv.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        inputsv[k] = v.to('cuda')\n",
    "                outputs = model(**inputsv)\n",
    "                lossv += outputs[0] \n",
    "                logits = outputs[1]\n",
    "                preds = logits if preds is None else nested_concat(preds, logits, dim=0)\n",
    "                label_ids = inputsv['labels'] if label_ids is None else nested_concat(label_ids, inputsv['labels'], dim=0)\n",
    "            preds = nested_numpify(preds)\n",
    "            label_ids = nested_numpify(label_ids)\n",
    "            metrics = compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\n",
    "            now = datetime.now()\n",
    "            print(f'{now.strftime(\"%Y-%m-%d %H:%M:%S\")} {int((now-time_start).total_seconds())}s epoch: {epoch}; train loss: {losst}; val loss: {lossv}\\nmetrics:{metrics}')\n",
    "        model.train()\n",
    "        time_start = now\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stepv, inputsv in enumerate(val_data_loader):\n",
    "    for k, v in inputsv.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            inputsv[k] = v.to('cuda')\n",
    "    outputs = model(**inputsv)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7820, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 250])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputsv['labels'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 250, 27])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.2478, -0.9541,  0.3502,  ..., -1.7577,  0.8842,  1.6488],\n",
       "         [ 9.1770,  0.4352,  1.2137,  ..., -1.2776,  1.2094, -1.8445],\n",
       "         [ 9.2644,  1.6467,  1.2957,  ..., -2.3641, -0.5144,  0.7105],\n",
       "         ...,\n",
       "         [ 7.5256, -0.1242,  0.6146,  ..., -1.9995, -0.8282,  0.6976],\n",
       "         [ 6.6889, -0.8332,  0.5084,  ..., -1.7485, -0.5956,  0.9563],\n",
       "         [ 7.7120,  0.1404,  0.7082,  ..., -0.8425, -0.8540,  0.2866]],\n",
       "\n",
       "        [[ 7.6796,  0.0823,  0.4245,  ..., -1.7960,  0.1496,  0.8459],\n",
       "         [10.2698,  0.8696,  1.0195,  ..., -1.0480,  1.3240, -1.1397],\n",
       "         [ 8.5301,  1.4456,  0.8132,  ..., -2.6058, -1.2604,  1.3601],\n",
       "         ...,\n",
       "         [ 8.3323, -0.8301,  0.1058,  ..., -1.7747, -0.8403, -0.2943],\n",
       "         [ 6.8183, -0.8213,  0.8348,  ..., -1.5840, -0.7093,  0.0195],\n",
       "         [ 7.3432,  0.2935,  0.7771,  ..., -1.3801, -1.3548, -0.1197]],\n",
       "\n",
       "        [[ 7.1986, -1.1691, -1.4432,  ..., -1.6779,  0.7274,  1.7633],\n",
       "         [ 5.2698, -0.9619, -2.1792,  ..., -1.4245,  4.4317,  0.7993],\n",
       "         [ 6.1204, -1.1639, -2.5819,  ..., -1.4613,  1.6102,  4.8688],\n",
       "         ...,\n",
       "         [ 7.7425, -1.5096, -1.9409,  ..., -0.8991,  0.8212,  1.1458],\n",
       "         [ 6.4747, -0.7821, -1.7911,  ..., -0.8616,  0.3715,  1.9913],\n",
       "         [ 6.7789, -0.6608, -1.9015,  ..., -1.1793,  0.5155,  2.2708]],\n",
       "\n",
       "        [[ 7.0512,  0.7099,  1.4611,  ..., -2.6870,  0.6281,  1.3925],\n",
       "         [ 5.7982, -0.7404,  1.2001,  ..., -2.0988,  2.7872, -1.1077],\n",
       "         [ 5.9901,  0.3456,  0.7951,  ..., -3.3665, -0.6008,  3.1883],\n",
       "         ...,\n",
       "         [ 7.6440,  0.2856,  0.8570,  ..., -2.4347,  0.0346,  0.4322],\n",
       "         [ 6.8668, -0.5722,  0.9231,  ..., -2.2504, -0.5063,  0.3728],\n",
       "         [ 7.7655,  0.2953,  1.3791,  ..., -1.7066, -1.1992,  0.4855]],\n",
       "\n",
       "        [[ 7.0089,  0.5749,  2.5517,  ..., -1.0439,  1.3626,  2.6441],\n",
       "         [ 5.7525, -0.3542,  1.8794,  ..., -0.5459,  3.8998,  0.6037],\n",
       "         [ 6.4009,  0.1821,  0.9498,  ..., -1.4078,  0.5871,  4.0352],\n",
       "         ...,\n",
       "         [ 7.6398, -0.1150,  1.9901,  ..., -1.4760,  0.3776,  2.1052],\n",
       "         [ 7.4371,  0.2070,  1.6096,  ..., -0.9991,  0.5968,  2.5966],\n",
       "         [ 7.8143,  0.5921,  2.0903,  ..., -0.5915,  0.0627,  1.5131]],\n",
       "\n",
       "        [[ 7.1710,  0.0502,  0.2088,  ..., -1.3016,  2.9124,  3.4837],\n",
       "         [ 5.8288, -0.7704,  0.0936,  ..., -0.9677,  5.4989,  0.7016],\n",
       "         [ 6.1491, -1.2918, -1.2071,  ..., -1.6367,  2.3666,  5.0269],\n",
       "         ...,\n",
       "         [ 7.7550, -0.6176, -0.3845,  ..., -1.0628,  1.5962,  2.5205],\n",
       "         [ 6.3486, -0.0262,  0.0437,  ..., -0.9447,  1.6778,  3.0892],\n",
       "         [ 6.9338,  0.7684,  0.5441,  ..., -1.1843,  1.3030,  3.0190]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nested_numpify(outputs[1].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = nested_numpify(inputsv['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229 229\n",
      "229 229\n",
      "81 81\n",
      "217 217\n",
      "214 214\n",
      "88 88\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DISEASE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'O', 'O', 'O', 'I-PERSON_GROUP', 'O', 'O', 'I-PERSON_GROUP', 'O', 'O', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-SYMPTOM'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FOOD_GROUP', 'I-FOOD_GROUP', 'O', 'B-FOOD_GROUP', 'I-FOOD_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FOOD_GROUP', 'I-FOOD_GROUP', 'O', 'B-FOOD_GROUP', 'I-FOOD_GROUP', 'O', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'B-DISEASE_GROUP', 'I-DISEASE_GROUP', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'B-DISEASE_GROUP', 'I-DISEASE_GROUP', 'O', 'B-DISEASE_GROUP', 'I-DISEASE_GROUP', 'I-DISEASE_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'B-FOOD_GROUP', 'I-FOOD_GROUP', 'O', 'B-FOOD_GROUP', 'I-FOOD_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'O', 'O', 'B-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'O', 'O', 'B-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-DRUG_DOSAGE', 'I-DRUG_DOSAGE', 'I-DRUG_DOSAGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON_GROUP', 'I-PERSON_GROUP', 'O', 'O', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'B-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'I-DRUG_EFFICACY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'I-SYNDROME', 'O', 'B-SYNDROME', 'I-SYNDROME', 'B-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'B-SYMPTOM', 'I-SYMPTOM', 'O', 'B-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'I-SYMPTOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.0, 'recall': 0.0, 'f1': 0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label= {i: label for i, label in enumerate(labels)}\n",
    "label2id={label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true =inputsv['labels'].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(outputs[1].cpu().detach().numpy(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 250)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 250)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "237.15px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
