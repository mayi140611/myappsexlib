{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp weak_supervision.snorkel\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weak_supervision-snorkel\n",
    "https://github.com/snorkel-team/snorkel\n",
    "\n",
    "官网: https://www.snorkel.org/\n",
    "\n",
    "https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam 这个tutorial 把snorkel功能 覆盖的很全了 大家有空可以走一遍\n",
    "\n",
    "https://github.com/HazyResearch/snorkel-superglue  snorkel 在superglue metric上目前是SOA 说明弱监督这个方法论很牛逼了 值得去了解 而且它对机器学习工程非常友好\n",
    "\n",
    "\n",
    "\n",
    "## install\n",
    "    Installing collected packages: tensorboard, scikit-learn\n",
    "      Found existing installation: scikit-learn 0.23.1\n",
    "        Uninstalling scikit-learn-0.23.1:\n",
    "          Successfully uninstalled scikit-learn-0.23.1\n",
    "    Successfully installed scikit-learn-0.21.3 tensorboard-1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snorkel==0.9.5\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install snorkel\n",
    "!pip freeze | grep snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get-started: Programmatically Building and Managing Training Data with Snorkel\n",
    "https://www.snorkel.org/get-started/\n",
    "\n",
    "https://github.com/snorkel-team/snorkel-tutorials/tree/master/getting_started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snorkel is a system for programmatically building and managing training datasets without manual labeling. In Snorkel, users can develop large training datasets in hours or days rather than hand-labeling them over weeks or months.\n",
    "\n",
    "Snorkel currently exposes three key programmatic operations:\n",
    "\n",
    "* Labeling data, e.g., using heuristic rules or distant supervision techniques\n",
    "* Transforming data, e.g., rotating or stretching images to perform data augmentation\n",
    "* Slicing data into different critical subsets for monitoring or targeted improvement\n",
    "\n",
    "Snorkel then automatically models, cleans, and integrates the resulting training data using novel, theoretically-grounded techniques.\n",
    "![](img/snorkel01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this quick walkthrough, we’ll preview the high-level workflow and interfaces of Snorkel using a canonical machine learning problem: classifying spam. \n",
    "\n",
    "We’ll use a public YouTube comments dataset, and see how Snorkel can enable \n",
    "### training a machine learning model without any hand-labeled training data! \n",
    "For more detailed versions of the sections in this walkthrough, see the corresponding tutorials: (Spam LFs, Spam TFs, Spam SFs).\n",
    "\n",
    "We’ll walk through five basic steps:\n",
    "\n",
    "1. Writing Labeling Functions (LFs): First, rather than hand-labeling any training data, we’ll programmatically label our unlabeled dataset with LFs.\n",
    "1. Modeling & Combining LFs: Next, we’ll use Snorkel’s LabelModel to automatically learn the accuracies of our LFs and reweight and combine their outputs into a single, confidence-weighted training label per data point.\n",
    "1. Writing Transformation Functions (TFs) for Data Augmentation: Then, we’ll augment this labeled training set by writing a simple TF.\n",
    "1. Writing Slicing Functions (SFs) for Data Subset Selection: We’ll also preview writing an SF to identify a critical subset or slice of our training set.\n",
    "1. Training a final ML model: Finally, we’ll train an ML model with our training set.\n",
    "\n",
    "We’ll start first by loading the unlabeled comments, which we’ll use as our training data, as a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Labeling Functions\n",
    "Labeling functions (LFs) are one of the core operators for building and managing training datasets programmatically in Snorkel. \n",
    "\n",
    "The basic idea is simple: a labeling function is a function that outputs a label for some subset of the training dataset. In our example here, each labeling function takes as input a comment data point, and either outputs a label (SPAM = 1 or NOT_SPAM = 0) or abstains from labeling (ABSTAIN = -1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label mappings for convenience\n",
    "ABSTAIN = -1\n",
    "NOT_SPAM = 0\n",
    "SPAM = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions can be used to represent many heuristic and/or noisy strategies for labeling data, often referred to as weak supervision. \n",
    "\n",
    "The basic idea of labeling functions, and other programmatic operators in Snorkel, is to let users inject domain information into machine learning models in higher level, higher bandwidth ways than manually labeling thousands or millions of individual data points. \n",
    "\n",
    "The key idea is that labeling functions do not need to be perfectly accurate, and can in fact even be correlated with each other. Snorkel will automatically estimate their accuracies and correlations in a provably consistent way, and then reweight and combine their output labels, leading to high-quality training labels.\n",
    "\n",
    "In our text data setting here, labeling functions use:\n",
    "\n",
    "### Keyword matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def lf_keyword_my(x):\n",
    "    \"\"\"Many spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if \"my\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def lf_regex_check_out(x):\n",
    "    \"\"\"Spam comments say 'check out my video', 'check it out', etc.\"\"\"\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrary heuristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def lf_short_comment(x):\n",
    "    \"\"\"Non-spam comments are often short, such as 'cool video!'.\"\"\"\n",
    "    return NOT_SPAM if len(x.text.split()) < 5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-party models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def lf_textblob_polarity(x):\n",
    "    \"\"\"\n",
    "    We use a third-party sentiment classification model, TextBlob.\n",
    "\n",
    "    We combine this with the heuristic that non-spam comments are often positive.\n",
    "    \"\"\"\n",
    "    return NOT_SPAM if TextBlob(x.text).sentiment.polarity > 0.3 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And much more! For many more types of labeling functions — including over data modalities beyond text — see the other tutorials and real-world examples.\n",
    "\n",
    "In general the process of developing labeling functions is, like any other development process, an iterative one that takes time. However, in many cases it can be orders-of-magnitude faster that hand-labeling training data. For more detail on the process of developing labeling functions and other training data operators in Snorkel, see the Introduction Tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining & Cleaning the Labels(核心技术和价值点)\n",
    "Our next step is to apply the labeling functions we wrote to the unlabeled training data. \n",
    "\n",
    "The result is a label matrix, L_train, where each row corresponds to a data point and each column corresponds to a labeling function. \n",
    "\n",
    "Since the labeling functions have unknown accuracies and correlations, their output labels may overlap and conflict. \n",
    "\n",
    "We use the LabelModel to automatically estimate their accuracies and correlations, reweight and combine their labels, and produce our final set of clean, integrated training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "# Define the set of labeling functions (LFs)\n",
    "lfs = [lf_keyword_my, lf_regex_check_out, lf_short_comment, lf_textblob_polarity]\n",
    "\n",
    "# Apply the LFs to the unlabeled training data\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "\n",
    "# Train the label model and compute the training labels\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "df_train[\"label\"] = label_model.predict(L=L_train, tie_break_policy=\"abstain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used the LabelModel to label data; however, on many data points, all the labeling functions abstain, and so the LabelModel abstains as well. We’ll filter these data points out of our training set now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train.label != ABSTAIN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ultimate goal is to use the resulting labeled training data points to train a machine learning model that can generalize beyond the coverage of the labeling functions and the LabelModel. However first we’ll explore some of Snorkel’s other operators for building and managing training data.\n",
    "\n",
    "## Writing Transformation Functions for Data Augmentation\n",
    "An increasingly popular and critical technique in modern machine learning is data augmentation, the strategy of artificially augmenting existing labeled training datasets by creating transformed copies of the data points. Data augmentation is a practical and powerful method for injecting information about domain invariances into ML models via the data, rather than by trying to modify their internal architectures. The canonical example is randomly rotating, stretching, and transforming images when training image classifiers — a ubiquitous technique in the field of computer vision today. However, data augmentation is increasingly used in a range of settings, including text.\n",
    "\n",
    "Here, we implement a simple text data augmentation strategy — randomly replacing a word with a synonym(同义词). We express this as a transformation function (TF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from snorkel.augmentation import transformation_function\n",
    "\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get the synonyms of word from Wordnet.\"\"\"\n",
    "    lemmas = set().union(*[s.lemmas() for s in wn.synsets(word)])\n",
    "    return list(set(l.name().lower().replace(\"_\", \" \") for l in lemmas) - {word})\n",
    "\n",
    "\n",
    "@transformation_function()\n",
    "def tf_replace_word_with_synonym(x):\n",
    "    \"\"\"Try to replace a random word with a synonym.\"\"\"\n",
    "    words = x.text.lower().split()\n",
    "    idx = random.choice(range(len(words)))\n",
    "    synonyms = get_synonyms(words[idx])\n",
    "    if len(synonyms) > 0:\n",
    "        x.text = \" \".join(words[:idx] + [synonyms[0]] + words[idx + 1 :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply this transformation function to our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.augmentation import ApplyOnePolicy, PandasTFApplier\n",
    "\n",
    "tf_policy = ApplyOnePolicy(n_per_original=2, keep_original=True)\n",
    "tf_applier = PandasTFApplier([tf_replace_word_with_synonym], tf_policy)\n",
    "df_train_augmented = tf_applier.apply(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a common challenge with data augmentation is figuring out how to tune and apply different transformation functions to best augment a training set. This is most commonly done as an ad hoc manual process; however, in Snorkel, various approaches for using automatically learned data augmentation policies are supported. For more detail, see the Spam TFs tutorial.\n",
    "\n",
    "## Writing a Slicing Function\n",
    "Finally, a third operator in Snorkel, slicing functions (SFs), handles the reality that many datasets have __certain subsets or slices that are more important than others__. \n",
    "\n",
    "In Snorkel, we can write SFs to \n",
    "* (a) monitor specific slices and \n",
    "* (b) improve model performance over them by adding representational capacity targeted on a per-slice basis.\n",
    "\n",
    "Writing a slicing function is simple. For example, we could write one that looks for suspiciously shortened links, which might be critical due to their likelihood of linking to malicious sites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing import slicing_function\n",
    "\n",
    "\n",
    "@slicing_function()\n",
    "def short_link(x):\n",
    "    \"\"\"Return whether text matches common pattern for shortened \".ly\" links.\"\"\"\n",
    "    return int(bool(re.search(r\"\\w+\\.ly\", x.text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use Snorkel to monitor the performance over this slice, and to add representational capacity to our model in order to potentially increase performance on this slice. For a walkthrough of these steps, see the Spam SFs tutorial.\n",
    "\n",
    "## Training a Classifier\n",
    "The ultimate goal in Snorkel is to create a training dataset, which can then be plugged into an arbitrary machine learning framework (e.g. TensorFlow, Keras, PyTorch, Scikit-Learn, Ludwig, XGBoost) to train powerful machine learning models. \n",
    "\n",
    "Here, to complete this initial walkthrough, we’ll train an extremely simple model — a “bag of n-grams” logistic regression model in Scikit-Learn — using the weakly labeled and augmented training set we made with our labeling and transformation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_text = df_train_augmented.text.tolist()\n",
    "X_train = CountVectorizer(ngram_range=(1, 2)).fit_transform(train_text)\n",
    "\n",
    "clf = LogisticRegression(solver=\"lbfgs\")\n",
    "clf.fit(X=X_train, y=df_train_augmented.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that’s it — you’ve trained your first model without hand-labeling any training data! \n",
    "\n",
    "Next, to learn more about Snorkel, check out the tutorials, resources, and documentation for much more on how to use Snorkel to power your own machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Snorkel弱监督和ULMFiT迁移学习的微博情感分类\n",
    "[基于Snorkel弱监督和ULMFiT迁移学习的微博情感分类](https://yq.aliyun.com/articles/715361)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script('.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
