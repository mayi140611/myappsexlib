{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.parameterized.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp algo.dl.torchtext\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algo-dl-torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/text/\n",
    "\n",
    "https://github.com/pytorch/text\n",
    "\n",
    "https://github.com/atnlp/torchtext-summary\n",
    "\n",
    "The torchtext package consists of \n",
    "* data processing utilities   \n",
    "对应的代码封装在 `torchtext.data`: Generic data loaders, abstractions, and iterators for text (including vocabulary and word vectors)\n",
    "* popular datasets for natural language.    \n",
    "`torchtext.datasets`: Pre-built loaders for common NLP datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchtext==0.7.0\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchtext -U\n",
    "!pip freeze | grep torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchtext.data\n",
    "## 数据抽象\n",
    "### Example级别\n",
    "* Dataset, TabularDataset, Batch:   \n",
    "Defines a dataset composed of Examples\n",
    "* Example  \n",
    "Defines a single training or test example.\n",
    "\n",
    "### Field级别\n",
    "主要的操作都在Field级别\n",
    "\n",
    "数据字段可以划分为两类: 序列数据和非序列数据。\n",
    "\n",
    "针对序列数据，常见的处理方式有: \n",
    "* 分词 \n",
    "* 填补至等长pad \n",
    "* 截断trucate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见的序列Field处理方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超级棒的封装，基本上囊括了所有的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Field(\n",
    "    sequential=True,  # Whether the datatype represents sequential data. If False,\n",
    "                        # no tokenization is applied. Default: True.\n",
    "    use_vocab=True,  \n",
    "    init_token=None,# A token that will be prepended to every example using this\n",
    "                        # field, or None for no initial token. Default: None.\n",
    "    eos_token=None,\n",
    "    fix_length=None,  #  A fixed length that all examples using this field will be\n",
    "                        # padded to, or None for flexible sequence lengths. Default: None.\n",
    "    dtype=torch.int64,\n",
    "    preprocessing=None,  #  The Pipeline that will be applied to examples\n",
    "                            # using this field after tokenizing but before numericalizing. Many\n",
    "                            # Datasets replace this attribute with a custom preprocessor.\n",
    "    postprocessing=None,\n",
    "    lower=False,\n",
    "    tokenize=None,  # The function used to tokenize strings using this field into\n",
    "                    # sequential examples. If \"spacy\", the SpaCy tokenizer is\n",
    "                    # used. If a non-serializable function is passed as an argument,\n",
    "                    # the field will not be able to be serialized. Default: string.split.\n",
    "    tokenizer_language='en',\n",
    "    include_lengths=False,  # Whether to return a tuple of a padded minibatch and\n",
    "                            # a list containing the lengths of each examples, or just a padded\n",
    "                            # minibatch. Default: False.\n",
    "    batch_first=False,\n",
    "    pad_token='<pad>',\n",
    "    unk_token='<unk>',\n",
    "    pad_first=False,\n",
    "    truncate_first=False,\n",
    "    stop_words=None,\n",
    "    is_target=False,  # Whether this field is a target variable.\n",
    "                        # Affects iteration over batches. Default: False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT=data.Field(sequential=True, \n",
    "#                 tokenize=tokenize,\n",
    "                lower=True,\n",
    "                batch_first=False,  # 注意，默认False，即处理后的结果是(seq_len, batch_size, embed_dim)\n",
    "                fix_length=3\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label字段处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL=data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Iterator(\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    sort_key=None,\n",
    "    device=None,  # (str or `torch.device`): A string or instance of `torch.device`\n",
    "                    # specifying which device the Variables are going to be created on.\n",
    "                    # If left as default, the tensors will be created on cpu. Default: None.\n",
    "    batch_size_fn=None,\n",
    "    train=True,\n",
    "    repeat=False,\n",
    "    shuffle=None,\n",
    "    sort=None,  # A key to use for sorting examples in order to batch together\n",
    "                # examples with similar lengths and minimize padding. The sort_key\n",
    "                # provided to the Iterator constructor overrides the sort_key\n",
    "                # attribute of the Dataset, or defers to it if None.\n",
    "    sort_within_batch=None,  # Whether to sort (in descending order according to\n",
    "                            # self.sort_key) within each batch. If None, defaults to self.sort.\n",
    "                            # If self.sort is True and this is False, the batch is left in the\n",
    "                            # original (ascending) sorted order.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.BucketIterator(\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    sort_key=None,\n",
    "    device=None,\n",
    "    batch_size_fn=None,\n",
    "    train=True,\n",
    "    repeat=False,\n",
    "    shuffle=None,\n",
    "    sort=None,\n",
    "    sort_within_batch=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例1\n",
    "https://www.yanxishe.com/blogDetail/14658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词函数\n",
    "def content_tokenize(text):    \n",
    "    return [item for item in str(text)]\n",
    "# 创建content字段的Field\n",
    "CONTENT = data.Field(sequential=True, tokenize=content_tokenize, batch_first=True)\n",
    "# 创建label字段的\n",
    "FieldLABEL = data.Field(sequential=False, use_vocab=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('label', LABEL), ('content', CONTENT)]\n",
    "examples = []\n",
    "for label, content in [(1, '我不退我也不买。我就看戏'), (0, '我今天很开心，很开心，微信'), \n",
    "                      (1, '跟诸葛撞单了')]:    \n",
    "    examples.append(data.Example.fromlist([label, content], fields))\n",
    "data_set = data.Dataset(examples, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.index(value, start=0, stop=9223372036854775807, /)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.examples.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为 CONTENT 字段创建词向量\n",
    "CONTENT.build_vocab(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x14c86cb00>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             '我': 2,\n",
       "             '不': 3,\n",
       "             '开': 4,\n",
       "             '很': 5,\n",
       "             '心': 6,\n",
       "             '，': 7,\n",
       "             '。': 8,\n",
       "             '也': 9,\n",
       "             '买': 10,\n",
       "             '了': 11,\n",
       "             '今': 12,\n",
       "             '信': 13,\n",
       "             '单': 14,\n",
       "             '天': 15,\n",
       "             '就': 16,\n",
       "             '微': 17,\n",
       "             '戏': 18,\n",
       "             '撞': 19,\n",
       "             '看': 20,\n",
       "             '葛': 21,\n",
       "             '诸': 22,\n",
       "             '跟': 23,\n",
       "             '退': 24})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTENT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '我',\n",
       " '不',\n",
       " '开',\n",
       " '很',\n",
       " '心',\n",
       " '，',\n",
       " '。',\n",
       " '也',\n",
       " '买',\n",
       " '了',\n",
       " '今',\n",
       " '信',\n",
       " '单',\n",
       " '天',\n",
       " '就',\n",
       " '微',\n",
       " '戏',\n",
       " '撞',\n",
       " '看',\n",
       " '葛',\n",
       " '诸',\n",
       " '跟',\n",
       " '退']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTENT.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'我': 4,\n",
       "         '不': 2,\n",
       "         '退': 1,\n",
       "         '也': 1,\n",
       "         '买': 1,\n",
       "         '。': 1,\n",
       "         '就': 1,\n",
       "         '看': 1,\n",
       "         '戏': 1,\n",
       "         '今': 1,\n",
       "         '天': 1,\n",
       "         '很': 2,\n",
       "         '开': 2,\n",
       "         '心': 2,\n",
       "         '，': 2,\n",
       "         '微': 1,\n",
       "         '信': 1,\n",
       "         '跟': 1,\n",
       "         '诸': 1,\n",
       "         '葛': 1,\n",
       "         '撞': 1,\n",
       "         '单': 1,\n",
       "         '了': 1})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTENT.vocab.freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例2 tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT = data.Field(sequential=True, fix_length=50, batch_first=True)\n",
    "# LABEL = data.Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "\n",
    "TEXT = data.Field(sequential=True, fix_length=50)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Dataset\n",
    "class MyDataset(data.Dataset):\n",
    "    name = 'TX Dataset'\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.creative_ids)\n",
    "\n",
    "    def __init__(self, path, text_field, label_field, test=False, aug=False, **kwargs):\n",
    "        fields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"creative_ids\", text_field), (\"gender\", label_field)]\n",
    "        \n",
    "        examples = []\n",
    "        df = pd.read_pickle(path)\n",
    "        print('read data from {}'.format(path))\n",
    "\n",
    "        if test:\n",
    "            # 如果为测试集，则不加载label\n",
    "            for text in tqdm(csv_data['comment_text']):\n",
    "                examples.append(data.Example.fromlist([None, text, None], fields))\n",
    "        else:\n",
    "            for text, label in tqdm(zip(df['creative_id'], df['gender'])):\n",
    "                if aug:\n",
    "                    # do augmentation\n",
    "                    rate = random.random()\n",
    "                    if rate > 0.5:\n",
    "                        text = self.dropout(text)\n",
    "                    else:\n",
    "                        text = self.shuffle(text)\n",
    "                # Example: Defines a single training or test example.Stores each column of the example as an attribute.\n",
    "                examples.append(data.Example.fromlist([None, text, label], fields))\n",
    "        # 之前是一些预处理操作，此处调用super调用父类构造方法，产生标准Dataset\n",
    "        # super(MyDataset, self).__init__(examples, fields, **kwargs)\n",
    "        super(MyDataset, self).__init__(examples, fields)\n",
    "\n",
    "    def shuffle(self, text):\n",
    "        text = np.random.permutation(text.strip().split())\n",
    "        return ' '.join(text)\n",
    "\n",
    "    def dropout(self, text, p=0.5):\n",
    "        # random delete some text\n",
    "        text = text.strip().split()\n",
    "        len_ = len(text)\n",
    "        indexs = np.random.choice(len_, int(len_ * p))\n",
    "        for i in indexs:\n",
    "            text[i] = ''\n",
    "        return ' '.join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MyDataset(\n",
    "    os.path.join(data_dir, 'train_gender.pkl'), \n",
    "    text_field=TEXT, \n",
    "    label_field=LABEL, \n",
    "    test=False, \n",
    "    aug=False)\n",
    "\n",
    "val = MyDataset(\n",
    "    os.path.join(data_dir, 'val_gender.pkl'), \n",
    "    text_field=TEXT, \n",
    "    label_field=LABEL, \n",
    "    test=False, \n",
    "    aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = BucketIterator.splits(\n",
    "        (train, val), # 构建数据集所需的数据集\n",
    "        batch_sizes=(100, 100),\n",
    "#         device=-1, # 如果使用gpu，此处将-1更换为GPU的编号\n",
    "        # device='cpu',\n",
    "        # sort_key=lambda x: len(x.creative_ids), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        # sort_within_batch=False,\n",
    "        # repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_iter):\n",
    "    print(batch)\n",
    "    text, label = batch.creative_ids, batch.gender\n",
    "    # print(text.shape, label.shape)\n",
    "    print(text)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with TorchText\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to use the text classification datasets in torchtext, including\n",
    "\n",
    "- AG_NEWS,\n",
    "- SogouNews,\n",
    "- DBpedia,\n",
    "- YelpReviewPolarity,\n",
    "- YelpReviewFull,\n",
    "- YahooAnswers,\n",
    "- AmazonReviewPolarity,\n",
    "- AmazonReviewFull\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script('.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
