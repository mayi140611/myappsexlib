{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp algo.ml.tree.lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM, Light Gradient Boosting Machine\n",
    "\n",
    "https://github.com/microsoft/LightGBM\n",
    "\n",
    "LightGBM是使用基于树的学习算法的梯度增强框架。 它被设计为分布式且高效的，具有以下优点：\n",
    "\n",
    "     训练速度更快，效率更高。\n",
    "\n",
    "     降低内存使用率。\n",
    "\n",
    "     更好的准确性。\n",
    "\n",
    "     支持并行和GPU学习。\n",
    "\n",
    "     能够处理大规模数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm==2.2.3\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm -U\n",
    "!pip freeze | grep lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python guide\n",
    "https://github.com/microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple_example\n",
    "https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/simple_example.py\n",
    "\n",
    "\n",
    "    Construct Dataset\n",
    "    Basic train and predict\n",
    "    Eval during training\n",
    "    Early stopping\n",
    "    Save model to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:47: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/lgb_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "# load or create your dataset\n",
    "df_train = pd.read_csv(data_dir + 'regression.train', header=None, sep='\\t')\n",
    "df_test = pd.read_csv(data_dir + 'regression.test', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[0]\n",
    "y_test = df_test[0]\n",
    "X_train = df_train.drop(0, axis=1)\n",
    "X_test = df_test.drop(0, axis=1)\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "[1]\tvalid_0's l2: 0.243898\tvalid_0's l1: 0.492841\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l2: 0.240605\tvalid_0's l1: 0.489327\n",
      "[3]\tvalid_0's l2: 0.236472\tvalid_0's l1: 0.484931\n",
      "[4]\tvalid_0's l2: 0.232586\tvalid_0's l1: 0.480567\n",
      "[5]\tvalid_0's l2: 0.22865\tvalid_0's l1: 0.475965\n",
      "[6]\tvalid_0's l2: 0.226187\tvalid_0's l1: 0.472861\n",
      "[7]\tvalid_0's l2: 0.223738\tvalid_0's l1: 0.469847\n",
      "[8]\tvalid_0's l2: 0.221012\tvalid_0's l1: 0.466258\n",
      "[9]\tvalid_0's l2: 0.218429\tvalid_0's l1: 0.462751\n",
      "[10]\tvalid_0's l2: 0.215505\tvalid_0's l1: 0.458755\n",
      "[11]\tvalid_0's l2: 0.213027\tvalid_0's l1: 0.455252\n",
      "[12]\tvalid_0's l2: 0.210809\tvalid_0's l1: 0.452051\n",
      "[13]\tvalid_0's l2: 0.208612\tvalid_0's l1: 0.448764\n",
      "[14]\tvalid_0's l2: 0.207468\tvalid_0's l1: 0.446667\n",
      "[15]\tvalid_0's l2: 0.206009\tvalid_0's l1: 0.444211\n",
      "[16]\tvalid_0's l2: 0.20465\tvalid_0's l1: 0.44186\n",
      "[17]\tvalid_0's l2: 0.202489\tvalid_0's l1: 0.438508\n",
      "[18]\tvalid_0's l2: 0.200668\tvalid_0's l1: 0.435919\n",
      "[19]\tvalid_0's l2: 0.19925\tvalid_0's l1: 0.433348\n",
      "[20]\tvalid_0's l2: 0.198136\tvalid_0's l1: 0.431211\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l2: 0.198136\tvalid_0's l1: 0.431211\n",
      "Saving model...\n",
      "Starting predicting...\n",
      "The rmse of prediction is: 0.44512434910807497\n"
     ]
    }
   ],
   "source": [
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "print('Saving model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "# eval\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn_example\n",
    "https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/sklearn_example.py\n",
    "\n",
    "\n",
    "    Create data for learning with sklearn interface\n",
    "    Basic train and predict with sklearn interface\n",
    "    Feature importances with sklearn interface\n",
    "    Self-defined eval metric with sklearn interface\n",
    "    Find best parameters for the model with sklearn's GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/lgb_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "# load or create your dataset\n",
    "df_train = pd.read_csv(data_dir + 'regression.train', header=None, sep='\\t')\n",
    "df_test = pd.read_csv(data_dir + 'regression.test', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "[1]\tvalid_0's l2: 0.242763\tvalid_0's l1: 0.491735\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l2: 0.237895\tvalid_0's l1: 0.486563\n",
      "[3]\tvalid_0's l2: 0.233277\tvalid_0's l1: 0.481489\n",
      "[4]\tvalid_0's l2: 0.22925\tvalid_0's l1: 0.476848\n",
      "[5]\tvalid_0's l2: 0.226155\tvalid_0's l1: 0.47305\n",
      "[6]\tvalid_0's l2: 0.222963\tvalid_0's l1: 0.469049\n",
      "[7]\tvalid_0's l2: 0.220364\tvalid_0's l1: 0.465556\n",
      "[8]\tvalid_0's l2: 0.217872\tvalid_0's l1: 0.462208\n",
      "[9]\tvalid_0's l2: 0.215328\tvalid_0's l1: 0.458676\n",
      "[10]\tvalid_0's l2: 0.212743\tvalid_0's l1: 0.454998\n",
      "[11]\tvalid_0's l2: 0.210805\tvalid_0's l1: 0.452047\n",
      "[12]\tvalid_0's l2: 0.208945\tvalid_0's l1: 0.449158\n",
      "[13]\tvalid_0's l2: 0.206986\tvalid_0's l1: 0.44608\n",
      "[14]\tvalid_0's l2: 0.205513\tvalid_0's l1: 0.443554\n",
      "[15]\tvalid_0's l2: 0.203728\tvalid_0's l1: 0.440643\n",
      "[16]\tvalid_0's l2: 0.201865\tvalid_0's l1: 0.437687\n",
      "[17]\tvalid_0's l2: 0.200639\tvalid_0's l1: 0.435454\n",
      "[18]\tvalid_0's l2: 0.199522\tvalid_0's l1: 0.433288\n",
      "[19]\tvalid_0's l2: 0.198552\tvalid_0's l1: 0.431297\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's l1: 0.428946\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's l1: 0.428946\n",
      "Starting predicting...\n",
      "The rmse of prediction is: 0.4441153344254208\n",
      "Feature importances: [23, 7, 0, 33, 5, 56, 9, 1, 1, 21, 2, 5, 1, 19, 9, 6, 1, 10, 4, 10, 0, 31, 61, 4, 48, 102, 52, 79]\n",
      "Starting training with custom eval function...\n",
      "[1]\tvalid_0's l2: 0.242763\tvalid_0's RMSLE: 0.344957\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l2: 0.237895\tvalid_0's RMSLE: 0.341693\n",
      "[3]\tvalid_0's l2: 0.233277\tvalid_0's RMSLE: 0.338462\n",
      "[4]\tvalid_0's l2: 0.22925\tvalid_0's RMSLE: 0.335656\n",
      "[5]\tvalid_0's l2: 0.226155\tvalid_0's RMSLE: 0.333431\n",
      "[6]\tvalid_0's l2: 0.222963\tvalid_0's RMSLE: 0.331104\n",
      "[7]\tvalid_0's l2: 0.220364\tvalid_0's RMSLE: 0.329193\n",
      "[8]\tvalid_0's l2: 0.217872\tvalid_0's RMSLE: 0.327337\n",
      "[9]\tvalid_0's l2: 0.215328\tvalid_0's RMSLE: 0.325433\n",
      "[10]\tvalid_0's l2: 0.212743\tvalid_0's RMSLE: 0.323523\n",
      "[11]\tvalid_0's l2: 0.210805\tvalid_0's RMSLE: 0.321986\n",
      "[12]\tvalid_0's l2: 0.208945\tvalid_0's RMSLE: 0.320523\n",
      "[13]\tvalid_0's l2: 0.206986\tvalid_0's RMSLE: 0.319027\n",
      "[14]\tvalid_0's l2: 0.205513\tvalid_0's RMSLE: 0.317796\n",
      "[15]\tvalid_0's l2: 0.203728\tvalid_0's RMSLE: 0.316383\n",
      "[16]\tvalid_0's l2: 0.201865\tvalid_0's RMSLE: 0.314827\n",
      "[17]\tvalid_0's l2: 0.200639\tvalid_0's RMSLE: 0.313876\n",
      "[18]\tvalid_0's l2: 0.199522\tvalid_0's RMSLE: 0.312948\n",
      "[19]\tvalid_0's l2: 0.198552\tvalid_0's RMSLE: 0.312116\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's RMSLE: 0.311032\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's RMSLE: 0.311032\n",
      "Starting training with multiple custom eval functions...\n",
      "[1]\tvalid_0's l2: 0.242763\tvalid_0's RMSLE: 0.344957\tvalid_0's RAE: 0.991146\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l2: 0.237895\tvalid_0's RMSLE: 0.341693\tvalid_0's RAE: 0.98072\n",
      "[3]\tvalid_0's l2: 0.233277\tvalid_0's RMSLE: 0.338462\tvalid_0's RAE: 0.970493\n",
      "[4]\tvalid_0's l2: 0.22925\tvalid_0's RMSLE: 0.335656\tvalid_0's RAE: 0.961139\n",
      "[5]\tvalid_0's l2: 0.226155\tvalid_0's RMSLE: 0.333431\tvalid_0's RAE: 0.953484\n",
      "[6]\tvalid_0's l2: 0.222963\tvalid_0's RMSLE: 0.331104\tvalid_0's RAE: 0.945419\n",
      "[7]\tvalid_0's l2: 0.220364\tvalid_0's RMSLE: 0.329193\tvalid_0's RAE: 0.938379\n",
      "[8]\tvalid_0's l2: 0.217872\tvalid_0's RMSLE: 0.327337\tvalid_0's RAE: 0.931631\n",
      "[9]\tvalid_0's l2: 0.215328\tvalid_0's RMSLE: 0.325433\tvalid_0's RAE: 0.92451\n",
      "[10]\tvalid_0's l2: 0.212743\tvalid_0's RMSLE: 0.323523\tvalid_0's RAE: 0.917099\n",
      "[11]\tvalid_0's l2: 0.210805\tvalid_0's RMSLE: 0.321986\tvalid_0's RAE: 0.911151\n",
      "[12]\tvalid_0's l2: 0.208945\tvalid_0's RMSLE: 0.320523\tvalid_0's RAE: 0.905328\n",
      "[13]\tvalid_0's l2: 0.206986\tvalid_0's RMSLE: 0.319027\tvalid_0's RAE: 0.899122\n",
      "[14]\tvalid_0's l2: 0.205513\tvalid_0's RMSLE: 0.317796\tvalid_0's RAE: 0.894031\n",
      "[15]\tvalid_0's l2: 0.203728\tvalid_0's RMSLE: 0.316383\tvalid_0's RAE: 0.888164\n",
      "[16]\tvalid_0's l2: 0.201865\tvalid_0's RMSLE: 0.314827\tvalid_0's RAE: 0.882206\n",
      "[17]\tvalid_0's l2: 0.200639\tvalid_0's RMSLE: 0.313876\tvalid_0's RAE: 0.877704\n",
      "[18]\tvalid_0's l2: 0.199522\tvalid_0's RMSLE: 0.312948\tvalid_0's RAE: 0.87334\n",
      "[19]\tvalid_0's l2: 0.198552\tvalid_0's RMSLE: 0.312116\tvalid_0's RAE: 0.869327\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's RMSLE: 0.311032\tvalid_0's RAE: 0.864588\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's RMSLE: 0.311032\tvalid_0's RAE: 0.864588\n",
      "Starting predicting...\n",
      "The rmsle of prediction is: 0.3110323289863278\n",
      "The rae of prediction is: 0.8645881044669875\n",
      "Best parameters found by grid search are: {'learning_rate': 0.1, 'n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "_train = df_train[0]\n",
    "y_test = df_test[0]\n",
    "X_train = df_train.drop(0, axis=1)\n",
    "X_test = df_test.drop(0, axis=1)\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.LGBMRegressor(num_leaves=31,\n",
    "                        learning_rate=0.05,\n",
    "                        n_estimators=20)\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "# eval\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importances_))\n",
    "\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool\n",
    "# Root Mean Squared Logarithmic Error (RMSLE)\n",
    "def rmsle(y_true, y_pred):\n",
    "    return 'RMSLE', np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2))), False\n",
    "\n",
    "\n",
    "print('Starting training with custom eval function...')\n",
    "# train\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=rmsle,\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "\n",
    "# another self-defined eval metric\n",
    "# f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool\n",
    "# Relative Absolute Error (RAE)\n",
    "def rae(y_true, y_pred):\n",
    "    return 'RAE', np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(np.mean(y_true) - y_true)), False\n",
    "\n",
    "\n",
    "print('Starting training with multiple custom eval functions...')\n",
    "# train\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=lambda y_true, y_pred: [rmsle(y_true, y_pred), rae(y_true, y_pred)],\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "# eval\n",
    "print('The rmsle of prediction is:', rmsle(y_test, y_pred)[1])\n",
    "print('The rae of prediction is:', rae(y_test, y_pred)[1])\n",
    "\n",
    "# other scikit-learn modules\n",
    "estimator = lgb.LGBMRegressor(num_leaves=31)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(estimator, param_grid, cv=3)\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters found by grid search are:', gbm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nb_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 00_template.ipynb.\n",
      "Converted algo_ml_shallow_tree_catboost.ipynb.\n",
      "Converted dl_keras.ipynb.\n",
      "Converted engineering_nbdev.ipynb.\n",
      "Converted engineering_panel.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
