{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp engineering.nbdev\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embeding\n",
    "主流的词嵌入实现方法有Mikolov的Word2Vec和斯坦福大学的Glove，也有人做过实验来比较这两种方法的优劣，并没有多大的差异。\n",
    "* Word2Vec是基于预测的词向量模型，简单来说，就是给定一个词，去预测这个词周围可能出现的词，或者给定一些词来确定位于中心位置的词。\n",
    "* Glove是基于统计方法的，通过对词-词共现矩阵里的非零元素进行训练。\n",
    "\n",
    "总体来说，Word2Vec使用的是局部信息，而Glove使用的是全局信息，因此前者训练起来更慢但不占用内存，而Glove通过更多的计算资源换取训练速度上的提升。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估词向量\n",
    "https://blog.csdn.net/TimEcho/article/details/106281209\n",
    "\n",
    "* 评估方式一：降维。训练好的词向量，首先要进行降维，使用工具TSNE降维到2维，这样映射到二维空间，便于观察词向量训练后是否能将同类词聚集到一起\n",
    "* 评估方式二：相似度对比，选定两个词向量，计算相似度，然后与人工标注的相似度进行对比\n",
    "* 评估方式三：类比。计算woman和man的距离，然后在词库中找与girl距离跟woman和man距离相近的词\n",
    "\n",
    "## word2vec中skipgram的缺点\n",
    "\n",
    "1. 不考虑上下文，即同一个word只有一种向量表示，没有考虑一词多义的情况  \n",
    "解决方法：context-aware word embedding:ELmo或者BERT\n",
    "2. 窗口长度有限，  \n",
    "解决：类似于LM，使用RNN，LSTM来学习出词向量\n",
    "3. 无法考虑全局，  \n",
    "解决方法：全局模型（MF矩阵分解）\n",
    "4. 无法有效学习低频词向量，\n",
    "5. 未登录词，只在训练集出现已经训练好词向量，但是测试集没有，（OOV：out-of-vacabulary）  \n",
    "4,5，使用subword embedding解决\n",
    "6. 严格意义上的语序，解决：LM（RNN/LSTM）  \n",
    "7. 可解释性不足,解决：非欧式空间\n",
    "\n",
    "## subword: \n",
    "解决低频词、OOV, 根据已有词向量学习出新词的向量\n",
    "\n",
    "这种subword的方式是这样的：加入窗口大小n=4，将每个单词分成多个部分，每个部分包含4个character，  \n",
    "训练是我已然是采样skipgram模型，所以得到的词向量是subword这种形式。那么对于未登录词，完全可以通过找词库中的这种subword词向量来拼接到一个未登录词。并得到他的条件概率。  \n",
    "不过这种方法不适用于中文。  \n",
    "论文：enriching word embedding via wubword information\n",
    "\n",
    "当数据量很少时，可以考虑使用这种subword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "\n",
    "## bag-of-words model\n",
    "This model transforms each document to a fixed-length vector of integers. For example, given the sentences:\n",
    "\n",
    "    John likes to watch movies. Mary likes movies too.\n",
    "\n",
    "    John also likes to watch football games. Mary hates football.\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "    [1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "    [1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "首先，他们会丢失所有有关单词顺序的信息：“约翰喜欢玛丽”和“玛丽喜欢约翰”对应于相同的向量。 There is a solution: bag of n-grams models consider word phrases of length n to represent documents as fixed-length vectors to capture local word order but suffer from data sparsity and high dimensionality.\n",
    "\n",
    "其次，该模型不会尝试学习基础单词的含义，因此，向量之间的距离并不总是反映出含义上的差异。 \n",
    "\n",
    "Word2Vec模型解决了第二个问题。\n",
    "\n",
    "## the Word2Vec Model\n",
    "Word2Vec是一种更新的模型，它使用浅层神经网络将单词嵌入到低维向量空间中。 结果是一组词向量，其中在向量空间中靠在一起的向量根据上下文具有相似的含义，而彼此远离的词向量具有不同的含义。\n",
    "\n",
    "The are two versions of this model and Word2Vec class implements them both:\n",
    "\n",
    "![](img/sk01.png)\n",
    "### Skip-grams (SG)\n",
    "The Word2Vec Skip-gram model, for example, takes in pairs (word1, word2) generated by moving a window across text data, and trains a 1-hidden-layer neural network based on the synthetic task of given an input word, giving us a predicted probability distribution of nearby words to the input. A virtual one-hot encoding of words goes through a ‘projection layer’ to the hidden layer; these projection weights are later interpreted as the word embeddings. So if the hidden layer has 300 neurons, this network will give us 300-dimensional word embeddings.\n",
    "\n",
    "### Continuous-bag-of-words (CBOW)\n",
    "Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It is also a 1-hidden-layer neural network. The synthetic training task now uses the average of multiple input context words, rather than a single word as in skip-gram, to predict the center word. Again, the projection weights that turn one-hot words into averageable vectors, of the same width as the hidden layer, are interpreted as the word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim.models.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim==3.7.2\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U gensim\n",
    "!pip freeze | grep gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 默认参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Word2Vec(\n",
    "        sentences=None,\n",
    "        corpus_file=None,\n",
    "        size=100,\n",
    "        alpha=0.025,\n",
    "        window=5,\n",
    "        min_count=5,\n",
    "        max_vocab_size=None,\n",
    "        sample=0.001,\n",
    "        seed=1,\n",
    "        workers=3,\n",
    "        min_alpha=0.0001,\n",
    "        sg=0,\n",
    "        hs=0,\n",
    "        negative=5,\n",
    "        ns_exponent=0.75,\n",
    "        cbow_mean=1,\n",
    "        hashfxn=<built-in function hash>,\n",
    "        iter=5,\n",
    "        null_word=0,\n",
    "        trim_rule=None,\n",
    "        sorted_vocab=1,\n",
    "        batch_words=10000,\n",
    "        compute_loss=False,\n",
    "        callbacks=(),\n",
    "        max_final_vocab=None,\n",
    "    )\n",
    "* iter : int, optional 默认是5，要调\n",
    "    Number of iterations (epochs) over the corpus.\n",
    "* alpha : float, optional\n",
    "    The initial learning rate.\n",
    "* min_alpha : float, optional\n",
    "    Learning rate will linearly drop to `min_alpha` as training progresses.\n",
    "* workers : int, optional  默认是3，所以训练时要手动设置一下\n",
    "    Use these many worker threads to train the model (=faster training with multicore machines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_count\n",
    "min_count用于修剪内部字典。 在十亿个单词的语料库中仅出现一次或两次的单词可能是无趣的错别字和垃圾。 此外，没有足够的数据来对这些词进行任何有意义的训练，因此最好忽略它们：\n",
    "\n",
    "min_count = 5的默认值\n",
    "#### size\n",
    "size是gensim Word2Vec将单词映射到的N维空间的维数（N）。\n",
    "\n",
    "较大的值需要更多的训练数据，但可以产生更好（更准确）的模型。 合理的值在数十到数百之间。\n",
    "\n",
    "default value of size=100\n",
    "#### worker\n",
    "worker，最后一个主要参数（此处为完整列表）用于训练并行化，以加快训练速度：\n",
    "\n",
    "default value of workers=3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Your Own Model\n",
    "这个语料库足够小，可以完全放入内存，但是我们将实现一个内存友好的迭代器，逐行读取它，以演示如何处理更大的语料库。\n",
    "\n",
    "如果我们想进行任何自定义的预处理，例如 解码非标准编码，小写字母，删除数字，提取命名实体……所有这些都可以在MyCorpus迭代器中完成，而word2vec不需要知道。 所需要的就是输入产生一个句子（另一个utf8单词列表）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.test.utils import datapath\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hundreds',\n",
       " 'of',\n",
       " 'people',\n",
       " 'have',\n",
       " 'been',\n",
       " 'forced',\n",
       " 'to',\n",
       " 'vacate',\n",
       " 'their',\n",
       " 'homes']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.simple_preprocess('Hundreds of people have been forced to vacate their homes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RuntimeError: you must first build vocabulary before training the model\n",
    "# 原因:训练文本太小\n",
    "# sentences = [\n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"cat\", \"say\", \"meow\"], \n",
    "#     [\"dog\", \"say\", \"woof\"]]\n",
    "# model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03686245, -0.01609651, -0.01444744, -0.02135829, -0.00200862,\n",
       "       -0.01151453, -0.01819683,  0.02734437, -0.10053819, -0.05595972,\n",
       "        0.04234225,  0.04281876,  0.02587122,  0.01270954,  0.04601529,\n",
       "       -0.00835075,  0.00156381, -0.01310062,  0.00457986,  0.02177754,\n",
       "        0.02796555,  0.03658348, -0.00421377,  0.02298167,  0.00075962,\n",
       "       -0.01811693, -0.08794788, -0.01969494, -0.01617615, -0.01301005,\n",
       "        0.03622155, -0.02132978,  0.00818219, -0.01280667, -0.06247395,\n",
       "       -0.02782101,  0.01052102,  0.03244915,  0.055819  ,  0.02236952,\n",
       "       -0.02228366, -0.06680909, -0.03983621, -0.02890607,  0.03852952,\n",
       "       -0.05652092, -0.00395071, -0.065943  ,  0.0195525 ,  0.02234451,\n",
       "        0.02618746, -0.01053842, -0.01325279, -0.03100039, -0.03426435,\n",
       "        0.02069941, -0.04345337,  0.02748772, -0.02539084, -0.01945122,\n",
       "       -0.01035313,  0.05203196,  0.01739551, -0.02849312, -0.04863365,\n",
       "       -0.01096844, -0.06444025, -0.01036584, -0.04000191,  0.04583897,\n",
       "       -0.0053553 , -0.00509666, -0.01606082, -0.07653043,  0.0604446 ,\n",
       "       -0.00667824,  0.00838656, -0.05040137,  0.07151395, -0.00786852,\n",
       "        0.02645928,  0.05306596,  0.02914787,  0.03906055,  0.01238154,\n",
       "        0.00020839, -0.01395124,  0.01821121, -0.02370092, -0.01446467,\n",
       "       -0.01132251,  0.06112367,  0.07590397, -0.0048133 , -0.03360593,\n",
       "       -0.03753424, -0.01607151, -0.02337008, -0.03243359,  0.00209838],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king = model.wv['king']\n",
    "vec_king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01486553,  0.01823401, -0.02772865,  0.00213301,  0.0372683 ,\n",
       "        0.01642814,  0.04816883, -0.05448446,  0.01253152,  0.03404462,\n",
       "        0.01756212, -0.04840738, -0.03839137, -0.01304129, -0.03804687,\n",
       "        0.05305994, -0.00860955, -0.00836853,  0.00134603, -0.03344008,\n",
       "       -0.03039226, -0.03281286, -0.00623441, -0.02607584,  0.00408613,\n",
       "        0.04711006,  0.03532708, -0.0055987 , -0.00130932,  0.02229508,\n",
       "       -0.04697058, -0.02922403,  0.0131788 , -0.01622746,  0.05389683,\n",
       "       -0.02905632,  0.03431457,  0.01364054,  0.00634684, -0.01930137,\n",
       "       -0.01475715, -0.04367147, -0.00581169, -0.03634231,  0.01892559,\n",
       "       -0.01157352,  0.02849645,  0.00408916,  0.03510711, -0.0240911 ,\n",
       "       -0.03448397, -0.00420031, -0.06505472,  0.00562562, -0.03190516,\n",
       "       -0.01666111, -0.04189085,  0.07402997,  0.00904749, -0.03249473,\n",
       "        0.00972437,  0.03047183, -0.00820827, -0.02223486, -0.01516857,\n",
       "       -0.00403475, -0.02458509,  0.02097399, -0.00187658,  0.01206905,\n",
       "       -0.04879121, -0.01009979, -0.024553  , -0.02041692, -0.01424486,\n",
       "        0.03416541,  0.01465811, -0.02790837, -0.05651117,  0.03401321,\n",
       "        0.05107525, -0.01604857,  0.04182722, -0.01838509,  0.00617696,\n",
       "       -0.017543  ,  0.02919475,  0.06655695,  0.04709207,  0.06052168,\n",
       "        0.02376736, -0.01636989,  0.01506402,  0.00921484, -0.00102513,\n",
       "       -0.01343615, -0.0189462 , -0.00384656,  0.01540534, -0.04651833],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['king']  # removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'cameroon' does not appear in this model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vec_cameroon = model.wv['cameroon']\n",
    "except KeyError:\n",
    "    print(\"The word 'cameroon' does not appear in this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取最相近的词向量topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('highway', 0.9974420070648193), ('reported', 0.9973129630088806)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = model.wv.most_similar('king', topn=2)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'king1' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-383d9b166514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'king1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'king1' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "r = model.wv.most_similar('king1', topn=2)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'highway': 0.9974420070648193, 'reported': 0.9973129630088806}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/luoyonggui/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     (highway, 0.9974420070648193)\n",
       "1    (reported, 0.9973129630088806)\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "highway     0.997442\n",
       "reported    0.997313\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dict(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('have', 0.9996969699859619), ('five', 0.9996781349182129), ('israeli', 0.9996719360351562), ('just', 0.9996703863143921), ('other', 0.999667763710022)]\n"
     ]
    }
   ],
   "source": [
    "# Print the 5 most similar words to “car” or “minivan”\n",
    "print(model.wv.most_similar(positive=['car', 'man'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py:858: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'land'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hundreds\n",
      "of\n",
      "people\n",
      "have\n",
      "been\n",
      "forced\n",
      "to\n",
      "their\n",
      "homes\n",
      "in\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/luoyonggui/PycharmProjects/nbdevlib'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model.save('/Users/luoyonggui/Downloads/tt.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "new_model = Word2Vec.load('/Users/luoyonggui/Downloads/tt.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01486553,  0.01823401, -0.02772865,  0.00213301,  0.0372683 ,\n",
       "        0.01642814,  0.04816883, -0.05448446,  0.01253152,  0.03404462,\n",
       "        0.01756212, -0.04840738, -0.03839137, -0.01304129, -0.03804687,\n",
       "        0.05305994, -0.00860955, -0.00836853,  0.00134603, -0.03344008,\n",
       "       -0.03039226, -0.03281286, -0.00623441, -0.02607584,  0.00408613,\n",
       "        0.04711006,  0.03532708, -0.0055987 , -0.00130932,  0.02229508,\n",
       "       -0.04697058, -0.02922403,  0.0131788 , -0.01622746,  0.05389683,\n",
       "       -0.02905632,  0.03431457,  0.01364054,  0.00634684, -0.01930137,\n",
       "       -0.01475715, -0.04367147, -0.00581169, -0.03634231,  0.01892559,\n",
       "       -0.01157352,  0.02849645,  0.00408916,  0.03510711, -0.0240911 ,\n",
       "       -0.03448397, -0.00420031, -0.06505472,  0.00562562, -0.03190516,\n",
       "       -0.01666111, -0.04189085,  0.07402997,  0.00904749, -0.03249473,\n",
       "        0.00972437,  0.03047183, -0.00820827, -0.02223486, -0.01516857,\n",
       "       -0.00403475, -0.02458509,  0.02097399, -0.00187658,  0.01206905,\n",
       "       -0.04879121, -0.01009979, -0.024553  , -0.02041692, -0.01424486,\n",
       "        0.03416541,  0.01465811, -0.02790837, -0.05651117,  0.03401321,\n",
       "        0.05107525, -0.01604857,  0.04182722, -0.01838509,  0.00617696,\n",
       "       -0.017543  ,  0.02919475,  0.06655695,  0.04709207,  0.06052168,\n",
       "        0.02376736, -0.01636989,  0.01506402,  0.00921484, -0.00102513,\n",
       "       -0.01343615, -0.0189462 , -0.00384656,  0.01540534, -0.04651833],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.wv['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nb_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 00_template.ipynb.\n",
      "Converted algo_dl_keras.ipynb.\n",
      "Converted algo_ml_shallow_tree_catboost.ipynb.\n",
      "Converted algo_rs_associated_rules.ipynb.\n",
      "Converted algo_rs_matrix.ipynb.\n",
      "Converted algo_seq_embeding.ipynb.\n",
      "Converted algo_seq_tfidf.ipynb.\n",
      "Converted engineering_nbdev.ipynb.\n",
      "Converted engineering_panel.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No notebooks were modified\r\n",
      "converting /Users/luoyonggui/PycharmProjects/nbdevlib/index.ipynb to README.md\r\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
