{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==2.1.0\r\n",
      "tensorflow-datasets==1.3.0\r\n",
      "tensorflow-estimator==2.1.0\r\n",
      "tensorflow-metadata==0.15.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提交记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## phase2\n",
    "### 日期:2020-04-19 17:06:44排名: 无\n",
    "user-item和item_item融合 想着会有提升，有些失望！\n",
    "\n",
    "* score:0.1343\n",
    "* hitrate_50_full:0.3238\n",
    "* ndcg_50_full:0.1343\n",
    "* hitrate_50_half:0.2429\n",
    "* ndcg_50_half:0.1013\n",
    "\n",
    "### 日期:2020-04-19 16:45:28排名: 无\n",
    "user-item  单个user_item的得分还是很满意的\n",
    "\n",
    "score:0.1295\n",
    "hitrate_50_full:0.3161\n",
    "ndcg_50_full:0.1295\n",
    "hitrate_50_half:0.2340\n",
    "ndcg_50_half:0.0932\n",
    "### 日期:2020-04-19 00:25:12排名: 无  对user点击item按时间做指数加权处理\n",
    "\n",
    "score:0.1275\n",
    "hitrate_50_full:0.3071\n",
    "ndcg_50_full:0.1275\n",
    "hitrate_50_half:0.2171\n",
    "ndcg_50_half:0.0952\n",
    "\n",
    "### 日期:2020-04-18 20:54:35排名: 无  对user点击item按时间做线性加权处理\n",
    "\n",
    "* score:0.1353\n",
    "* hitrate_50_full:0.3291\n",
    "* ndcg_50_full:0.1353\n",
    "* hitrate_50_half:0.2454\n",
    "* ndcg_50_half:0.1010\n",
    "\n",
    "### 日期:2020-04-18 18:48:13排名: 无 phase2 use_iif=False\n",
    "\n",
    "score:0.1195\n",
    "hitrate_50_full:0.3001\n",
    "ndcg_50_full:0.1195\n",
    "hitrate_50_half:0.2228\n",
    "ndcg_50_half:0.0857\n",
    "\n",
    "### 日期:2020-04-18 18:41:10排名: 无 phase2\n",
    "\n",
    "score:0.1264\n",
    "hitrate_50_full:0.3139\n",
    "ndcg_50_full:0.1264\n",
    "hitrate_50_half:0.2331\n",
    "ndcg_50_half:0.0934\n",
    "## phase1\n",
    "### 日期:2020-04-17 16:07:36排名: 无 phase1\n",
    "\n",
    "score:0.0841\n",
    "hitrate_50_full:0.2068\n",
    "ndcg_50_full:0.0841\n",
    "hitrate_50_half:0.1581\n",
    "ndcg_50_half:0.0632\n",
    "改进：\n",
    "用所有的数据预测phasen\n",
    "初始版本是用phase0-n的数据预测phasen，而没有用到后面的数据\n",
    "\n",
    "### 日期:2020-04-17 15:22:28排名: 无\n",
    "修改了一行bug\n",
    "for j, wij in sorted(sim_item_corr[i].items(), reverse=True)[:top_k]:\n",
    "\n",
    "score:0.0787\n",
    "hitrate_50_full:0.1996\n",
    "ndcg_50_full:0.0787\n",
    "hitrate_50_half:0.1383\n",
    "ndcg_50_half:0.0556\n",
    "\n",
    "### 高手开源baseline\n",
    "https://tianchi.aliyun.com/forum/postDetail?postId=103530\n",
    "日期:2020-04-17 10:42:13\n",
    "\n",
    "score:0.0707\n",
    "hitrate_50_full:0.1784\n",
    "ndcg_50_full:0.0707\n",
    "hitrate_50_half:0.1405\n",
    "ndcg_50_half:0.0558\n",
    "### 日期:2020-04-13 18:37:27排名: 无  这么惨，因为有一个很明显的bug。。。\n",
    "\n",
    "score:0.0006\n",
    "hitrate_50_full:0.0030\n",
    "ndcg_50_full:0.0006\n",
    "hitrate_50_half:0.0011\n",
    "ndcg_50_half:0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline0_itemCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen train_data_matrix begin...\n",
      "cal cosine_similarity begin...\n",
      "cal cosine_similarity end\n",
      "100%|████████████████████████████████████| 27195/27195 [00:45<00:00, 600.60it/s]\n",
      "935.4970018863678s\n",
      "complete!!!\n"
     ]
    }
   ],
   "source": [
    "!python baseline0_itemCF.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline1_itemCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: 0\n",
      "phase: 1\n",
      "phase: 2\n",
      "phase: 3\n",
      "100%|███████████████████████████████████| 26046/26046 [00:16<00:00, 1544.30it/s]\n",
      "100%|███████████████████████████████████| 71904/71904 [00:10<00:00, 6772.11it/s]\n",
      "100%|███████████████████████████████████████| 6754/6754 [02:01<00:00, 55.63it/s]\n",
      "151.77655220031738s\n",
      "complete!!!\n"
     ]
    }
   ],
   "source": [
    "!python baseline1_itemCF.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: 0\n",
      "phase: 1\n",
      "phase: 2\n",
      "phase: 3\n",
      "100%|███████████████████████████████████| 26046/26046 [00:16<00:00, 1625.97it/s]\n",
      "100%|███████████████████████████████████| 71904/71904 [00:08<00:00, 8627.12it/s]\n",
      "100%|███████████████████████████████████████| 6754/6754 [01:52<00:00, 60.09it/s]\n",
      "140.00655221939087s\n",
      "complete!!!\n"
     ]
    }
   ],
   "source": [
    "!python baseline1_itemCF.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思路总结\n",
    "## baseline0_itemCF\n",
    "通过建立 user-item矩阵，通过用户的点击行为得出item的相似性，如item1和item2都只被u1和u2各点击过1次，那么item1和item2相似度为1\n",
    "\n",
    "推荐过程:\n",
    "已知u1点击了i1，那么拿到i1最相近的前50个item推荐\n",
    "\n",
    "\n",
    "## baseline1_itemCF\n",
    "通过建立 item-item共现矩阵，通过item1和item2被同一user同时点击来计算item的相似性，如u1点击过i1和i2，则对i1和i2共现次数贡献1，如果u2也点击过i1和i2，则对i1和i2共现次数贡献+1\n",
    "\n",
    "推荐过程：\n",
    "并没有简单的将user最近一次购买的item共现系数最高的前50推荐，而是考虑了user历史购买过的所有item，把与这些item共现系数最高的topk=500个item都找出来，然后取出其中 user没有购买过的 共现系数最高的前50个做推荐，如果不足50个，则用所有点击中最高的前50item补足\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题：\n",
    "1. 没有考虑user，即u1点击i1和u2点击i1的推荐结果一样\n",
    "baseline0_itemCF\n",
    "2. 没有考虑user的历史点击行为，即u1在点击i1之前点击了i2或者i3的推荐结果一样\n",
    "baseline0_itemCF\n",
    "3. 没有考虑user自身特征\n",
    "baseline0_itemCF\n",
    "baseline1_itemCF\n",
    "4. 没有考虑item自身特征\n",
    "baseline0_itemCF\n",
    "baseline1_itemCF\n",
    "5. 没有考虑user对item的点击次数影响，即u1对i1点击过1次、对i2点击过1次和u2对i1点击过100次、对i2点击过100次，在计算i1和i2的共现系数时效果相同\n",
    "baseline1_itemCF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 待改进\n",
    "### baseline0_itemCF\n",
    "建立 user-item矩阵，\n",
    "点击过程需要做平滑，如u1对i1点击1次，和u1对i1点击10000次要做平滑，如在计算完点击次数后，取log(cnt+1)，这样，点击0次，为0，点击>0次，结果大于0\n",
    "\n",
    "推荐过程可以参照baseline1_itemCF的做改进\n",
    "取出历史购买过的所有item，然后再在里面取出前50做推荐，而不是只考虑最近点击过的一个item\n",
    "\n",
    "### baseline1_itemCF\n",
    "推荐过程 改进\n",
    "1. item添加权重 第一次点击的item是最后一次点击item权重的0.5 done  \n",
    "分数明显改善！  \n",
    "如一个用户共点击过100个item，那么最开始点击的item1，肯定要比最近点击的item100对推荐的影响要小，因此item1的相似的topk个物品的权重要小于item100的相似的topk个物品的权重  \n",
    "本来想着可以用time作为权重，但是它们的差异实在太小了！  \n",
    "算了，自己造一个把，最开始点击的item的权重是0.5，最近一次点击的item权重取1，如果item被user点击了多次，则去最近点击的那次\n",
    "2. item权重调优  优先级调低，后面再做！  \n",
    "相同time的权重应该相同，看了一下，应该是没有相同的time。。。\n",
    "\n",
    "权重应该和time的值有关系，而不是排序。如，有2个值的time序列，[5, 6], [5, 10], [5, 20], 目前的处理方式，第一个的权重都是第二个权重的0.5，但显然[5, 6]中第一个值权重应该大于[5, 10]中第一个值权重应该大于[5, 20]中第一个值权重。\n",
    "\n",
    "线性衰减，也可以考虑指数衰减，相差越大 衰减的越快！ 指数衰减貌似更接近实际情况\n",
    "指数衰减不太试了一下，效果不好，可能区间太大了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python  \n",
    "# -*- coding:utf-8 -*-  \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "from tqdm import tqdm  \n",
    "from collections import defaultdict  \n",
    "import math  \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "  \n",
    "def get_sim_item(df, user_col, item_col, use_iif=False, mode='item_item'): \n",
    "    \"\"\"\n",
    "    use_iif:\n",
    "        True: 把用户点击过的item总数考虑进去，\n",
    "            如果用户1和用户2都点击了item[1, 2]，但是用户1一共就点击过10个item，而用户2一共点击过10000个item\n",
    "            那么在计算item[1, 2]的共现系数时，用户1肯定比用户2的价值大\n",
    "    \"\"\"\n",
    "    user_item_dict = None\n",
    "    if mode == 'item_item':\n",
    "        user_item_ = df.groupby(user_col)[item_col].agg(set).reset_index()  \n",
    "        user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))  \n",
    "\n",
    "        sim_item = {}  # sim_item[i][j] 表示item_i和item_j共现的次数\n",
    "        item_cnt = defaultdict(int)  # 统计某个item有多少个user购买过\n",
    "        for user, items in tqdm(user_item_dict.items()):  \n",
    "            for i in items:  \n",
    "                item_cnt[i] += 1  \n",
    "                sim_item.setdefault(i, {})  \n",
    "                for relate_item in items:  \n",
    "                    if i == relate_item:  \n",
    "                        continue  \n",
    "                    sim_item[i].setdefault(relate_item, 0)  \n",
    "                    if not use_iif:  \n",
    "                        sim_item[i][relate_item] += 1  \n",
    "                    else:  \n",
    "                        sim_item[i][relate_item] += 1 / math.log(1 + len(items))  \n",
    "\n",
    "        sim_item_corr = sim_item.copy()  \n",
    "        for i, related_items in tqdm(sim_item.items()):  \n",
    "            for j, cij in related_items.items():  \n",
    "                # item_i和item_j共现的次数/各自出现的次数之积\n",
    "                sim_item_corr[i][j] = cij/math.sqrt(item_cnt[i]*item_cnt[j])  \n",
    "    elif mode == 'user_item':\n",
    "        click_df = df\n",
    "        click_df = click_df.drop_duplicates()\n",
    "\n",
    "        n_users = click_df.user_id.nunique()\n",
    "\n",
    "        n_items = click_df.item_id.nunique()\n",
    "\n",
    "        print('gen train_data_matrix begin...')\n",
    "        train_data_matrix = np.zeros((n_users, n_items))\n",
    "        user_id_dict, item_id_dict = dict(), dict()\n",
    "        user_id_dict_inv, item_id_dict_inv = dict(), dict()\n",
    "        u_cnt, i_cnt = 0, 0\n",
    "        for line in click_df.itertuples():\n",
    "            if line.user_id not in user_id_dict:\n",
    "                user_id_dict[line.user_id] = u_cnt\n",
    "                user_id_dict_inv[u_cnt] = line.user_id\n",
    "                u_cnt += 1\n",
    "            if line.item_id not in item_id_dict:\n",
    "                item_id_dict[line.item_id] = i_cnt\n",
    "                item_id_dict_inv[i_cnt] = line.item_id\n",
    "                i_cnt += 1\n",
    "            train_data_matrix[user_id_dict[line.user_id], item_id_dict[line.item_id]] += 1\n",
    "        train_data_matrix = np.log(train_data_matrix + 1)\n",
    "        print('cal cosine_similarity begin...')  \n",
    "        #所有test中出现的user购买过的item_id 改为click_test中user购买的item_id\n",
    "#         all_test_item_id = df.loc[df.user_id.isin(click_test.user_id.unique().tolist()), 'item_id'].unique().tolist()\n",
    "        all_test_item_id = click_test.item_id.unique().tolist()\n",
    "        # 把item_id转换为matrix index\n",
    "        tt = [item_id_dict[i] for i in all_test_item_id]    \n",
    "        print(len(tt))\n",
    "        t = cosine_similarity(train_data_matrix.T, train_data_matrix[:, tt].T)\n",
    "#         xb = train_data_matrix.T\n",
    "#         xq = train_data_matrix[:, tt].T\n",
    "#         import faiss                   # make faiss available\n",
    "# #         faiss.normalize_L2(xb)\n",
    "# #         faiss.normalize_L2(xq)\n",
    "#         index = faiss.IndexFlatIP(n_users)   # build the index\n",
    "#         print(index.is_trained)\n",
    "#         index.add(xb)                  # add vectors to the index\n",
    "#         print(index.ntotal)\n",
    "#         D, I = index.search(xq, 500)\n",
    "        print('cal cosine_similarity end')\n",
    "        dft = pd.DataFrame(t)\n",
    "        dft.to_pickle('dft.pkl')\n",
    "#         dft = pd.read_pickle('dft.pkl')\n",
    "        print(dft.shape)\n",
    "        sim_item_corr = dict()\n",
    "#         for i in tqdm(range(I.shape[0])):\n",
    "#             sim_item_corr[item_id_dict_inv[tt[i]]] = {item_id_dict_inv[v]: D[i, ii] for ii, v in enumerate(I[i, 1:])}\n",
    "        for i in tqdm(range(dft.shape[1])):\n",
    "            sim_item_corr[item_id_dict_inv[tt[i]]] = {item_id_dict_inv[ii]: v \n",
    "                                                      for ii, v in dft.iloc[:, i].sort_values(ascending=False).iloc[:501].items()}\n",
    "    return sim_item_corr, user_item_dict  \n",
    "  \n",
    "\n",
    "def recommend(sim_item_corr, user_item_dict, user_id, top_k, item_num, weight_mode='linear'):  \n",
    "    \"\"\"\n",
    "    向user_id推荐item_num个该user_id没有买过的item\n",
    "    \"\"\"\n",
    "    rank = {}  \n",
    "    # 该user_id购买过的items\n",
    "    dft = click_test[click_test.user_id == user_id].sort_values('time').drop_duplicates('item_id', keep='last')\n",
    "    if weight_mode=='linear':\n",
    "        dft['t'] = range(dft.shape[0], dft.shape[0] * 2)\n",
    "        dft['t'] = dft['t'] - dft.shape[0] * 3//4\n",
    "    elif weight_mode=='exp':\n",
    "        dft['t'] = ((dft['time'] - 0.9837) * 10000).map(math.exp) # 最大值是最小值的15倍左右，score:0.1275\n",
    "        \n",
    "    interacted_items = dft['item_id'].tolist()    \n",
    "    weights = dft['t'].tolist()\n",
    "#     print(dft.shape[0], dft.head())\n",
    "    # 遍历该user购买过的items，\n",
    "    cnt = 0\n",
    "    for i in interacted_items:  \n",
    "        # 遍历该item共现最高的top_k个item，把其中用户没有买过的加入推荐列表\n",
    "#         for j, wij in sorted(sim_item_corr[i].items(), reverse=True)[:top_k]:  \n",
    "        for j, wij in sorted(sim_item_corr[i].items(), key=lambda d: d[1], reverse=True)[:top_k]:  \n",
    "            if j not in interacted_items:  \n",
    "                rank.setdefault(j, 0)  \n",
    "                rank[j] += wij * weights[cnt]\n",
    "        cnt += 1\n",
    "    return sorted(rank.items(), key=lambda d: d[1], reverse=True)[:item_num]  \n",
    "  \n",
    "def get_predict(df, pred_col, top_fill):  \n",
    "    \"\"\"\n",
    "    fill user to 50 items\n",
    "    逻辑就是如果推荐给用户的items少于50个，就用点击数最高的item补足\n",
    "    \"\"\"\n",
    "    top_fill = [int(t) for t in top_fill.split(',')]  \n",
    "    scores = [-1 * i for i in range(1, len(top_fill) + 1)]  \n",
    "    ids = list(df['user_id'].unique())  \n",
    "    fill_df = pd.DataFrame(ids * len(top_fill), columns=['user_id'])  \n",
    "    fill_df.sort_values('user_id', inplace=True)  \n",
    "    fill_df['item_id'] = top_fill * len(ids)  \n",
    "    fill_df[pred_col] = scores * len(ids)  \n",
    "    df = df.append(fill_df)  \n",
    "    df.sort_values(pred_col, ascending=False, inplace=True)  \n",
    "    df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')  \n",
    "    df['rank'] = df.groupby('user_id')[pred_col].rank(method='first', ascending=False)  \n",
    "    df = df[df['rank'] <= 50]  \n",
    "    df = df.groupby('user_id')['item_id'].apply(lambda x: ','.join([str(i) for i in x])).str.split(',', expand=True).reset_index()  \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(now_phase):\n",
    "#     now_phase = 2  \n",
    "    train_path = './data_origin/underexpose_train'  \n",
    "    test_path = './data_origin/underexpose_test'  \n",
    "    recom_item = []  \n",
    "\n",
    "    whole_click = pd.DataFrame()  \n",
    "    click_train = pd.DataFrame()   \n",
    "    click_test = pd.DataFrame()  \n",
    "    test_qtime = pd.DataFrame()  \n",
    "    for c in range(now_phase + 1):  \n",
    "        print('phase:', c)  \n",
    "        click_train1 = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "        click_test1 = pd.read_csv(test_path + '/underexpose_test_click-{}/underexpose_test_click-{}.csv'.format(c, c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "        test_qtime1 = pd.read_csv(test_path + '/underexpose_test_click-{}/underexpose_test_qtime-{}.csv'.format(c, c), header=None,  names=['user_id','query_time'])  \n",
    "\n",
    "        click_train = click_train.append(click_train1) \n",
    "    #     all_click = click_train.append(click_test1)  \n",
    "        click_test = click_test.append(click_test1) \n",
    "        test_qtime = test_qtime.append(test_qtime1) \n",
    "\n",
    "#     # 去掉 train中time>query_time的数据    \n",
    "#     click_train = pd.merge(click_train, test_qtime, how='left').fillna(10)  \n",
    "#     click_train = click_train[click_train.time <= click_train.query_time]\n",
    "#     del click_train['query_time']\n",
    "    whole_click = click_train.append(click_test)  \n",
    "    whole_click = whole_click.drop_duplicates()\n",
    "    return whole_click, click_train, click_test, test_qtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: 0\n",
      "phase: 1\n",
      "phase: 2\n",
      "phase: 3\n",
      "gen train_data_matrix begin...\n",
      "cal cosine_similarity begin...\n",
      "42800\n",
      "cal cosine_similarity end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/42800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71904, 42800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42800/42800 [10:31<00:00, 67.76it/s]\n",
      "100%|██████████| 6754/6754 [01:03<00:00, 106.67it/s]\n"
     ]
    }
   ],
   "source": [
    "now_phase = 3\n",
    "whole_click, click_train, click_test, test_qtime = load_data(now_phase)\n",
    "# item_sim_list, user_item = get_sim_item(whole_click, 'user_id', 'item_id', use_iif=True, mode='item_item')\n",
    "item_sim_list, user_item = get_sim_item(whole_click, 'user_id', 'item_id', use_iif=True, mode='user_item')   \n",
    "\n",
    "\n",
    "recom_item = [] \n",
    "for i in tqdm(click_test['user_id'].unique()):  \n",
    "    rank_item = recommend(item_sim_list, user_item, i, 500, 50)  \n",
    "    for j in rank_item:  \n",
    "        recom_item.append([i, j[0], j[1]])  \n",
    "# find most popular items  \n",
    "top50_click = whole_click['item_id'].value_counts().index[:50].values  \n",
    "top50_click = ','.join([str(i) for i in top50_click])  \n",
    "\n",
    "recom_df = pd.DataFrame(recom_item, columns=['user_id', 'item_id', 'sim'])  \n",
    "result = get_predict(recom_df, 'sim', top50_click)  \n",
    "\n",
    "result.to_csv('/Users/luoyonggui/Downloads/baseline1_itemcf2.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重新来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def get_sim_item(df, user_col, item_col, use_iif=False): \n",
    "    \"\"\"\n",
    "    use_iif:\n",
    "        True: 把用户点击过的item总数考虑进去，\n",
    "            如果用户1和用户2都点击了item[1, 2]，但是用户1一共就点击过10个item，而用户2一共点击过10000个item\n",
    "            那么在计算item[1, 2]的共现系数时，用户1肯定比用户2的价值大\n",
    "    \"\"\"\n",
    "    click_df = df\n",
    "    click_df = click_df.drop_duplicates()\n",
    "\n",
    "    n_users = click_df.user_id.nunique()\n",
    "\n",
    "    n_items = click_df.item_id.nunique()\n",
    "\n",
    "    print('gen train_data_matrix begin...')\n",
    "#     train_data_matrix = np.zeros((n_users, n_items))\n",
    "    import scipy.sparse as sp\n",
    "    train_data_matrix = sp.lil_matrix((n_users, n_items))\n",
    "    user_id_dict, item_id_dict = dict(), dict()\n",
    "    user_id_dict_inv, item_id_dict_inv = dict(), dict()\n",
    "    u_cnt, i_cnt = 0, 0\n",
    "    for line in click_df.itertuples():\n",
    "        if line.user_id not in user_id_dict:\n",
    "            user_id_dict[line.user_id] = u_cnt\n",
    "            user_id_dict_inv[u_cnt] = line.user_id\n",
    "            u_cnt += 1\n",
    "        if line.item_id not in item_id_dict:\n",
    "            item_id_dict[line.item_id] = i_cnt\n",
    "            item_id_dict_inv[i_cnt] = line.item_id\n",
    "            i_cnt += 1\n",
    "        train_data_matrix[user_id_dict[line.user_id], item_id_dict[line.item_id]] += 1\n",
    "    train_data_matrix = train_data_matrix.tocsc()\n",
    "    train_data_matrix.data = np.log(train_data_matrix.data + 1)\n",
    "#     train_data_matrix = np.log(train_data_matrix + 1) + np.random.RandomState(14).uniform(size=(n_users, n_items))*0.001\n",
    "    print(f'cal cosine_similarity begin...{datetime.now()}')  \n",
    "    #所有test中出现的user购买过的item_id 改为click_test中user购买的item_id\n",
    "    all_test_item_id = df.loc[df.user_id.isin(click_test.user_id.unique().tolist()), 'item_id'].unique().tolist()\n",
    "#     all_test_item_id = click_test.item_id.unique().tolist()\n",
    "    # 把item_id转换为matrix index\n",
    "    tt = [item_id_dict[i] for i in all_test_item_id]    \n",
    "    print(len(tt))\n",
    "    t = cosine_similarity(train_data_matrix.T, train_data_matrix[:, tt].T)\n",
    "    dft = pd.DataFrame(t)\n",
    "#     dft.to_pickle('dft.pkl')\n",
    "#   dft = pd.read_pickle('dft.pkl')\n",
    "#     xb = train_data_matrix.T\n",
    "#     xq = train_data_matrix[:, tt].T\n",
    "#     import faiss                   # make faiss available\n",
    "#     faiss.normalize_L2(xb)\n",
    "#     faiss.normalize_L2(xq)\n",
    "#     index = faiss.IndexFlatIP(n_users)   # build the index\n",
    "#     print(index.is_trained)\n",
    "#     index.add(xb)                  # add vectors to the index\n",
    "#     print(index.ntotal)\n",
    "#     D, I = index.search(xq, 500)\n",
    "    print(f'cal cosine_similarity end..{datetime.now()}')\n",
    "#     print(dft.shape)\n",
    "    sim_item_corr = dict()\n",
    "#     for i in tqdm(range(I.shape[0])):\n",
    "#         sim_item_corr[item_id_dict_inv[tt[i]]] = {item_id_dict_inv[v]: D[i, ii] for ii, v in enumerate(I[i, 1:])}\n",
    "    for i in tqdm(range(dft.shape[1])):\n",
    "        sim_item_corr[item_id_dict_inv[tt[i]]] = {item_id_dict_inv[ii]: v \n",
    "                                                  for ii, v in dft.iloc[:, i].sort_values(ascending=False).iloc[:501].items()}\n",
    "    return sim_item_corr, user_item_dict  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_item(df, user_col, item_col, use_iif=False):  \n",
    "    user_item_ = df.groupby(user_col)[item_col].agg(set).reset_index()  \n",
    "    user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))  \n",
    "    \n",
    "    item_user_ = df.groupby(item_col)[user_col].agg(set).reset_index()  \n",
    "    item_user_dict = dict(zip(item_user_[item_col], item_user_[user_col]))    \n",
    "\n",
    "    item_cnt = defaultdict(int)  \n",
    "    for user, items in tqdm(user_item_dict.items()):  \n",
    "        for i in items:  \n",
    "            item_cnt[i] += 1  \n",
    "\n",
    "    sim_item = {}\n",
    "\n",
    "    for item, users in tqdm(item_user_dict.items()):\n",
    "    \n",
    "        sim_item.setdefault(item, {}) \n",
    "    \n",
    "        for u in users:\n",
    "        \n",
    "            tmp_len = len(user_item_dict[u])\n",
    "        \n",
    "            for relate_item in user_item_dict[u]:\n",
    "                sim_item[item].setdefault(relate_item, 0)\n",
    "                sim_item[item][relate_item] += 1/ (math.log(len(users)+1) * math.log(tmp_len+1))\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "    return sim_item, user_item_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils import *\n",
    "\n",
    "import pandas as pd  \n",
    "from tqdm import tqdm  \n",
    "from collections import defaultdict  \n",
    "import math  \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "  \n",
    "def get_sim_item(df, user_col, item_col, use_iif=False):  \n",
    "    user_item_ = df.groupby(user_col)[item_col].agg(set).reset_index()  \n",
    "    user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))  \n",
    "  \n",
    "    sim_item = {}  \n",
    "    item_cnt = defaultdict(int)  \n",
    "    for user, items in tqdm(user_item_dict.items()):  \n",
    "        for i in items:  \n",
    "            item_cnt[i] += 1  \n",
    "            sim_item.setdefault(i, {})  \n",
    "            for relate_item in items:  \n",
    "                if i == relate_item:  \n",
    "                    continue  \n",
    "                sim_item[i].setdefault(relate_item, 0)  \n",
    "                if not use_iif:  \n",
    "                    sim_item[i][relate_item] += 1  \n",
    "                else:  \n",
    "                    sim_item[i][relate_item] += 1 / math.log(1 + len(items))  \n",
    "    sim_item_corr = sim_item.copy()  \n",
    "    for i, related_items in tqdm(sim_item.items()):  \n",
    "        for j, cij in related_items.items():  \n",
    "#             sim_item_corr[i][j] = cij/math.sqrt(item_cnt[i]*item_cnt[j]) \n",
    "            sim_item_corr[i][j] = cij/(np.log(item_cnt[i]+1) * np.log(item_cnt[j]+1)) \n",
    "#             sim_item_corr[i][j] = cij/np.log((item_cnt[i]+1)*(item_cnt[j]+1)) \n",
    "  \n",
    "    return sim_item_corr, user_item_dict  \n",
    "\n",
    "\n",
    "def recommend(whole_click_train_, sim_item_corr, user_item_dict, user_id, top_k, item_num, weight_mode='linear'):  \n",
    "    rank = {}  \n",
    "    # 该user_id购买过的items\n",
    "    dft = whole_click_train_[whole_click_train_.user_id == user_id].sort_values('time').drop_duplicates('item_id', keep='last')\n",
    "    if weight_mode=='linear':\n",
    "        dft['t'] = range(dft.shape[0], dft.shape[0] * 2)\n",
    "        dft['t'] = dft['t'] - dft.shape[0] // 2 - dft.shape[0] // 4\n",
    "#         dft['t'] = dft['t'] - dft.shape[0] // 2 - dft.shape[0] // 4 - dft.shape[0] // 8\n",
    "#         dft['t'] = range(1, dft.shape[0] + 1)\n",
    "#         dft['t'] = range(0, dft.shape[0])\n",
    "    elif weight_mode=='exp':\n",
    "        dft['t'] = ((dft['time'] - 0.9837) * 10000).map(math.exp) # 最大值是最小值的15倍左右，score:0.1275\n",
    "        \n",
    "    interacted_items = dft['item_id'].tolist()    \n",
    "    weights = dft['t'].tolist()\n",
    "    cnt = 0\n",
    "    for i in interacted_items:  \n",
    "        for j, wij in sorted(sim_item_corr[i].items(), key=lambda d: d[1], reverse=True)[0:top_k]:  \n",
    "            if j not in interacted_items:  \n",
    "                rank.setdefault(j, 0)  \n",
    "                rank[j] += wij  * weights[cnt]\n",
    "        cnt += 1\n",
    "    return sorted(rank.items(), key=lambda d: d[1], reverse=True)[:item_num]  \n",
    "  \n",
    "\n",
    "# fill user to 50 items  \n",
    "def get_predict(df, pred_col, top_fill):  \n",
    "    top_fill = [int(t) for t in top_fill.split(',')]  \n",
    "    scores = [-1 * i for i in range(1, len(top_fill) + 1)]  \n",
    "    ids = list(df['user_id'].unique())  \n",
    "    fill_df = pd.DataFrame(ids * len(top_fill), columns=['user_id'])  \n",
    "    fill_df.sort_values('user_id', inplace=True)  \n",
    "    fill_df['item_id'] = top_fill * len(ids)  \n",
    "    fill_df[pred_col] = scores * len(ids)  \n",
    "    df = df.append(fill_df)  \n",
    "    df.sort_values(pred_col, ascending=False, inplace=True)  \n",
    "    df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')  \n",
    "    df['rank'] = df.groupby('user_id')[pred_col].rank(method='first', ascending=False)  \n",
    "    df = df[df['rank'] <= 50]  \n",
    "    df = df.groupby('user_id')['item_id'].apply(lambda x: ','.join([str(i) for i in x])).str.split(',', expand=True).reset_index()  \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(whole_click_train_, answers_, test_qtime_):\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    recom_item = []  \n",
    "    item_sim_list, user_item = get_sim_item(whole_click_train_, 'user_id', 'item_id', use_iif=True)  \n",
    "\n",
    "    for i in tqdm(test_qtime_['user_id'].unique()):  \n",
    "        rank_item = recommend(whole_click_train_, item_sim_list, user_item, i, 500, 50)  \n",
    "        for j in rank_item:  \n",
    "            recom_item.append([i, j[0], j[1]])  \n",
    "    # find most popular items  \n",
    "    top50_click = whole_click_train_['item_id'].value_counts().index[:50].values  \n",
    "    top50_click = ','.join([str(i) for i in top50_click])  \n",
    "\n",
    "    recom_df = pd.DataFrame(recom_item, columns=['user_id', 'item_id', 'sim'])  \n",
    "    result = get_predict(recom_df, 'sim', top50_click)  \n",
    "    \n",
    "    result1 = result.set_index('user_id')\n",
    "    result1 = result1.applymap(int)\n",
    "    result1['p'] = result1.values.tolist()\n",
    "    print(evaluate_each_phase(result1['p'].to_dict(), answers_))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f'{t1-t0}s')\n",
    "    return result, item_sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: 0\n",
      "phase: 1\n",
      "phase: 2\n",
      "phase: 3\n",
      "phase: 4\n"
     ]
    }
   ],
   "source": [
    "now_phase = 4\n",
    "whole_click_train, click_test_val, test_qtime, all_click_df = load_click_data(now_phase)\n",
    "\n",
    "val = click_test_val[['user_id', 'item_id', 'item_deg']].set_index('user_id')\n",
    "val['p'] = val.values.tolist()\n",
    "answers = val['p'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27768/27768 [00:21<00:00, 1290.86it/s]\n",
      "100%|██████████| 80750/80750 [01:33<00:00, 859.11it/s] \n",
      "100%|██████████| 8462/8462 [04:32<00:00, 31.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08650437 0.03250136 0.0629975  0.02177384]\n",
      "390.4417133331299s\n"
     ]
    }
   ],
   "source": [
    "result, item_sim_list = main(whole_click_train, answers, test_qtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27768/27768 [00:21<00:00, 1271.39it/s]\n",
      "100%|██████████| 80750/80750 [01:40<00:00, 803.15it/s] \n",
      "100%|██████████| 8462/8462 [04:31<00:00, 31.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "396.76055121421814s\n"
     ]
    }
   ],
   "source": [
    "result = main(whole_click_train.append(click_test_val), answers, test_qtime)\n",
    "result[0].to_csv('/Users/luoyonggui/Downloads/baseline1_itemcf24.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase4\n",
    "[0.08650437 0.03250136 0.0629975  0.02177384]\n",
    "0.6090 0.2496 0.3724 0.1374  --online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08424637, 0.03159376, 0.07090602, 0.0267413 ], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 原始的qyxs版本\n",
    "# array([0.05567071, 0.02003306, 0.05627462, 0.02107223], dtype=float32)\n",
    "# 0.1 修改了其中的排序bug\n",
    "# array([0.06944034, 0.02491091, 0.05993247, 0.02189971], dtype=float32)\n",
    "# 0.1.1 get_sim_item1 sim_item_corr[i][j] = cij / (np.log(item_cnt[i]+1) * np.log(item_cnt[j]+1))\n",
    "# array([0.07817589, 0.02840868, 0.05374226, 0.01814048], dtype=float32)\n",
    "# 0.1.1.1 添加线性时间衰减 0.5\n",
    "# [0.08779982, 0.03312078, 0.06077659, 0.02157981], dtype=float32)\n",
    "# 0.1.1.2 添加线性时间衰减 0.5 //2 //4\n",
    "# [0.09076103 0.03535827 0.06499719 0.02348245]\n",
    "# 0.4925 0.2029 0.3096 0.1181  --------------------online\n",
    "# 0.1.1.3 添加线性时间衰减 0.5 //2 //4 //8\n",
    "# [0.09135327 0.03565276 0.06527856 0.02362418]\n",
    "# 0.1.1.4 添加线性时间衰减  range(1, dft.shape[0] + 1)\n",
    "# [0.09327806 0.03668222 0.0658413  0.02404739]\n",
    "# 0.4865 0.2010 0.3061 0.1167   --------------------online\n",
    "# 0.1.1.5 添加线性时间衰减  range(0, dft.shape[0])\n",
    "# [0.09150133 0.03644358 0.06443444 0.02380092]\n",
    "# 0.1.1.2.1  get_sim_item1 sim_item_corr[i][j] = cij/np.log((item_cnt[i]+1)*(item_cnt[j]+1)) \n",
    "# [0.08794788 0.03424193 0.04895892 0.01559871]\n",
    "# 0.1.2 添加线性时间衰减 //2 //4\n",
    "# array([0.08424637, 0.03159376, 0.07090602, 0.0267413 ], dtype=float32)\n",
    "# 0.2 双向网络\n",
    "# 0.2005 0.4871 0.2005 0.2059 0.0678\n",
    "# 0.3 改进版，没有修改排序bug\n",
    "# array([0.03050044, 0.01043385, 0.03685988, 0.01230763], dtype=float32)\n",
    "# 改进版，效果有点出乎意料啊。。。\n",
    "# array([0.03642286, 0.01217174, 0.03629713, 0.01195004], dtype=float32)\n",
    "# 感觉recommend的减少权重有点过 0.7**loc，先去掉看看，效果稍好\n",
    "# array([0.03997631, 0.0140847 , 0.01660101, 0.00548362], dtype=float32)\n",
    "# 修正逆向位置权重 0.5 * (0.9**(loc1-loc2-1))\n",
    "# array([0.06114895, 0.02198378, 0.03292065, 0.01053025], dtype=float32)\n",
    "# 修正逆向位置权重 0.8 * (0.9**(loc1-loc2-1)) \n",
    "# array([0.06499852, 0.02380453, 0.03376477, 0.01079678], dtype=float32)\n",
    "# get_sim_item1 sim_item_corr[i][j] = cij / ((item_cnt[i] * item_cnt[j]))\n",
    "# array([0.03523838, 0.01321244, 0.05486776, 0.02201659], dtype=float32)\n",
    "# get_sim_item1 sim_item_corr[i][j] = cij / (np.log(item_cnt[i]+1) * np.log(item_cnt[j]+1))\n",
    "# array([0.06440628, 0.02371753, 0.04361283, 0.01563565], dtype=float32)\n",
    "evaluate_each_phase(result1['p'].to_dict(), answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: 0\n",
      "phase: 1\n",
      "phase: 2\n",
      "phase: 3\n",
      "phase: 4\n"
     ]
    }
   ],
   "source": [
    "now_phase = 4\n",
    "whole_click_train, click_test_val, test_qtime, all_click_df = load_click_data(now_phase)\n",
    "\n",
    "val = click_test_val[['user_id', 'item_id', 'item_deg']].set_index('user_id')\n",
    "val['p'] = val.values.tolist()\n",
    "answers = val['p'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_col = 'user_id'\n",
    "item_col = 'item_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27768it [34:59, 13.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_item_item_matrix(click_df, user_col, item_col, use_iif=False):\n",
    "    click_df_ = click_df.copy()\n",
    "    click_df_ = click_df_.sort_values('time')\n",
    "    n_items = click_df_.item_id.nunique()\n",
    "    print(f'n_items: {n_items}')\n",
    "    user_item_ = click_df_.groupby(user_col)[item_col, 'time'].agg(list).reset_index() \n",
    "    click_time_std = click_df_.time.describe().loc['std']\n",
    "    item_cnt = click_df_.groupby(item_col)['time'].count()\n",
    "    train_data_matrix = sp.lil_matrix((n_items, n_items))\n",
    "    print(train_data_matrix.shape)\n",
    "    cnt = 0\n",
    "    item_id_dict, item_id_dict_inv = dict(), dict()\n",
    "    for line in tqdm(user_item_.itertuples()):\n",
    "        for i, item in enumerate(line.item_id):\n",
    "    #         print(i, item)\n",
    "    #         break\n",
    "            if item not in item_id_dict:\n",
    "                item_id_dict[item] = cnt\n",
    "                item_id_dict_inv[cnt] = item\n",
    "                cnt += 1\n",
    "            tmp = 0\n",
    "            for j, relate_item in enumerate(line.item_id):\n",
    "                if item == relate_item: continue\n",
    "                if relate_item not in item_id_dict:\n",
    "                    item_id_dict[relate_item] = cnt\n",
    "                    item_id_dict_inv[cnt] = relate_item\n",
    "                    cnt += 1\n",
    "                tmp += 1\n",
    "                # 如果用户a共点击了n个items， 那n越大，共现的价值越小  \n",
    "                tmp /= math.log(1 + len(line.item_id)) \n",
    "                # items点击的time间隔越远，共现价值越小\n",
    "                # items的先后顺序，先a后b，则b的价值高，先b后a，则b的价值低\n",
    "                alpha = 1\n",
    "                if line.time[i] < line.time[j]:\n",
    "                    alpha = 0.9\n",
    "                tmp = alpha * 1/math.exp(math.fabs(line.time[i]-line.time[j])/click_time_std)\n",
    "                # items一共被点击的次数，被点击次数越多，价值越低\n",
    "    #             print(item_cnt.loc[item])\n",
    "                tmp /= (math.log(item_cnt.loc[item]+1)*math.log(item_cnt.loc[relate_item]+1))\n",
    "    #             print(item_id_dict[item], item_id_dict[relate_item], tmp)\n",
    "                train_data_matrix[item_id_dict[item],item_id_dict[relate_item]] += tmp\n",
    "    pd.Series(item_id_dict).to_pickle('data_gen/item_id_series.pkl')\n",
    "    pd.Series(item_id_dict_inv).to_pickle('data_gen/item_id_series_inv.pkl')\n",
    "    sp.save_npz('data_gen/item_item_train_phase4.npz', train_data_matrix.tocsc())\n",
    "    return train_data_matrix, item_id_dict, item_id_dict_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.save_npz('data_gen/item_item_train_phase4.npz', train_data_matrix_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(whole_click_train_, train_data_matrix1, item_id_dict, item_id_dict_inv, user_id, top_k, item_num, weight_mode='linear'):  \n",
    "    rank = {}  \n",
    "    # 该user_id购买过的items\n",
    "    dft = whole_click_train_[whole_click_train_.user_id == user_id].sort_values('time').drop_duplicates('item_id', keep='last')\n",
    "    if weight_mode=='linear':\n",
    "        dft['t'] = range(dft.shape[0], dft.shape[0] * 2)\n",
    "#         dft['t'] = dft['t'] - dft.shape[0] // 2 - dft.shape[0] // 4\n",
    "#         dft['t'] = dft['t'] - dft.shape[0] // 2 - dft.shape[0] // 4 - dft.shape[0] // 8\n",
    "#         dft['t'] = range(1, dft.shape[0] + 1)\n",
    "#         dft['t'] = range(0, dft.shape[0])\n",
    "    elif weight_mode=='exp':\n",
    "        dft['t'] = ((dft['time'] - 0.9837) * 10000).map(math.exp) # 最大值是最小值的15倍左右，score:0.1275\n",
    "        \n",
    "    interacted_items = dft['item_id'].tolist()    \n",
    "    weights = dft['t'].tolist()\n",
    "    cnt = 0\n",
    "    for i in interacted_items:  \n",
    "        st = pd.Series(train_data_matrix1.data[item_id_dict[i]], index=train_data_matrix1.rows[item_id_dict[i]])\n",
    "        for j, wij in st.sort_values().iloc[0:top_k].items():  \n",
    "            if item_id_dict_inv[j] not in interacted_items:  \n",
    "                rank.setdefault(item_id_dict_inv[j], 0)  \n",
    "                rank[item_id_dict_inv[j]] += wij  * weights[cnt]\n",
    "        cnt += 1\n",
    "    return sorted(rank.items(), key=lambda d: d[1], reverse=True)[:item_num]  \n",
    "  \n",
    "\n",
    "# fill user to 50 items  \n",
    "def get_predict(df, pred_col, top_fill):  \n",
    "    top_fill = [int(t) for t in top_fill.split(',')]  \n",
    "    scores = [-1 * i for i in range(1, len(top_fill) + 1)]  \n",
    "    ids = list(df['user_id'].unique())  \n",
    "    fill_df = pd.DataFrame(ids * len(top_fill), columns=['user_id'])  \n",
    "    fill_df.sort_values('user_id', inplace=True)  \n",
    "    fill_df['item_id'] = top_fill * len(ids)  \n",
    "    fill_df[pred_col] = scores * len(ids)  \n",
    "    df = df.append(fill_df)  \n",
    "    df.sort_values(pred_col, ascending=False, inplace=True)  \n",
    "    df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')  \n",
    "    df['rank'] = df.groupby('user_id')[pred_col].rank(method='first', ascending=False)  \n",
    "    df = df[df['rank'] <= 50]  \n",
    "    df = df.groupby('user_id')['item_id'].apply(lambda x: ','.join([str(i) for i in x])).str.split(',', expand=True).reset_index()  \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(whole_click_train_, answers_, test_qtime_):\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    recom_item = []  \n",
    "#     item_sim_list, user_item = get_sim_item(whole_click_train_, 'user_id', 'item_id', use_iif=True)  \n",
    "\n",
    "    for i in tqdm(test_qtime_['user_id'].unique()):  \n",
    "        rank_item = recommend(whole_click_train_, train_data_matrix, item_id_dict, item_id_dict_inv,  i, 500, 50)  \n",
    "        for j in rank_item:  \n",
    "            recom_item.append([i, j[0], j[1]])  \n",
    "    # find most popular items  \n",
    "    top50_click = whole_click_train_['item_id'].value_counts().index[:50].values  \n",
    "    top50_click = ','.join([str(i) for i in top50_click])  \n",
    "\n",
    "    recom_df = pd.DataFrame(recom_item, columns=['user_id', 'item_id', 'sim'])  \n",
    "    result = get_predict(recom_df, 'sim', top50_click)  \n",
    "    \n",
    "    result1 = result.set_index('user_id')\n",
    "    result1 = result1.applymap(int)\n",
    "    result1['p'] = result1.values.tolist()\n",
    "    print(evaluate_each_phase(result1['p'].to_dict(), answers_))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f'{t1-t0}s')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8462/8462 [06:45<00:00, 20.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05069723 0.02043129 0.05253582 0.02125187]\n",
      "408.10114312171936s\n"
     ]
    }
   ],
   "source": [
    "result = main(whole_click_train, answers, test_qtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('/Users/luoyonggui/Downloads/baseline1_itemcf24.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 融合\n",
    "简单融合，交叉后取前50个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfii = pd.read_csv('baseline1.csv', header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfui = pd.read_csv('baseline4.csv', header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5079, 50)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfui.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5079, 50)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfii.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32360</td>\n",
       "      <td>9748</td>\n",
       "      <td>95241</td>\n",
       "      <td>55738</td>\n",
       "      <td>52102</td>\n",
       "      <td>109854</td>\n",
       "      <td>16433</td>\n",
       "      <td>19228</td>\n",
       "      <td>22924</td>\n",
       "      <td>57418</td>\n",
       "      <td>...</td>\n",
       "      <td>54959</td>\n",
       "      <td>80769</td>\n",
       "      <td>19889</td>\n",
       "      <td>58929</td>\n",
       "      <td>61591</td>\n",
       "      <td>35217</td>\n",
       "      <td>90725</td>\n",
       "      <td>57001</td>\n",
       "      <td>71716</td>\n",
       "      <td>53825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115199</td>\n",
       "      <td>115093</td>\n",
       "      <td>113547</td>\n",
       "      <td>114017</td>\n",
       "      <td>112582</td>\n",
       "      <td>114262</td>\n",
       "      <td>110389</td>\n",
       "      <td>113345</td>\n",
       "      <td>105902</td>\n",
       "      <td>109632</td>\n",
       "      <td>...</td>\n",
       "      <td>81302</td>\n",
       "      <td>31278</td>\n",
       "      <td>54099</td>\n",
       "      <td>86488</td>\n",
       "      <td>82709</td>\n",
       "      <td>77193</td>\n",
       "      <td>4208</td>\n",
       "      <td>10612</td>\n",
       "      <td>78916</td>\n",
       "      <td>4207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10528</td>\n",
       "      <td>18355</td>\n",
       "      <td>12652</td>\n",
       "      <td>108373</td>\n",
       "      <td>59382</td>\n",
       "      <td>113564</td>\n",
       "      <td>34835</td>\n",
       "      <td>5977</td>\n",
       "      <td>22093</td>\n",
       "      <td>59057</td>\n",
       "      <td>...</td>\n",
       "      <td>13612</td>\n",
       "      <td>645</td>\n",
       "      <td>10800</td>\n",
       "      <td>79868</td>\n",
       "      <td>13660</td>\n",
       "      <td>15087</td>\n",
       "      <td>25635</td>\n",
       "      <td>17923</td>\n",
       "      <td>67879</td>\n",
       "      <td>5296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>35243</td>\n",
       "      <td>60887</td>\n",
       "      <td>40179</td>\n",
       "      <td>20953</td>\n",
       "      <td>57329</td>\n",
       "      <td>54911</td>\n",
       "      <td>37662</td>\n",
       "      <td>38278</td>\n",
       "      <td>70989</td>\n",
       "      <td>78057</td>\n",
       "      <td>...</td>\n",
       "      <td>91241</td>\n",
       "      <td>41180</td>\n",
       "      <td>91503</td>\n",
       "      <td>50541</td>\n",
       "      <td>3829</td>\n",
       "      <td>52873</td>\n",
       "      <td>14289</td>\n",
       "      <td>34677</td>\n",
       "      <td>35826</td>\n",
       "      <td>64806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10513</td>\n",
       "      <td>43263</td>\n",
       "      <td>25435</td>\n",
       "      <td>86011</td>\n",
       "      <td>47804</td>\n",
       "      <td>56104</td>\n",
       "      <td>6969</td>\n",
       "      <td>55931</td>\n",
       "      <td>49603</td>\n",
       "      <td>115206</td>\n",
       "      <td>...</td>\n",
       "      <td>25452</td>\n",
       "      <td>10874</td>\n",
       "      <td>105485</td>\n",
       "      <td>47163</td>\n",
       "      <td>108852</td>\n",
       "      <td>113508</td>\n",
       "      <td>61850</td>\n",
       "      <td>43937</td>\n",
       "      <td>25520</td>\n",
       "      <td>6830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1       2       3       4       5       6       7       8       9   \\\n",
       "0                                                                            \n",
       "1    32360    9748   95241   55738   52102  109854   16433   19228   22924   \n",
       "2   115199  115093  113547  114017  112582  114262  110389  113345  105902   \n",
       "11   10528   18355   12652  108373   59382  113564   34835    5977   22093   \n",
       "13   35243   60887   40179   20953   57329   54911   37662   38278   70989   \n",
       "22   10513   43263   25435   86011   47804   56104    6969   55931   49603   \n",
       "\n",
       "        10  ...     41     42      43     44      45      46     47     48  \\\n",
       "0           ...                                                              \n",
       "1    57418  ...  54959  80769   19889  58929   61591   35217  90725  57001   \n",
       "2   109632  ...  81302  31278   54099  86488   82709   77193   4208  10612   \n",
       "11   59057  ...  13612    645   10800  79868   13660   15087  25635  17923   \n",
       "13   78057  ...  91241  41180   91503  50541    3829   52873  14289  34677   \n",
       "22  115206  ...  25452  10874  105485  47163  108852  113508  61850  43937   \n",
       "\n",
       "       49     50  \n",
       "0                 \n",
       "1   71716  53825  \n",
       "2   78916   4207  \n",
       "11  67879   5296  \n",
       "13  35826  64806  \n",
       "22  25520   6830  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfii.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32360</td>\n",
       "      <td>52102</td>\n",
       "      <td>67026</td>\n",
       "      <td>90725</td>\n",
       "      <td>107027</td>\n",
       "      <td>8825</td>\n",
       "      <td>9748</td>\n",
       "      <td>44596</td>\n",
       "      <td>8632</td>\n",
       "      <td>27130</td>\n",
       "      <td>...</td>\n",
       "      <td>50410</td>\n",
       "      <td>38501</td>\n",
       "      <td>50636</td>\n",
       "      <td>13468</td>\n",
       "      <td>77683</td>\n",
       "      <td>17054</td>\n",
       "      <td>95241</td>\n",
       "      <td>38953</td>\n",
       "      <td>16420</td>\n",
       "      <td>39949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115199</td>\n",
       "      <td>114262</td>\n",
       "      <td>116057</td>\n",
       "      <td>113547</td>\n",
       "      <td>115093</td>\n",
       "      <td>105902</td>\n",
       "      <td>110389</td>\n",
       "      <td>113345</td>\n",
       "      <td>110740</td>\n",
       "      <td>114017</td>\n",
       "      <td>...</td>\n",
       "      <td>111447</td>\n",
       "      <td>100794</td>\n",
       "      <td>28612</td>\n",
       "      <td>115073</td>\n",
       "      <td>20439</td>\n",
       "      <td>88261</td>\n",
       "      <td>111808</td>\n",
       "      <td>109649</td>\n",
       "      <td>112542</td>\n",
       "      <td>83969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10528</td>\n",
       "      <td>26469</td>\n",
       "      <td>59382</td>\n",
       "      <td>82122</td>\n",
       "      <td>59057</td>\n",
       "      <td>18355</td>\n",
       "      <td>47250</td>\n",
       "      <td>34835</td>\n",
       "      <td>79868</td>\n",
       "      <td>74935</td>\n",
       "      <td>...</td>\n",
       "      <td>13612</td>\n",
       "      <td>108373</td>\n",
       "      <td>12652</td>\n",
       "      <td>41495</td>\n",
       "      <td>40879</td>\n",
       "      <td>116058</td>\n",
       "      <td>113564</td>\n",
       "      <td>26062</td>\n",
       "      <td>10800</td>\n",
       "      <td>37182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>40179</td>\n",
       "      <td>54911</td>\n",
       "      <td>57329</td>\n",
       "      <td>20953</td>\n",
       "      <td>35243</td>\n",
       "      <td>78057</td>\n",
       "      <td>60887</td>\n",
       "      <td>38278</td>\n",
       "      <td>35945</td>\n",
       "      <td>18663</td>\n",
       "      <td>...</td>\n",
       "      <td>83445</td>\n",
       "      <td>15579</td>\n",
       "      <td>47744</td>\n",
       "      <td>37825</td>\n",
       "      <td>45839</td>\n",
       "      <td>42026</td>\n",
       "      <td>58114</td>\n",
       "      <td>8809</td>\n",
       "      <td>98083</td>\n",
       "      <td>64578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10513</td>\n",
       "      <td>43263</td>\n",
       "      <td>55931</td>\n",
       "      <td>25435</td>\n",
       "      <td>86011</td>\n",
       "      <td>47804</td>\n",
       "      <td>56104</td>\n",
       "      <td>6969</td>\n",
       "      <td>16511</td>\n",
       "      <td>115206</td>\n",
       "      <td>...</td>\n",
       "      <td>66736</td>\n",
       "      <td>107257</td>\n",
       "      <td>8338</td>\n",
       "      <td>34078</td>\n",
       "      <td>25520</td>\n",
       "      <td>61850</td>\n",
       "      <td>20996</td>\n",
       "      <td>376</td>\n",
       "      <td>100813</td>\n",
       "      <td>65208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1       2       3       4       5       6       7       8       9   \\\n",
       "0                                                                            \n",
       "1    32360   52102   67026   90725  107027    8825    9748   44596    8632   \n",
       "2   115199  114262  116057  113547  115093  105902  110389  113345  110740   \n",
       "11   10528   26469   59382   82122   59057   18355   47250   34835   79868   \n",
       "13   40179   54911   57329   20953   35243   78057   60887   38278   35945   \n",
       "22   10513   43263   55931   25435   86011   47804   56104    6969   16511   \n",
       "\n",
       "        10  ...      41      42     43      44     45      46      47      48  \\\n",
       "0           ...                                                                 \n",
       "1    27130  ...   50410   38501  50636   13468  77683   17054   95241   38953   \n",
       "2   114017  ...  111447  100794  28612  115073  20439   88261  111808  109649   \n",
       "11   74935  ...   13612  108373  12652   41495  40879  116058  113564   26062   \n",
       "13   18663  ...   83445   15579  47744   37825  45839   42026   58114    8809   \n",
       "22  115206  ...   66736  107257   8338   34078  25520   61850   20996     376   \n",
       "\n",
       "        49     50  \n",
       "0                  \n",
       "1    16420  39949  \n",
       "2   112542  83969  \n",
       "11   10800  37182  \n",
       "13   98083  64578  \n",
       "22  100813  65208  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfui.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cbook import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfr = pd.DataFrame(columns=range(50))\n",
    "for i in dfii.index.tolist():\n",
    "#     print(i)\n",
    "    a = dfii.loc[i].tolist()\n",
    "    b = dfui.loc[i].tolist()\n",
    "    dfr.loc[i] = pd.Series(flatten(zip(a, b))).drop_duplicates().iloc[:50].tolist()\n",
    "#     print(pd.Series(flatten(zip(a, b))).drop_duplicates().iloc[:50])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32360</td>\n",
       "      <td>9748</td>\n",
       "      <td>52102</td>\n",
       "      <td>95241</td>\n",
       "      <td>67026</td>\n",
       "      <td>55738</td>\n",
       "      <td>90725</td>\n",
       "      <td>107027</td>\n",
       "      <td>109854</td>\n",
       "      <td>8825</td>\n",
       "      <td>...</td>\n",
       "      <td>73982</td>\n",
       "      <td>91290</td>\n",
       "      <td>107816</td>\n",
       "      <td>70001</td>\n",
       "      <td>36489</td>\n",
       "      <td>62185</td>\n",
       "      <td>113424</td>\n",
       "      <td>57001</td>\n",
       "      <td>58929</td>\n",
       "      <td>38885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115199</td>\n",
       "      <td>115093</td>\n",
       "      <td>114262</td>\n",
       "      <td>113547</td>\n",
       "      <td>116057</td>\n",
       "      <td>114017</td>\n",
       "      <td>112582</td>\n",
       "      <td>105902</td>\n",
       "      <td>110389</td>\n",
       "      <td>113345</td>\n",
       "      <td>...</td>\n",
       "      <td>4207</td>\n",
       "      <td>83410</td>\n",
       "      <td>86823</td>\n",
       "      <td>102858</td>\n",
       "      <td>115073</td>\n",
       "      <td>11848</td>\n",
       "      <td>82469</td>\n",
       "      <td>25627</td>\n",
       "      <td>81302</td>\n",
       "      <td>111447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10528</td>\n",
       "      <td>18355</td>\n",
       "      <td>26469</td>\n",
       "      <td>12652</td>\n",
       "      <td>59382</td>\n",
       "      <td>108373</td>\n",
       "      <td>82122</td>\n",
       "      <td>59057</td>\n",
       "      <td>113564</td>\n",
       "      <td>34835</td>\n",
       "      <td>...</td>\n",
       "      <td>13660</td>\n",
       "      <td>95346</td>\n",
       "      <td>32237</td>\n",
       "      <td>88589</td>\n",
       "      <td>39026</td>\n",
       "      <td>41221</td>\n",
       "      <td>106715</td>\n",
       "      <td>21517</td>\n",
       "      <td>31527</td>\n",
       "      <td>21941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>35243</td>\n",
       "      <td>40179</td>\n",
       "      <td>60887</td>\n",
       "      <td>54911</td>\n",
       "      <td>57329</td>\n",
       "      <td>20953</td>\n",
       "      <td>78057</td>\n",
       "      <td>37662</td>\n",
       "      <td>38278</td>\n",
       "      <td>70989</td>\n",
       "      <td>...</td>\n",
       "      <td>90181</td>\n",
       "      <td>49054</td>\n",
       "      <td>35335</td>\n",
       "      <td>100216</td>\n",
       "      <td>58114</td>\n",
       "      <td>93449</td>\n",
       "      <td>39916</td>\n",
       "      <td>36829</td>\n",
       "      <td>69620</td>\n",
       "      <td>52999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10513</td>\n",
       "      <td>43263</td>\n",
       "      <td>25435</td>\n",
       "      <td>55931</td>\n",
       "      <td>86011</td>\n",
       "      <td>47804</td>\n",
       "      <td>56104</td>\n",
       "      <td>6969</td>\n",
       "      <td>49603</td>\n",
       "      <td>16511</td>\n",
       "      <td>...</td>\n",
       "      <td>30562</td>\n",
       "      <td>58916</td>\n",
       "      <td>103330</td>\n",
       "      <td>104269</td>\n",
       "      <td>46796</td>\n",
       "      <td>116155</td>\n",
       "      <td>1438</td>\n",
       "      <td>109936</td>\n",
       "      <td>25452</td>\n",
       "      <td>66736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2       3       4       5       6       7       8   \\\n",
       "1    32360    9748   52102   95241   67026   55738   90725  107027  109854   \n",
       "2   115199  115093  114262  113547  116057  114017  112582  105902  110389   \n",
       "11   10528   18355   26469   12652   59382  108373   82122   59057  113564   \n",
       "13   35243   40179   60887   54911   57329   20953   78057   37662   38278   \n",
       "22   10513   43263   25435   55931   86011   47804   56104    6969   49603   \n",
       "\n",
       "        9   ...     40     41      42      43      44      45      46      47  \\\n",
       "1     8825  ...  73982  91290  107816   70001   36489   62185  113424   57001   \n",
       "2   113345  ...   4207  83410   86823  102858  115073   11848   82469   25627   \n",
       "11   34835  ...  13660  95346   32237   88589   39026   41221  106715   21517   \n",
       "13   70989  ...  90181  49054   35335  100216   58114   93449   39916   36829   \n",
       "22   16511  ...  30562  58916  103330  104269   46796  116155    1438  109936   \n",
       "\n",
       "       48      49  \n",
       "1   58929   38885  \n",
       "2   81302  111447  \n",
       "11  31527   21941  \n",
       "13  69620   52999  \n",
       "22  25452   66736  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5079, 50)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr.to_csv('df_merge.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 100)  # 设置显示数据的最大列数，防止出现省略号…，导致数据显示不全\n",
    "pd.set_option('expand_frame_repr', False)  # 当列太多时不自动换行\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font='Arial Unicode MS')  # 解决Seaborn中文显示问题\n",
    "import sys\n",
    "sys.path.append('/Users/luoyonggui/PycharmProjects/mayiutils_n1/mayiutils/data_prepare')\n",
    "from data_explore import DataExplore as de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.inputs import SparseFeat, VarLenSparseFeat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "from deepmatch.models import *\n",
    "from deepmatch.utils import sampledsoftmaxloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data_origin/'\n",
    "train_user_df = pd.read_csv(path+'underexpose_train/underexpose_user_feat.csv', names=['user_id','user_age_level','user_gender','user_city_level'])\n",
    "train_user_df = train_user_df.drop_duplicates('user_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_df = pd.read_csv(path+'underexpose_train/underexpose_item_feat.csv', sep=r',\\s+|,\\[|\\],\\[',names=['item_id']+list(range(256)))\n",
    "train_item_df.iloc[:, -1] = train_item_df.iloc[:, -1].str.replace(']', '').map(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_click_no_val = pd.read_pickle('data_gen/whole_click_no_val_set.pkl')\n",
    "whole_click_val = pd.read_pickle('data_gen/whole_click_val_set.pkl')\n",
    "test_qtime = pd.read_pickle('data_gen/test_qtime.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_click = pd.concat([whole_click_no_val, whole_click_val], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((480381, 3), (5079, 3))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_click.shape, whole_click_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>475302</th>\n",
       "      <td>22946</td>\n",
       "      <td>102303</td>\n",
       "      <td>0.983740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475303</th>\n",
       "      <td>22352</td>\n",
       "      <td>41533</td>\n",
       "      <td>0.983740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475304</th>\n",
       "      <td>20922</td>\n",
       "      <td>13603</td>\n",
       "      <td>0.983741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475305</th>\n",
       "      <td>34496</td>\n",
       "      <td>20710</td>\n",
       "      <td>0.983742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475306</th>\n",
       "      <td>15290</td>\n",
       "      <td>111807</td>\n",
       "      <td>0.983744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id      time\n",
       "475302    22946   102303  0.983740\n",
       "475303    22352    41533  0.983740\n",
       "475304    20922    13603  0.983741\n",
       "475305    34496    20710  0.983742\n",
       "475306    15290   111807  0.983744"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_click.iloc[-5079:-5074]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>22946</td>\n",
       "      <td>102303</td>\n",
       "      <td>0.983740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14900</th>\n",
       "      <td>22352</td>\n",
       "      <td>41533</td>\n",
       "      <td>0.983740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20690</th>\n",
       "      <td>20922</td>\n",
       "      <td>13603</td>\n",
       "      <td>0.983741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6518</th>\n",
       "      <td>34496</td>\n",
       "      <td>20710</td>\n",
       "      <td>0.983742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8907</th>\n",
       "      <td>15290</td>\n",
       "      <td>111807</td>\n",
       "      <td>0.983744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id      time\n",
       "541      22946   102303  0.983740\n",
       "14900    22352    41533  0.983740\n",
       "20690    20922    13603  0.983741\n",
       "6518     34496    20710  0.983742\n",
       "8907     15290   111807  0.983744"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_click_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(pd.merge(whole_click, train_user_df, how='left'), train_item_df, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>time</th>\n",
       "      <th>user_age_level</th>\n",
       "      <th>user_gender</th>\n",
       "      <th>user_city_level</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25129</td>\n",
       "      <td>3852</td>\n",
       "      <td>0.98374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;UNK&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.571915</td>\n",
       "      <td>-1.814055</td>\n",
       "      <td>3.279824</td>\n",
       "      <td>-2.754879</td>\n",
       "      <td>-2.226882</td>\n",
       "      <td>0.628462</td>\n",
       "      <td>-0.960409</td>\n",
       "      <td>1.276046</td>\n",
       "      <td>-3.626350</td>\n",
       "      <td>-2.434744</td>\n",
       "      <td>0.246905</td>\n",
       "      <td>-0.608179</td>\n",
       "      <td>5.237006</td>\n",
       "      <td>-0.237802</td>\n",
       "      <td>-0.199239</td>\n",
       "      <td>0.064605</td>\n",
       "      <td>-2.160981</td>\n",
       "      <td>0.971480</td>\n",
       "      <td>-2.408688</td>\n",
       "      <td>-0.884277</td>\n",
       "      <td>1.025688</td>\n",
       "      <td>-1.907494</td>\n",
       "      <td>-1.067752</td>\n",
       "      <td>-2.969791</td>\n",
       "      <td>-0.127235</td>\n",
       "      <td>0.711640</td>\n",
       "      <td>2.428432</td>\n",
       "      <td>-0.760012</td>\n",
       "      <td>0.370263</td>\n",
       "      <td>1.928983</td>\n",
       "      <td>-0.747229</td>\n",
       "      <td>-1.465866</td>\n",
       "      <td>-1.025934</td>\n",
       "      <td>-3.069760</td>\n",
       "      <td>2.918849</td>\n",
       "      <td>-2.058115</td>\n",
       "      <td>-0.805159</td>\n",
       "      <td>-2.803984</td>\n",
       "      <td>2.498572</td>\n",
       "      <td>3.038785</td>\n",
       "      <td>-1.862499</td>\n",
       "      <td>0.825423</td>\n",
       "      <td>-0.043094</td>\n",
       "      <td>3.431325</td>\n",
       "      <td>...</td>\n",
       "      <td>2.421870</td>\n",
       "      <td>0.667413</td>\n",
       "      <td>-2.090987</td>\n",
       "      <td>-1.924392</td>\n",
       "      <td>-0.108686</td>\n",
       "      <td>-2.947167</td>\n",
       "      <td>-2.780642</td>\n",
       "      <td>0.503893</td>\n",
       "      <td>1.921814</td>\n",
       "      <td>-1.270335</td>\n",
       "      <td>-0.167638</td>\n",
       "      <td>-2.014415</td>\n",
       "      <td>0.980605</td>\n",
       "      <td>-0.432822</td>\n",
       "      <td>-1.538932</td>\n",
       "      <td>-0.497246</td>\n",
       "      <td>3.262029</td>\n",
       "      <td>-1.889152</td>\n",
       "      <td>2.447979</td>\n",
       "      <td>2.023919</td>\n",
       "      <td>1.444185</td>\n",
       "      <td>-0.095509</td>\n",
       "      <td>1.549015</td>\n",
       "      <td>3.191395</td>\n",
       "      <td>-0.198340</td>\n",
       "      <td>-0.035811</td>\n",
       "      <td>-0.905403</td>\n",
       "      <td>0.748054</td>\n",
       "      <td>1.069559</td>\n",
       "      <td>0.216951</td>\n",
       "      <td>-1.438549</td>\n",
       "      <td>-2.388932</td>\n",
       "      <td>2.662791</td>\n",
       "      <td>4.099761</td>\n",
       "      <td>-0.267581</td>\n",
       "      <td>-3.321726</td>\n",
       "      <td>-0.155415</td>\n",
       "      <td>-0.525411</td>\n",
       "      <td>1.735601</td>\n",
       "      <td>-0.589674</td>\n",
       "      <td>1.453013</td>\n",
       "      <td>1.571093</td>\n",
       "      <td>-1.223752</td>\n",
       "      <td>-4.705770</td>\n",
       "      <td>-2.396826</td>\n",
       "      <td>0.079413</td>\n",
       "      <td>0.606347</td>\n",
       "      <td>1.943259</td>\n",
       "      <td>1.899397</td>\n",
       "      <td>-1.245537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20937</td>\n",
       "      <td>18599</td>\n",
       "      <td>0.98374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;UNK&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.869179</td>\n",
       "      <td>-3.099122</td>\n",
       "      <td>2.084314</td>\n",
       "      <td>-2.454703</td>\n",
       "      <td>-2.094862</td>\n",
       "      <td>-0.662508</td>\n",
       "      <td>-3.808012</td>\n",
       "      <td>1.813844</td>\n",
       "      <td>-6.038182</td>\n",
       "      <td>-0.180724</td>\n",
       "      <td>-1.103885</td>\n",
       "      <td>-1.550228</td>\n",
       "      <td>5.279609</td>\n",
       "      <td>-0.503311</td>\n",
       "      <td>3.120579</td>\n",
       "      <td>0.063907</td>\n",
       "      <td>0.029678</td>\n",
       "      <td>0.480394</td>\n",
       "      <td>-1.509667</td>\n",
       "      <td>0.377320</td>\n",
       "      <td>1.514299</td>\n",
       "      <td>-1.014878</td>\n",
       "      <td>1.615282</td>\n",
       "      <td>-5.186666</td>\n",
       "      <td>-0.365947</td>\n",
       "      <td>-0.587060</td>\n",
       "      <td>0.525926</td>\n",
       "      <td>0.869364</td>\n",
       "      <td>0.476346</td>\n",
       "      <td>1.914612</td>\n",
       "      <td>-4.939361</td>\n",
       "      <td>-3.769417</td>\n",
       "      <td>2.798874</td>\n",
       "      <td>1.344090</td>\n",
       "      <td>3.873302</td>\n",
       "      <td>-3.572078</td>\n",
       "      <td>-1.601236</td>\n",
       "      <td>-3.016315</td>\n",
       "      <td>2.205385</td>\n",
       "      <td>3.553573</td>\n",
       "      <td>-2.045253</td>\n",
       "      <td>-0.157650</td>\n",
       "      <td>2.266847</td>\n",
       "      <td>5.036000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481929</td>\n",
       "      <td>-0.373578</td>\n",
       "      <td>0.544273</td>\n",
       "      <td>0.111372</td>\n",
       "      <td>-1.719494</td>\n",
       "      <td>-1.100947</td>\n",
       "      <td>-2.666645</td>\n",
       "      <td>-3.169693</td>\n",
       "      <td>-0.838876</td>\n",
       "      <td>-0.808355</td>\n",
       "      <td>3.324217</td>\n",
       "      <td>3.368498</td>\n",
       "      <td>0.751686</td>\n",
       "      <td>-2.285471</td>\n",
       "      <td>-1.774423</td>\n",
       "      <td>-1.140164</td>\n",
       "      <td>3.305172</td>\n",
       "      <td>-4.562526</td>\n",
       "      <td>-0.137779</td>\n",
       "      <td>0.451743</td>\n",
       "      <td>2.721603</td>\n",
       "      <td>2.801044</td>\n",
       "      <td>3.219163</td>\n",
       "      <td>1.965619</td>\n",
       "      <td>0.347214</td>\n",
       "      <td>-1.566591</td>\n",
       "      <td>2.519880</td>\n",
       "      <td>1.113690</td>\n",
       "      <td>0.224589</td>\n",
       "      <td>0.269521</td>\n",
       "      <td>-1.206637</td>\n",
       "      <td>-3.876595</td>\n",
       "      <td>2.657075</td>\n",
       "      <td>0.765382</td>\n",
       "      <td>0.361337</td>\n",
       "      <td>-2.198026</td>\n",
       "      <td>0.345006</td>\n",
       "      <td>3.772189</td>\n",
       "      <td>1.842122</td>\n",
       "      <td>-0.739535</td>\n",
       "      <td>3.972774</td>\n",
       "      <td>-0.904352</td>\n",
       "      <td>3.413301</td>\n",
       "      <td>-2.395801</td>\n",
       "      <td>-0.955557</td>\n",
       "      <td>3.624955</td>\n",
       "      <td>1.230665</td>\n",
       "      <td>1.725096</td>\n",
       "      <td>-1.393228</td>\n",
       "      <td>-4.908030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22528</td>\n",
       "      <td>48057</td>\n",
       "      <td>0.98374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;UNK&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.078118</td>\n",
       "      <td>-0.378743</td>\n",
       "      <td>0.272353</td>\n",
       "      <td>-2.633809</td>\n",
       "      <td>-1.767918</td>\n",
       "      <td>0.949223</td>\n",
       "      <td>1.033278</td>\n",
       "      <td>0.314563</td>\n",
       "      <td>-4.805015</td>\n",
       "      <td>-4.950263</td>\n",
       "      <td>2.126089</td>\n",
       "      <td>1.625062</td>\n",
       "      <td>4.022007</td>\n",
       "      <td>-2.070587</td>\n",
       "      <td>0.192334</td>\n",
       "      <td>-0.201567</td>\n",
       "      <td>-1.964089</td>\n",
       "      <td>0.027071</td>\n",
       "      <td>-2.421269</td>\n",
       "      <td>-1.420727</td>\n",
       "      <td>-2.351256</td>\n",
       "      <td>-1.689599</td>\n",
       "      <td>-0.386515</td>\n",
       "      <td>-1.401658</td>\n",
       "      <td>-1.278733</td>\n",
       "      <td>0.355593</td>\n",
       "      <td>1.742722</td>\n",
       "      <td>-0.018574</td>\n",
       "      <td>-1.864175</td>\n",
       "      <td>0.927499</td>\n",
       "      <td>0.648029</td>\n",
       "      <td>-2.921231</td>\n",
       "      <td>-1.621955</td>\n",
       "      <td>0.473017</td>\n",
       "      <td>2.966018</td>\n",
       "      <td>-1.656037</td>\n",
       "      <td>0.981539</td>\n",
       "      <td>-3.516828</td>\n",
       "      <td>2.144614</td>\n",
       "      <td>2.134675</td>\n",
       "      <td>-2.026774</td>\n",
       "      <td>-0.093928</td>\n",
       "      <td>2.424305</td>\n",
       "      <td>-0.054545</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120378</td>\n",
       "      <td>-3.074750</td>\n",
       "      <td>-0.436548</td>\n",
       "      <td>1.226916</td>\n",
       "      <td>2.349869</td>\n",
       "      <td>2.804942</td>\n",
       "      <td>-0.416353</td>\n",
       "      <td>0.158974</td>\n",
       "      <td>2.758401</td>\n",
       "      <td>-1.814317</td>\n",
       "      <td>0.509540</td>\n",
       "      <td>2.357375</td>\n",
       "      <td>1.151527</td>\n",
       "      <td>-0.204829</td>\n",
       "      <td>-2.014134</td>\n",
       "      <td>-1.505196</td>\n",
       "      <td>0.490951</td>\n",
       "      <td>-0.478856</td>\n",
       "      <td>4.660158</td>\n",
       "      <td>1.820699</td>\n",
       "      <td>1.378949</td>\n",
       "      <td>-0.040161</td>\n",
       "      <td>0.497809</td>\n",
       "      <td>4.419123</td>\n",
       "      <td>0.049237</td>\n",
       "      <td>3.057560</td>\n",
       "      <td>3.268921</td>\n",
       "      <td>1.871532</td>\n",
       "      <td>1.794398</td>\n",
       "      <td>2.954940</td>\n",
       "      <td>-0.558485</td>\n",
       "      <td>0.106806</td>\n",
       "      <td>0.409342</td>\n",
       "      <td>0.381947</td>\n",
       "      <td>2.422366</td>\n",
       "      <td>0.706619</td>\n",
       "      <td>-5.211069</td>\n",
       "      <td>1.736306</td>\n",
       "      <td>3.246878</td>\n",
       "      <td>-0.308061</td>\n",
       "      <td>-1.036637</td>\n",
       "      <td>2.311875</td>\n",
       "      <td>-0.909021</td>\n",
       "      <td>-3.420048</td>\n",
       "      <td>2.800025</td>\n",
       "      <td>1.820050</td>\n",
       "      <td>0.257765</td>\n",
       "      <td>2.410485</td>\n",
       "      <td>1.843264</td>\n",
       "      <td>0.536979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12136</td>\n",
       "      <td>28195</td>\n",
       "      <td>0.98374</td>\n",
       "      <td>7.0</td>\n",
       "      <td>F</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.158704</td>\n",
       "      <td>-2.213841</td>\n",
       "      <td>3.299623</td>\n",
       "      <td>-3.816672</td>\n",
       "      <td>1.817951</td>\n",
       "      <td>-0.562453</td>\n",
       "      <td>-3.637601</td>\n",
       "      <td>-0.311089</td>\n",
       "      <td>-5.418423</td>\n",
       "      <td>-2.103671</td>\n",
       "      <td>-0.442526</td>\n",
       "      <td>-3.839618</td>\n",
       "      <td>4.380653</td>\n",
       "      <td>-3.727894</td>\n",
       "      <td>1.042084</td>\n",
       "      <td>-2.424195</td>\n",
       "      <td>3.287570</td>\n",
       "      <td>-0.606853</td>\n",
       "      <td>-1.033391</td>\n",
       "      <td>0.160277</td>\n",
       "      <td>-0.038618</td>\n",
       "      <td>-1.071147</td>\n",
       "      <td>0.780368</td>\n",
       "      <td>-0.159890</td>\n",
       "      <td>-1.548846</td>\n",
       "      <td>-2.328499</td>\n",
       "      <td>-0.205781</td>\n",
       "      <td>-3.355978</td>\n",
       "      <td>-5.415924</td>\n",
       "      <td>-1.503188</td>\n",
       "      <td>-5.305884</td>\n",
       "      <td>-1.843716</td>\n",
       "      <td>-0.245827</td>\n",
       "      <td>3.008088</td>\n",
       "      <td>3.399326</td>\n",
       "      <td>-3.194445</td>\n",
       "      <td>-3.397100</td>\n",
       "      <td>-0.312699</td>\n",
       "      <td>2.398918</td>\n",
       "      <td>4.553303</td>\n",
       "      <td>-2.690272</td>\n",
       "      <td>1.037717</td>\n",
       "      <td>1.235074</td>\n",
       "      <td>4.735878</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.572505</td>\n",
       "      <td>-1.981386</td>\n",
       "      <td>-1.908951</td>\n",
       "      <td>3.034426</td>\n",
       "      <td>-4.374824</td>\n",
       "      <td>0.489178</td>\n",
       "      <td>-3.051432</td>\n",
       "      <td>-2.832758</td>\n",
       "      <td>-2.986957</td>\n",
       "      <td>-1.558040</td>\n",
       "      <td>4.542650</td>\n",
       "      <td>-0.078699</td>\n",
       "      <td>2.846123</td>\n",
       "      <td>2.866818</td>\n",
       "      <td>-2.151008</td>\n",
       "      <td>0.465624</td>\n",
       "      <td>3.206414</td>\n",
       "      <td>-2.886617</td>\n",
       "      <td>-0.973224</td>\n",
       "      <td>4.858570</td>\n",
       "      <td>0.401090</td>\n",
       "      <td>1.429223</td>\n",
       "      <td>-1.306388</td>\n",
       "      <td>2.436626</td>\n",
       "      <td>0.375304</td>\n",
       "      <td>0.796462</td>\n",
       "      <td>-1.397474</td>\n",
       "      <td>0.019915</td>\n",
       "      <td>-4.959344</td>\n",
       "      <td>-0.943585</td>\n",
       "      <td>0.348210</td>\n",
       "      <td>-2.549550</td>\n",
       "      <td>1.375585</td>\n",
       "      <td>-3.331098</td>\n",
       "      <td>0.260209</td>\n",
       "      <td>-1.604760</td>\n",
       "      <td>4.503458</td>\n",
       "      <td>0.519022</td>\n",
       "      <td>2.495067</td>\n",
       "      <td>-2.482817</td>\n",
       "      <td>-2.561544</td>\n",
       "      <td>-0.005109</td>\n",
       "      <td>-0.008450</td>\n",
       "      <td>-0.523985</td>\n",
       "      <td>1.464158</td>\n",
       "      <td>0.655260</td>\n",
       "      <td>-0.055013</td>\n",
       "      <td>0.743994</td>\n",
       "      <td>2.011905</td>\n",
       "      <td>0.713488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23813</td>\n",
       "      <td>85991</td>\n",
       "      <td>0.98374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;UNK&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.123619</td>\n",
       "      <td>0.246924</td>\n",
       "      <td>2.151865</td>\n",
       "      <td>-3.861373</td>\n",
       "      <td>0.273938</td>\n",
       "      <td>2.500305</td>\n",
       "      <td>-0.403458</td>\n",
       "      <td>2.441836</td>\n",
       "      <td>-4.951871</td>\n",
       "      <td>-2.013716</td>\n",
       "      <td>0.665134</td>\n",
       "      <td>-1.473728</td>\n",
       "      <td>4.141944</td>\n",
       "      <td>-2.242674</td>\n",
       "      <td>0.358546</td>\n",
       "      <td>2.623345</td>\n",
       "      <td>-1.288955</td>\n",
       "      <td>-1.594516</td>\n",
       "      <td>-0.740792</td>\n",
       "      <td>-1.215392</td>\n",
       "      <td>-0.184543</td>\n",
       "      <td>-3.765427</td>\n",
       "      <td>0.018637</td>\n",
       "      <td>1.372923</td>\n",
       "      <td>-0.771917</td>\n",
       "      <td>-0.938931</td>\n",
       "      <td>2.259439</td>\n",
       "      <td>-2.209422</td>\n",
       "      <td>-0.749772</td>\n",
       "      <td>0.417961</td>\n",
       "      <td>-0.318627</td>\n",
       "      <td>-0.330905</td>\n",
       "      <td>-3.615103</td>\n",
       "      <td>-1.176750</td>\n",
       "      <td>3.807855</td>\n",
       "      <td>-3.536855</td>\n",
       "      <td>-2.091277</td>\n",
       "      <td>-0.382268</td>\n",
       "      <td>1.693937</td>\n",
       "      <td>4.152753</td>\n",
       "      <td>-3.023640</td>\n",
       "      <td>-0.282809</td>\n",
       "      <td>-0.441101</td>\n",
       "      <td>2.353881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661521</td>\n",
       "      <td>1.498854</td>\n",
       "      <td>-1.368444</td>\n",
       "      <td>-0.288594</td>\n",
       "      <td>-1.600247</td>\n",
       "      <td>-4.185519</td>\n",
       "      <td>-2.918602</td>\n",
       "      <td>-0.955657</td>\n",
       "      <td>-4.288156</td>\n",
       "      <td>1.280472</td>\n",
       "      <td>2.506398</td>\n",
       "      <td>-2.346975</td>\n",
       "      <td>2.036786</td>\n",
       "      <td>4.400709</td>\n",
       "      <td>-2.177859</td>\n",
       "      <td>-0.210130</td>\n",
       "      <td>4.194752</td>\n",
       "      <td>-2.178113</td>\n",
       "      <td>0.062957</td>\n",
       "      <td>2.581826</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>-2.102277</td>\n",
       "      <td>2.860797</td>\n",
       "      <td>3.667301</td>\n",
       "      <td>-0.106592</td>\n",
       "      <td>2.052163</td>\n",
       "      <td>0.036313</td>\n",
       "      <td>2.343337</td>\n",
       "      <td>-1.988721</td>\n",
       "      <td>-1.199962</td>\n",
       "      <td>-4.261702</td>\n",
       "      <td>-0.823224</td>\n",
       "      <td>1.403279</td>\n",
       "      <td>1.683417</td>\n",
       "      <td>0.453512</td>\n",
       "      <td>-2.599358</td>\n",
       "      <td>3.861367</td>\n",
       "      <td>-3.822283</td>\n",
       "      <td>-2.003078</td>\n",
       "      <td>-1.023006</td>\n",
       "      <td>0.105580</td>\n",
       "      <td>-0.102837</td>\n",
       "      <td>-0.943832</td>\n",
       "      <td>-0.198437</td>\n",
       "      <td>-2.339242</td>\n",
       "      <td>-0.575321</td>\n",
       "      <td>-0.752446</td>\n",
       "      <td>2.749357</td>\n",
       "      <td>1.433090</td>\n",
       "      <td>3.696346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 262 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id     time  user_age_level user_gender  user_city_level         0         1         2         3         4         5         6         7         8         9        10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43  ...       206       207       208       209       210       211       212       213       214       215       216       217       218       219       220       221       222       223       224       225       226       227       228       229       230       231       232       233       234       235       236       237       238       239       240       241       242       243       244       245       246       247       248       249       250       251       252       253       254       255\n",
       "0    25129     3852  0.98374             0.0       <UNK>              0.0  2.571915 -1.814055  3.279824 -2.754879 -2.226882  0.628462 -0.960409  1.276046 -3.626350 -2.434744  0.246905 -0.608179  5.237006 -0.237802 -0.199239  0.064605 -2.160981  0.971480 -2.408688 -0.884277  1.025688 -1.907494 -1.067752 -2.969791 -0.127235  0.711640  2.428432 -0.760012  0.370263  1.928983 -0.747229 -1.465866 -1.025934 -3.069760  2.918849 -2.058115 -0.805159 -2.803984  2.498572  3.038785 -1.862499  0.825423 -0.043094  3.431325  ...  2.421870  0.667413 -2.090987 -1.924392 -0.108686 -2.947167 -2.780642  0.503893  1.921814 -1.270335 -0.167638 -2.014415  0.980605 -0.432822 -1.538932 -0.497246  3.262029 -1.889152  2.447979  2.023919  1.444185 -0.095509  1.549015  3.191395 -0.198340 -0.035811 -0.905403  0.748054  1.069559  0.216951 -1.438549 -2.388932  2.662791  4.099761 -0.267581 -3.321726 -0.155415 -0.525411  1.735601 -0.589674  1.453013  1.571093 -1.223752 -4.705770 -2.396826  0.079413  0.606347  1.943259  1.899397 -1.245537\n",
       "1    20937    18599  0.98374             0.0       <UNK>              0.0  2.869179 -3.099122  2.084314 -2.454703 -2.094862 -0.662508 -3.808012  1.813844 -6.038182 -0.180724 -1.103885 -1.550228  5.279609 -0.503311  3.120579  0.063907  0.029678  0.480394 -1.509667  0.377320  1.514299 -1.014878  1.615282 -5.186666 -0.365947 -0.587060  0.525926  0.869364  0.476346  1.914612 -4.939361 -3.769417  2.798874  1.344090  3.873302 -3.572078 -1.601236 -3.016315  2.205385  3.553573 -2.045253 -0.157650  2.266847  5.036000  ...  0.481929 -0.373578  0.544273  0.111372 -1.719494 -1.100947 -2.666645 -3.169693 -0.838876 -0.808355  3.324217  3.368498  0.751686 -2.285471 -1.774423 -1.140164  3.305172 -4.562526 -0.137779  0.451743  2.721603  2.801044  3.219163  1.965619  0.347214 -1.566591  2.519880  1.113690  0.224589  0.269521 -1.206637 -3.876595  2.657075  0.765382  0.361337 -2.198026  0.345006  3.772189  1.842122 -0.739535  3.972774 -0.904352  3.413301 -2.395801 -0.955557  3.624955  1.230665  1.725096 -1.393228 -4.908030\n",
       "2    22528    48057  0.98374             0.0       <UNK>              0.0  4.078118 -0.378743  0.272353 -2.633809 -1.767918  0.949223  1.033278  0.314563 -4.805015 -4.950263  2.126089  1.625062  4.022007 -2.070587  0.192334 -0.201567 -1.964089  0.027071 -2.421269 -1.420727 -2.351256 -1.689599 -0.386515 -1.401658 -1.278733  0.355593  1.742722 -0.018574 -1.864175  0.927499  0.648029 -2.921231 -1.621955  0.473017  2.966018 -1.656037  0.981539 -3.516828  2.144614  2.134675 -2.026774 -0.093928  2.424305 -0.054545  ... -0.120378 -3.074750 -0.436548  1.226916  2.349869  2.804942 -0.416353  0.158974  2.758401 -1.814317  0.509540  2.357375  1.151527 -0.204829 -2.014134 -1.505196  0.490951 -0.478856  4.660158  1.820699  1.378949 -0.040161  0.497809  4.419123  0.049237  3.057560  3.268921  1.871532  1.794398  2.954940 -0.558485  0.106806  0.409342  0.381947  2.422366  0.706619 -5.211069  1.736306  3.246878 -0.308061 -1.036637  2.311875 -0.909021 -3.420048  2.800025  1.820050  0.257765  2.410485  1.843264  0.536979\n",
       "3    12136    28195  0.98374             7.0           F              4.0  4.158704 -2.213841  3.299623 -3.816672  1.817951 -0.562453 -3.637601 -0.311089 -5.418423 -2.103671 -0.442526 -3.839618  4.380653 -3.727894  1.042084 -2.424195  3.287570 -0.606853 -1.033391  0.160277 -0.038618 -1.071147  0.780368 -0.159890 -1.548846 -2.328499 -0.205781 -3.355978 -5.415924 -1.503188 -5.305884 -1.843716 -0.245827  3.008088  3.399326 -3.194445 -3.397100 -0.312699  2.398918  4.553303 -2.690272  1.037717  1.235074  4.735878  ... -1.572505 -1.981386 -1.908951  3.034426 -4.374824  0.489178 -3.051432 -2.832758 -2.986957 -1.558040  4.542650 -0.078699  2.846123  2.866818 -2.151008  0.465624  3.206414 -2.886617 -0.973224  4.858570  0.401090  1.429223 -1.306388  2.436626  0.375304  0.796462 -1.397474  0.019915 -4.959344 -0.943585  0.348210 -2.549550  1.375585 -3.331098  0.260209 -1.604760  4.503458  0.519022  2.495067 -2.482817 -2.561544 -0.005109 -0.008450 -0.523985  1.464158  0.655260 -0.055013  0.743994  2.011905  0.713488\n",
       "4    23813    85991  0.98374             0.0       <UNK>              0.0  4.123619  0.246924  2.151865 -3.861373  0.273938  2.500305 -0.403458  2.441836 -4.951871 -2.013716  0.665134 -1.473728  4.141944 -2.242674  0.358546  2.623345 -1.288955 -1.594516 -0.740792 -1.215392 -0.184543 -3.765427  0.018637  1.372923 -0.771917 -0.938931  2.259439 -2.209422 -0.749772  0.417961 -0.318627 -0.330905 -3.615103 -1.176750  3.807855 -3.536855 -2.091277 -0.382268  1.693937  4.152753 -3.023640 -0.282809 -0.441101  2.353881  ...  0.661521  1.498854 -1.368444 -0.288594 -1.600247 -4.185519 -2.918602 -0.955657 -4.288156  1.280472  2.506398 -2.346975  2.036786  4.400709 -2.177859 -0.210130  4.194752 -2.178113  0.062957  2.581826  0.013468 -2.102277  2.860797  3.667301 -0.106592  2.052163  0.036313  2.343337 -1.988721 -1.199962 -4.261702 -0.823224  1.403279  1.683417  0.453512 -2.599358  3.861367 -3.822283 -2.003078 -1.023006  0.105580 -0.102837 -0.943832 -0.198437 -2.339242 -0.575321 -0.752446  2.749357  1.433090  3.696346\n",
       "\n",
       "[5 rows x 262 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[:, 'user_age_level'] = data.loc[:, 'user_age_level'].fillna(0)\n",
    "\n",
    "data.loc[:, 'user_city_level'] = data.loc[:, 'user_city_level'].fillna(0)\n",
    "\n",
    "data.loc[:, 'user_gender'] = data.loc[:, 'user_gender'].fillna('<UNK>')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480381, 262)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建特征列，训练模型，导出embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = [\"item_id\", \n",
    "                   \"user_id\",\"user_gender\", \"user_age_level\", \"user_city_level\", ]\n",
    "\n",
    "\n",
    "# 1.Label Encoding for sparse features,and process sequence features with `gen_date_set` and `gen_model_input`\n",
    "\n",
    "features = [\"item_id\", \"user_id\",\n",
    "                    \"user_gender\", \"user_age_level\", \"user_city_level\", ]\n",
    "feature_max_idx = {}\n",
    "lbe_list = []\n",
    "for feature in features:\n",
    "#     print(feature)\n",
    "    lbe = LabelEncoder()\n",
    "#     data.loc[:, feature] = data[feature].fillna('<UNK>')\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
    "#     data.loc[data[feature].notnull(), feature] = lbe.fit_transform(data.loc[data[feature].notnull(), feature]) + 1\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "    lbe_list.append(lbe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile = data[[\"user_id\", \"user_gender\", \"user_age_level\", \"user_city_level\"]].drop_duplicates('user_id')\n",
    "\n",
    "item_profile = data[[\"item_id\"]].drop_duplicates('item_id')\n",
    "\n",
    "user_profile.set_index(\"user_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    23816.000000\n",
       "mean        20.170516\n",
       "std         19.394592\n",
       "min          2.000000\n",
       "25%          8.000000\n",
       "50%         14.000000\n",
       "75%         25.000000\n",
       "max        233.000000\n",
       "Name: item_id, dtype: float64"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_list = data.groupby(\"user_id\")['item_id'].apply(list)\n",
    "# User点击的item list的平均长度为20\n",
    "user_item_list.map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "1    [42465, 14686, 48588, 41442, 47502, 42615, 346...\n",
       "2    [39649, 4326, 15450, 39742, 59988, 56439, 3953...\n",
       "Name: item_id, dtype: object"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_list.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 25 # 75%分位点\n",
    "negsample = 1 # 产生一倍的负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def gen_data_set(data, negsample: int=0):\n",
    "    \"\"\"\n",
    "    negsample: 负样本数相对于正样本数的倍数\n",
    "    \"\"\"\n",
    "    data.sort_values(\"time\", inplace=True)\n",
    "    item_ids = data['item_id'].unique()\n",
    "\n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    test_set = []\n",
    "    pred_user_list = lbe_list[1].transform(whole_click_val.user_id)+1\n",
    "    print(len(pred_user_list))\n",
    "#     print(data.iloc[(-whole_click_val.shape[0]):].user_id.nunique())\n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "#         user_id = hist.user_id.iloc[0]\n",
    "        pos_list = hist['item_id'].tolist()\n",
    "        rating_list = [1] * len(hist['item_id'])\n",
    "\n",
    "        if negsample > 0:\n",
    "            # 产生负样本，策略：从用户没有评分的items中随机有放回采样\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))\n",
    "            neg_list = np.random.choice(candidate_set,size=len(pos_list)*negsample,replace=True)\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            if i != len(pos_list) - 1:\n",
    "                train_set.append((reviewerID, hist[::-1], pos_list[i], 1,len(hist[::-1]),rating_list[i]))\n",
    "                for negi in range(negsample):\n",
    "                    train_set.append((reviewerID, hist[::-1], neg_list[i*negsample+negi], 0,len(hist[::-1])))\n",
    "            else:\n",
    "                # 取最后一个点击过的item 作为 val set\n",
    "                val_set.append((reviewerID, hist[::-1], pos_list[i],1,len(hist[::-1]),rating_list[i]))\n",
    "        if reviewerID in pred_user_list:\n",
    "            test_set.append((reviewerID, pos_list[::-1], -1,1,len(pos_list[::-1]),1))\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(val_set)\n",
    "\n",
    "    print(len(train_set[0]),len(test_set[0]))\n",
    "\n",
    "#     return test_set\n",
    "    return train_set,val_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23816/23816 [00:07<00:00, 3099.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# pred_set = gen_data_set(data, negsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23816/23816 [07:30<00:00, 52.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set, pred_set = gen_data_set(data, negsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model_input(train_set,user_profile,seq_max_len):\n",
    "\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_seq = [line[1] for line in train_set]\n",
    "    train_iid = np.array([line[2] for line in train_set])\n",
    "    train_label = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    train_model_input = {\"user_id\": train_uid, \"item_id\": train_iid, \"hist_item_id\": train_seq_pad,\n",
    "                         \"hist_len\": train_hist_len}\n",
    "    \n",
    "#     no_feat_users = pd.Series(train_model_input['user_id']).drop_duplicates()\n",
    "#     no_feat_users = no_feat_users[~no_feat_users.isin(user_profile.index.tolist())]\n",
    "#     for i in no_feat_users:\n",
    "#         user_profile.loc[i, [\"user_gender\", \"user_age_level\", \"user_city_level\"]] = [0, 0, 0]\n",
    "    for key in [\"user_gender\", \"user_age_level\", \"user_city_level\"]:\n",
    "        \n",
    "        train_model_input[key] = user_profile.loc[train_model_input['user_id'], key].values\n",
    "\n",
    "    return train_model_input,train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)\n",
    "pred_model_input, _ = gen_model_input(pred_set, user_profile, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_model_input, _ = gen_model_input(pred_set, user_profile, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': array([  421, 14143, 11801, ...,  5040,  6868,  1884]),\n",
       " 'item_id': array([15534, 32906, 14885, ..., 52854,  8216, 42627]),\n",
       " 'hist_item_id': array([[48986, 38326,  6232, ..., 12398,  4768, 22180],\n",
       "        [21534, 56553, 53714, ...,     0,     0,     0],\n",
       "        [ 2359,  2804,  2337, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 5453, 29545, 45489, ...,  9160, 28743,     0],\n",
       "        [32971, 17707, 43940, ...,     0,     0,     0],\n",
       "        [39596, 13682, 21823, ...,     0,     0,     0]], dtype=int32),\n",
       " 'hist_len': array([30, 22, 15, ..., 24,  3,  3]),\n",
       " 'user_gender': array([1, 2, 1, ..., 1, 1, 2]),\n",
       " 'user_age_level': array([1, 5, 1, ..., 1, 1, 7]),\n",
       " 'user_city_level': array([1, 2, 1, ..., 1, 1, 4])}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5079,)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_model_input['user_id'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    432749\n",
       "0    432749\n",
       "dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_label).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.count #unique features for each sparse field and generate feature config for sequence feature\n",
    "\n",
    "embedding_dim = 32\n",
    "\n",
    "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
    "                        SparseFeat(\"user_gender\", feature_max_idx['user_gender'], 16),\n",
    "                        SparseFeat(\"user_age_level\", feature_max_idx['user_age_level'], 16),\n",
    "                        SparseFeat(\"user_city_level\", feature_max_idx['user_city_level'], 16),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_item_id', feature_max_idx['item_id'], embedding_dim,\n",
    "                                                    embedding_name=\"item_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        ]\n",
    "\n",
    "item_feature_columns = [SparseFeat('item_id', feature_max_idx['item_id'], embedding_dim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x1391ad250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x1391ad250>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x1391ad250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x1391ad250>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x132d00a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method DNN.call of <deepctr.layers.core.DNN object at 0x132d00f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <deepctr.layers.core.DNN object at 0x132d00f90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method DNN.call of <deepctr.layers.core.DNN object at 0x132d00f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DNN.call of <deepctr.layers.core.DNN object at 0x132d00f90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method EmbeddingIndex.call of <deepmatch.layers.core.EmbeddingIndex object at 0x138b984d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method EmbeddingIndex.call of <deepmatch.layers.core.EmbeddingIndex object at 0x138b984d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method EmbeddingIndex.call of <deepmatch.layers.core.EmbeddingIndex object at 0x138b984d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method EmbeddingIndex.call of <deepmatch.layers.core.EmbeddingIndex object at 0x138b984d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x12aeff510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x12aeff510>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x12aeff510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x12aeff510>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method PoolingLayer.call of <deepmatch.layers.core.PoolingLayer object at 0x1384e12d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PoolingLayer.call of <deepmatch.layers.core.PoolingLayer object at 0x1384e12d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method PoolingLayer.call of <deepmatch.layers.core.PoolingLayer object at 0x1384e12d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PoolingLayer.call of <deepmatch.layers.core.PoolingLayer object at 0x1384e12d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method SampledSoftmaxLayer.call of <deepmatch.layers.core.SampledSoftmaxLayer object at 0x13820d410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SampledSoftmaxLayer.call of <deepmatch.layers.core.SampledSoftmaxLayer object at 0x13820d410>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method SampledSoftmaxLayer.call of <deepmatch.layers.core.SampledSoftmaxLayer object at 0x13820d410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SampledSoftmaxLayer.call of <deepmatch.layers.core.SampledSoftmaxLayer object at 0x13820d410>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# 3.Define Model and train\n",
    "\n",
    "K.set_learning_phase(True)\n",
    "\n",
    "model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=256, user_dnn_hidden_units=(128,64, embedding_dim))\n",
    "# model = MIND(user_feature_columns,item_feature_columns,dynamic_k=True,p=1,k_max=2,num_sampled=5,user_dnn_hidden_units=(64,16),init_std=0.001)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)  # \"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 848188 samples, validate on 17310 samples\n",
      "Epoch 1/200\n",
      "848188/848188 [==============================] - 104s 122us/sample - loss: 3.4546 - val_loss: 5.5275\n",
      "Epoch 2/200\n",
      "848188/848188 [==============================] - 78s 92us/sample - loss: 3.4459 - val_loss: 5.5171\n",
      "Epoch 3/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.4442 - val_loss: 5.4468\n",
      "Epoch 4/200\n",
      "848188/848188 [==============================] - 78s 92us/sample - loss: 3.4293 - val_loss: 5.5343\n",
      "Epoch 5/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.4341 - val_loss: 5.6399\n",
      "Epoch 6/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.4279 - val_loss: 5.5931\n",
      "Epoch 7/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.4255 - val_loss: 5.5747\n",
      "Epoch 8/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.4193 - val_loss: 5.6032\n",
      "Epoch 9/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.4106 - val_loss: 5.6157\n",
      "Epoch 10/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.4177 - val_loss: 5.6198\n",
      "Epoch 11/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.4170 - val_loss: 5.5984\n",
      "Epoch 12/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.4037 - val_loss: 5.6489\n",
      "Epoch 13/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.4027 - val_loss: 5.7247\n",
      "Epoch 14/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.4097 - val_loss: 5.6015\n",
      "Epoch 15/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.4008 - val_loss: 5.7694\n",
      "Epoch 16/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.4080 - val_loss: 5.7249\n",
      "Epoch 17/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.4021 - val_loss: 5.7528\n",
      "Epoch 18/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.4005 - val_loss: 5.7078\n",
      "Epoch 19/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3971 - val_loss: 5.7917\n",
      "Epoch 20/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3921 - val_loss: 5.8029\n",
      "Epoch 21/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3893 - val_loss: 5.8315\n",
      "Epoch 22/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.3939 - val_loss: 5.7607\n",
      "Epoch 23/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.3860 - val_loss: 5.7792\n",
      "Epoch 24/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3874 - val_loss: 5.7658\n",
      "Epoch 25/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.3850 - val_loss: 5.8525\n",
      "Epoch 26/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3895 - val_loss: 5.7456\n",
      "Epoch 27/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.3823 - val_loss: 5.8543\n",
      "Epoch 28/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3743 - val_loss: 5.8020\n",
      "Epoch 29/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3836 - val_loss: 5.9165\n",
      "Epoch 30/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3835 - val_loss: 5.8425\n",
      "Epoch 31/200\n",
      "848188/848188 [==============================] - 78s 92us/sample - loss: 3.3807 - val_loss: 5.8540\n",
      "Epoch 32/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3803 - val_loss: 5.8818\n",
      "Epoch 33/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3787 - val_loss: 5.6799\n",
      "Epoch 34/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.3741 - val_loss: 5.9225\n",
      "Epoch 35/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3714 - val_loss: 5.8180\n",
      "Epoch 36/200\n",
      "848188/848188 [==============================] - 79s 94us/sample - loss: 3.3706 - val_loss: 5.8773\n",
      "Epoch 37/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3810 - val_loss: 5.9486\n",
      "Epoch 38/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3712 - val_loss: 6.0156\n",
      "Epoch 39/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3699 - val_loss: 5.8950\n",
      "Epoch 40/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3746 - val_loss: 5.9451\n",
      "Epoch 41/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3699 - val_loss: 5.9631\n",
      "Epoch 42/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.3721 - val_loss: 5.9662\n",
      "Epoch 43/200\n",
      "848188/848188 [==============================] - 79s 94us/sample - loss: 3.3724 - val_loss: 5.9295\n",
      "Epoch 44/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3663 - val_loss: 6.0370\n",
      "Epoch 45/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3704 - val_loss: 5.9990\n",
      "Epoch 46/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3588 - val_loss: 6.0145\n",
      "Epoch 47/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3696 - val_loss: 6.0224\n",
      "Epoch 48/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3704 - val_loss: 6.0148\n",
      "Epoch 49/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3618 - val_loss: 5.9926\n",
      "Epoch 50/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3581 - val_loss: 6.0824\n",
      "Epoch 51/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3641 - val_loss: 6.0118\n",
      "Epoch 52/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3628 - val_loss: 6.0441\n",
      "Epoch 53/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3622 - val_loss: 6.0971\n",
      "Epoch 54/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3616 - val_loss: 6.0891\n",
      "Epoch 55/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.3608 - val_loss: 6.0816\n",
      "Epoch 56/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3600 - val_loss: 6.0837\n",
      "Epoch 57/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3612 - val_loss: 6.1485\n",
      "Epoch 58/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3639 - val_loss: 6.0087\n",
      "Epoch 59/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3620 - val_loss: 6.0383\n",
      "Epoch 60/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3611 - val_loss: 6.1035\n",
      "Epoch 61/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3615 - val_loss: 6.0999\n",
      "Epoch 62/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3606 - val_loss: 6.1070\n",
      "Epoch 63/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.3561 - val_loss: 6.0934\n",
      "Epoch 64/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3577 - val_loss: 6.0917\n",
      "Epoch 65/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3557 - val_loss: 6.2373\n",
      "Epoch 66/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3577 - val_loss: 6.1439\n",
      "Epoch 67/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3582 - val_loss: 6.1766\n",
      "Epoch 68/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.3552 - val_loss: 6.1799\n",
      "Epoch 69/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3540 - val_loss: 6.2415\n",
      "Epoch 70/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.3609 - val_loss: 6.1394\n",
      "Epoch 71/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3541 - val_loss: 6.1850\n",
      "Epoch 72/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.3570 - val_loss: 6.1664\n",
      "Epoch 73/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3519 - val_loss: 6.2600\n",
      "Epoch 74/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3596 - val_loss: 6.2313\n",
      "Epoch 75/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3550 - val_loss: 6.2033\n",
      "Epoch 76/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.3540 - val_loss: 6.3007\n",
      "Epoch 77/200\n",
      "848188/848188 [==============================] - 78s 92us/sample - loss: 3.3548 - val_loss: 6.2807\n",
      "Epoch 78/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3522 - val_loss: 6.2314\n",
      "Epoch 79/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3610 - val_loss: 6.2023\n",
      "Epoch 80/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3565 - val_loss: 6.2200\n",
      "Epoch 81/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3538 - val_loss: 6.2004\n",
      "Epoch 82/200\n",
      "848188/848188 [==============================] - 79s 93us/sample - loss: 3.3575 - val_loss: 6.3126\n",
      "Epoch 83/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3570 - val_loss: 6.3497\n",
      "Epoch 84/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3544 - val_loss: 6.2769\n",
      "Epoch 85/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3535 - val_loss: 6.2915\n",
      "Epoch 86/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.3459 - val_loss: 6.2407\n",
      "Epoch 87/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.3565 - val_loss: 6.2870\n",
      "Epoch 88/200\n",
      "848188/848188 [==============================] - 82s 96us/sample - loss: 3.3517 - val_loss: 6.3390\n",
      "Epoch 89/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3562 - val_loss: 6.2823\n",
      "Epoch 90/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3512 - val_loss: 6.3333\n",
      "Epoch 91/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3479 - val_loss: 6.3405\n",
      "Epoch 92/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3568 - val_loss: 6.2272\n",
      "Epoch 93/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.3508 - val_loss: 6.3250\n",
      "Epoch 94/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3541 - val_loss: 6.2983\n",
      "Epoch 95/200\n",
      "848188/848188 [==============================] - 82s 96us/sample - loss: 3.3492 - val_loss: 6.3860\n",
      "Epoch 96/200\n",
      "848188/848188 [==============================] - 83s 97us/sample - loss: 3.3509 - val_loss: 6.3473\n",
      "Epoch 97/200\n",
      "848188/848188 [==============================] - 83s 97us/sample - loss: 3.3559 - val_loss: 6.4011\n",
      "Epoch 98/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3554 - val_loss: 6.3813\n",
      "Epoch 99/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3460 - val_loss: 6.3193\n",
      "Epoch 100/200\n",
      "848188/848188 [==============================] - 82s 96us/sample - loss: 3.3456 - val_loss: 6.3633\n",
      "Epoch 101/200\n",
      "848188/848188 [==============================] - 83s 97us/sample - loss: 3.3494 - val_loss: 6.3285\n",
      "Epoch 102/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3461 - val_loss: 6.3716\n",
      "Epoch 103/200\n",
      "848188/848188 [==============================] - 84s 99us/sample - loss: 3.3478 - val_loss: 6.3703\n",
      "Epoch 104/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3491 - val_loss: 6.4163\n",
      "Epoch 105/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3519 - val_loss: 6.3942\n",
      "Epoch 106/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.3523 - val_loss: 6.3793\n",
      "Epoch 107/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3474 - val_loss: 6.4823\n",
      "Epoch 108/200\n",
      "848188/848188 [==============================] - 82s 96us/sample - loss: 3.3435 - val_loss: 6.4062\n",
      "Epoch 109/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.3476 - val_loss: 6.4378\n",
      "Epoch 110/200\n",
      "848188/848188 [==============================] - 82s 96us/sample - loss: 3.3503 - val_loss: 6.4120\n",
      "Epoch 111/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3481 - val_loss: 6.3403\n",
      "Epoch 112/200\n",
      "848188/848188 [==============================] - 82s 96us/sample - loss: 3.3482 - val_loss: 6.4806\n",
      "Epoch 113/200\n",
      "848188/848188 [==============================] - 82s 96us/sample - loss: 3.3444 - val_loss: 6.4874\n",
      "Epoch 114/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.3461 - val_loss: 6.4856\n",
      "Epoch 115/200\n",
      "848188/848188 [==============================] - 81s 96us/sample - loss: 3.3487 - val_loss: 6.4977\n",
      "Epoch 116/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3433 - val_loss: 6.4761\n",
      "Epoch 117/200\n",
      "848188/848188 [==============================] - 82s 96us/sample - loss: 3.3482 - val_loss: 6.5166\n",
      "Epoch 118/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3426 - val_loss: 6.4554\n",
      "Epoch 119/200\n",
      "848188/848188 [==============================] - 83s 97us/sample - loss: 3.3433 - val_loss: 6.4543\n",
      "Epoch 120/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3466 - val_loss: 6.4358\n",
      "Epoch 121/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3436 - val_loss: 6.4368\n",
      "Epoch 122/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3491 - val_loss: 6.4387\n",
      "Epoch 123/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3459 - val_loss: 6.4950\n",
      "Epoch 124/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3466 - val_loss: 6.4358\n",
      "Epoch 125/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3429 - val_loss: 6.4586\n",
      "Epoch 126/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3484 - val_loss: 6.5984\n",
      "Epoch 127/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3431 - val_loss: 6.4218\n",
      "Epoch 128/200\n",
      "848188/848188 [==============================] - 83s 97us/sample - loss: 3.3467 - val_loss: 6.4820\n",
      "Epoch 129/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.3403 - val_loss: 6.4497\n",
      "Epoch 130/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3443 - val_loss: 6.4987\n",
      "Epoch 131/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.3468 - val_loss: 6.5325\n",
      "Epoch 132/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3422 - val_loss: 6.5270\n",
      "Epoch 133/200\n",
      "848188/848188 [==============================] - 82s 96us/sample - loss: 3.3394 - val_loss: 6.5842\n",
      "Epoch 134/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3444 - val_loss: 6.5376\n",
      "Epoch 135/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3418 - val_loss: 6.5604\n",
      "Epoch 136/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3422 - val_loss: 6.5029\n",
      "Epoch 137/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3403 - val_loss: 6.5084\n",
      "Epoch 138/200\n",
      "848188/848188 [==============================] - 81s 95us/sample - loss: 3.3428 - val_loss: 6.6004\n",
      "Epoch 139/200\n",
      "848188/848188 [==============================] - 80s 94us/sample - loss: 3.3378 - val_loss: 6.6266\n",
      "Epoch 140/200\n",
      "848188/848188 [==============================] - 80s 95us/sample - loss: 3.3477 - val_loss: 6.4885\n",
      "Epoch 141/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3433 - val_loss: 6.5923\n",
      "Epoch 142/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3384 - val_loss: 6.5901\n",
      "Epoch 143/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3399 - val_loss: 6.5691\n",
      "Epoch 144/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3422 - val_loss: 6.5162\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3391 - val_loss: 6.4637\n",
      "Epoch 146/200\n",
      "848188/848188 [==============================] - 83s 97us/sample - loss: 3.3422 - val_loss: 6.6553\n",
      "Epoch 147/200\n",
      "848188/848188 [==============================] - 83s 97us/sample - loss: 3.3400 - val_loss: 6.4908\n",
      "Epoch 148/200\n",
      "848188/848188 [==============================] - 84s 99us/sample - loss: 3.3381 - val_loss: 6.5810\n",
      "Epoch 149/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3353 - val_loss: 6.5716\n",
      "Epoch 150/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3352 - val_loss: 6.6701\n",
      "Epoch 151/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3466 - val_loss: 6.6318\n",
      "Epoch 152/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3409 - val_loss: 6.6206\n",
      "Epoch 153/200\n",
      "848188/848188 [==============================] - 83s 97us/sample - loss: 3.3440 - val_loss: 6.6396\n",
      "Epoch 154/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3378 - val_loss: 6.6009\n",
      "Epoch 155/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3375 - val_loss: 6.5591\n",
      "Epoch 156/200\n",
      "848188/848188 [==============================] - 84s 99us/sample - loss: 3.3392 - val_loss: 6.6594\n",
      "Epoch 157/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3405 - val_loss: 6.6056\n",
      "Epoch 158/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3329 - val_loss: 6.5864\n",
      "Epoch 159/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3345 - val_loss: 6.6757\n",
      "Epoch 160/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3372 - val_loss: 6.6034\n",
      "Epoch 161/200\n",
      "848188/848188 [==============================] - 83s 97us/sample - loss: 3.3337 - val_loss: 6.6479\n",
      "Epoch 162/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3369 - val_loss: 6.7114\n",
      "Epoch 163/200\n",
      "848188/848188 [==============================] - 84s 99us/sample - loss: 3.3411 - val_loss: 6.6307\n",
      "Epoch 164/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3386 - val_loss: 6.6178\n",
      "Epoch 165/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3297 - val_loss: 6.7994\n",
      "Epoch 166/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3368 - val_loss: 6.6584\n",
      "Epoch 167/200\n",
      "848188/848188 [==============================] - 82s 97us/sample - loss: 3.3392 - val_loss: 6.6669\n",
      "Epoch 168/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3355 - val_loss: 6.6855\n",
      "Epoch 169/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3289 - val_loss: 6.7602\n",
      "Epoch 170/200\n",
      "848188/848188 [==============================] - 83s 97us/sample - loss: 3.3363 - val_loss: 6.6566\n",
      "Epoch 171/200\n",
      "848188/848188 [==============================] - 84s 99us/sample - loss: 3.3334 - val_loss: 6.7047\n",
      "Epoch 172/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3359 - val_loss: 6.6882\n",
      "Epoch 173/200\n",
      "848188/848188 [==============================] - 84s 99us/sample - loss: 3.3311 - val_loss: 6.7097\n",
      "Epoch 174/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3339 - val_loss: 6.7582\n",
      "Epoch 175/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3317 - val_loss: 6.8196\n",
      "Epoch 176/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3368 - val_loss: 6.6709\n",
      "Epoch 177/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3341 - val_loss: 6.7721\n",
      "Epoch 178/200\n",
      "848188/848188 [==============================] - 84s 99us/sample - loss: 3.3347 - val_loss: 6.7370\n",
      "Epoch 179/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3332 - val_loss: 6.7190\n",
      "Epoch 180/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3347 - val_loss: 6.7482\n",
      "Epoch 181/200\n",
      "848188/848188 [==============================] - 83s 97us/sample - loss: 3.3293 - val_loss: 6.7573\n",
      "Epoch 182/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3284 - val_loss: 6.7339\n",
      "Epoch 183/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3317 - val_loss: 6.7906\n",
      "Epoch 184/200\n",
      "848188/848188 [==============================] - 84s 99us/sample - loss: 3.3331 - val_loss: 6.7139\n",
      "Epoch 185/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3316 - val_loss: 6.6464\n",
      "Epoch 186/200\n",
      "848188/848188 [==============================] - 85s 100us/sample - loss: 3.3377 - val_loss: 6.7125\n",
      "Epoch 187/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3331 - val_loss: 6.7304\n",
      "Epoch 188/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3311 - val_loss: 6.6998\n",
      "Epoch 189/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3240 - val_loss: 6.7038\n",
      "Epoch 190/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3277 - val_loss: 6.7062\n",
      "Epoch 191/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3230 - val_loss: 6.7827\n",
      "Epoch 192/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3241 - val_loss: 6.8277\n",
      "Epoch 193/200\n",
      "848188/848188 [==============================] - 84s 99us/sample - loss: 3.3318 - val_loss: 6.7601\n",
      "Epoch 194/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3234 - val_loss: 6.8355\n",
      "Epoch 195/200\n",
      "848188/848188 [==============================] - 84s 99us/sample - loss: 3.3297 - val_loss: 6.7512\n",
      "Epoch 196/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3362 - val_loss: 6.7534\n",
      "Epoch 197/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3236 - val_loss: 6.7553\n",
      "Epoch 198/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3221 - val_loss: 6.8077\n",
      "Epoch 199/200\n",
      "848188/848188 [==============================] - 83s 98us/sample - loss: 3.3250 - val_loss: 6.6628\n",
      "Epoch 200/200\n",
      "848188/848188 [==============================] - 84s 99us/sample - loss: 3.3241 - val_loss: 6.9085\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_model_input, train_label,  # train_label,\n",
    "                    batch_size=512, epochs=200, verbose=1, validation_split=0.02, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.engine.training.Model"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23816, 32)\n",
      "(61905, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23816it [00:05, 4102.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall 0.016753443063486733\n",
      "hit rate 0.016753443063486733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Generate user features for testing and full item features for retrieval\n",
    "test_user_model_input = test_model_input\n",
    "all_item_model_input = {\"item_id\": item_profile['item_id'].values,}\n",
    "\n",
    "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "\n",
    "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
    "# user_embs = user_embs[:, i, :]  i in [0,k_max) if MIND\n",
    "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "\n",
    "print(user_embs.shape)\n",
    "print(item_embs.shape)\n",
    "\n",
    "\n",
    "\n",
    "test_true_label = {line[0]:[line[2]] for line in test_set}\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from deepmatch.utils import recall_N\n",
    "\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "# faiss.normalize_L2(item_embs)\n",
    "index.add(item_embs)\n",
    "\n",
    "# faiss.normalize_L2(user_embs)\n",
    "D, I = index.search(user_embs, 50)\n",
    "\n",
    "I.shape, D.shape\n",
    "\n",
    "D\n",
    "\n",
    "s = []\n",
    "hit = 0\n",
    "for i, uid in tqdm(enumerate(test_user_model_input['user_id'])):\n",
    "#     try:\n",
    "    if True:\n",
    "        # 获取预测的item_id\n",
    "        pred = [item_profile['item_id'].values[x] for x in I[i]]\n",
    "#         print(pred)\n",
    "#         break\n",
    "        filter_item = None\n",
    "        recall_score = recall_N(test_true_label[uid], pred, N=50)\n",
    "        s.append(recall_score)\n",
    "        if test_true_label[uid] in pred:\n",
    "            hit += 1\n",
    "#     except:\n",
    "#         print(i)\n",
    "print(\"\")\n",
    "print(\"recall\", np.mean(s))\n",
    "print(\"hit rate\", hit / len(test_user_model_input['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5079, 32)\n",
      "(61905, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5079it [01:25, 59.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# 4. Generate user features for testing and full item features for retrieval\n",
    "# test_user_model_input = test_model_input\n",
    "test_user_model_input = pred_model_input\n",
    "all_item_model_input = {\"item_id\": item_profile['item_id'].values,}\n",
    "\n",
    "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "\n",
    "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
    "# user_embs = user_embs[:, i, :]  i in [0,k_max) if MIND\n",
    "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "\n",
    "print(user_embs.shape)\n",
    "print(item_embs.shape)\n",
    "\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "# faiss.normalize_L2(item_embs)\n",
    "index.add(item_embs)\n",
    "\n",
    "# faiss.normalize_L2(user_embs)\n",
    "# D 距离矩阵， I 索引矩阵\n",
    "D, I = index.search(user_embs, 500)\n",
    "\n",
    "I.shape, D.shape\n",
    "\n",
    "\n",
    "\n",
    "dfr = pd.DataFrame(pd.DataFrame(columns=[f'item_{i}' for i in range(50)]))\n",
    "\n",
    "\n",
    "s = []\n",
    "hit = 0\n",
    "\n",
    "# lbe_list[1].inverse_transform([i])\n",
    "u1 = test_user_model_input['user_id']\n",
    "for i, uid in tqdm(enumerate(lbe_list[1].inverse_transform(test_user_model_input['user_id']-1))):\n",
    "#     try:\n",
    "    if True:\n",
    "        \n",
    "        \n",
    "        # 获取预测的item_id\n",
    "        pred = [item_profile['item_id'].values[x] for x in I[i]]\n",
    "        dfr.loc[uid] = lbe_list[0].inverse_transform([ii-1 for ii in pred if ii not in user_item_list.loc[u1[i]]])[:50]\n",
    "        \n",
    "# print(dfr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr.to_csv('/Users/luoyonggui/Downloads/baseline1_itemcf6.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_0</th>\n",
       "      <th>item_1</th>\n",
       "      <th>item_2</th>\n",
       "      <th>item_3</th>\n",
       "      <th>item_4</th>\n",
       "      <th>item_5</th>\n",
       "      <th>item_6</th>\n",
       "      <th>item_7</th>\n",
       "      <th>item_8</th>\n",
       "      <th>item_9</th>\n",
       "      <th>item_10</th>\n",
       "      <th>item_11</th>\n",
       "      <th>item_12</th>\n",
       "      <th>item_13</th>\n",
       "      <th>item_14</th>\n",
       "      <th>item_15</th>\n",
       "      <th>item_16</th>\n",
       "      <th>item_17</th>\n",
       "      <th>item_18</th>\n",
       "      <th>item_19</th>\n",
       "      <th>item_20</th>\n",
       "      <th>item_21</th>\n",
       "      <th>item_22</th>\n",
       "      <th>item_23</th>\n",
       "      <th>item_24</th>\n",
       "      <th>item_25</th>\n",
       "      <th>item_26</th>\n",
       "      <th>item_27</th>\n",
       "      <th>item_28</th>\n",
       "      <th>item_29</th>\n",
       "      <th>item_30</th>\n",
       "      <th>item_31</th>\n",
       "      <th>item_32</th>\n",
       "      <th>item_33</th>\n",
       "      <th>item_34</th>\n",
       "      <th>item_35</th>\n",
       "      <th>item_36</th>\n",
       "      <th>item_37</th>\n",
       "      <th>item_38</th>\n",
       "      <th>item_39</th>\n",
       "      <th>item_40</th>\n",
       "      <th>item_41</th>\n",
       "      <th>item_42</th>\n",
       "      <th>item_43</th>\n",
       "      <th>item_44</th>\n",
       "      <th>item_45</th>\n",
       "      <th>item_46</th>\n",
       "      <th>item_47</th>\n",
       "      <th>item_48</th>\n",
       "      <th>item_49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108007</td>\n",
       "      <td>102958</td>\n",
       "      <td>84267</td>\n",
       "      <td>94926</td>\n",
       "      <td>98806</td>\n",
       "      <td>108288</td>\n",
       "      <td>93960</td>\n",
       "      <td>105496</td>\n",
       "      <td>30956</td>\n",
       "      <td>76595</td>\n",
       "      <td>105886</td>\n",
       "      <td>64109</td>\n",
       "      <td>109322</td>\n",
       "      <td>104436</td>\n",
       "      <td>74204</td>\n",
       "      <td>63609</td>\n",
       "      <td>108151</td>\n",
       "      <td>109416</td>\n",
       "      <td>109735</td>\n",
       "      <td>75446</td>\n",
       "      <td>102650</td>\n",
       "      <td>99058</td>\n",
       "      <td>83204</td>\n",
       "      <td>80776</td>\n",
       "      <td>61628</td>\n",
       "      <td>114125</td>\n",
       "      <td>112806</td>\n",
       "      <td>87570</td>\n",
       "      <td>105116</td>\n",
       "      <td>94961</td>\n",
       "      <td>111560</td>\n",
       "      <td>88947</td>\n",
       "      <td>95690</td>\n",
       "      <td>112555</td>\n",
       "      <td>113094</td>\n",
       "      <td>79754</td>\n",
       "      <td>105590</td>\n",
       "      <td>99470</td>\n",
       "      <td>47192</td>\n",
       "      <td>93538</td>\n",
       "      <td>90602</td>\n",
       "      <td>80555</td>\n",
       "      <td>113708</td>\n",
       "      <td>93116</td>\n",
       "      <td>72782</td>\n",
       "      <td>102274</td>\n",
       "      <td>96063</td>\n",
       "      <td>95743</td>\n",
       "      <td>66688</td>\n",
       "      <td>96481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102958</td>\n",
       "      <td>94926</td>\n",
       "      <td>108007</td>\n",
       "      <td>98806</td>\n",
       "      <td>93960</td>\n",
       "      <td>109322</td>\n",
       "      <td>76595</td>\n",
       "      <td>94961</td>\n",
       "      <td>105496</td>\n",
       "      <td>109416</td>\n",
       "      <td>104436</td>\n",
       "      <td>96063</td>\n",
       "      <td>74204</td>\n",
       "      <td>105116</td>\n",
       "      <td>113115</td>\n",
       "      <td>84267</td>\n",
       "      <td>30956</td>\n",
       "      <td>111318</td>\n",
       "      <td>113361</td>\n",
       "      <td>114125</td>\n",
       "      <td>113708</td>\n",
       "      <td>95965</td>\n",
       "      <td>99058</td>\n",
       "      <td>63609</td>\n",
       "      <td>111560</td>\n",
       "      <td>111751</td>\n",
       "      <td>93538</td>\n",
       "      <td>88947</td>\n",
       "      <td>112959</td>\n",
       "      <td>112806</td>\n",
       "      <td>73835</td>\n",
       "      <td>104747</td>\n",
       "      <td>105886</td>\n",
       "      <td>113467</td>\n",
       "      <td>75326</td>\n",
       "      <td>112555</td>\n",
       "      <td>108151</td>\n",
       "      <td>80555</td>\n",
       "      <td>55823</td>\n",
       "      <td>81311</td>\n",
       "      <td>108010</td>\n",
       "      <td>105590</td>\n",
       "      <td>110033</td>\n",
       "      <td>94632</td>\n",
       "      <td>104880</td>\n",
       "      <td>56287</td>\n",
       "      <td>104900</td>\n",
       "      <td>75446</td>\n",
       "      <td>102650</td>\n",
       "      <td>90602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>112351</td>\n",
       "      <td>88002</td>\n",
       "      <td>102863</td>\n",
       "      <td>98129</td>\n",
       "      <td>82194</td>\n",
       "      <td>110355</td>\n",
       "      <td>69793</td>\n",
       "      <td>53751</td>\n",
       "      <td>103411</td>\n",
       "      <td>113976</td>\n",
       "      <td>108948</td>\n",
       "      <td>25095</td>\n",
       "      <td>115250</td>\n",
       "      <td>116130</td>\n",
       "      <td>78213</td>\n",
       "      <td>106023</td>\n",
       "      <td>82044</td>\n",
       "      <td>39704</td>\n",
       "      <td>109136</td>\n",
       "      <td>53888</td>\n",
       "      <td>106286</td>\n",
       "      <td>102399</td>\n",
       "      <td>111215</td>\n",
       "      <td>48349</td>\n",
       "      <td>89888</td>\n",
       "      <td>104693</td>\n",
       "      <td>82138</td>\n",
       "      <td>52766</td>\n",
       "      <td>69911</td>\n",
       "      <td>105224</td>\n",
       "      <td>100821</td>\n",
       "      <td>88528</td>\n",
       "      <td>93379</td>\n",
       "      <td>96137</td>\n",
       "      <td>48051</td>\n",
       "      <td>19758</td>\n",
       "      <td>92218</td>\n",
       "      <td>110903</td>\n",
       "      <td>32990</td>\n",
       "      <td>28014</td>\n",
       "      <td>114125</td>\n",
       "      <td>101938</td>\n",
       "      <td>116778</td>\n",
       "      <td>94797</td>\n",
       "      <td>41138</td>\n",
       "      <td>80976</td>\n",
       "      <td>85899</td>\n",
       "      <td>101321</td>\n",
       "      <td>116697</td>\n",
       "      <td>108782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>108324</td>\n",
       "      <td>101669</td>\n",
       "      <td>102924</td>\n",
       "      <td>84166</td>\n",
       "      <td>103279</td>\n",
       "      <td>97032</td>\n",
       "      <td>111751</td>\n",
       "      <td>105886</td>\n",
       "      <td>108938</td>\n",
       "      <td>103630</td>\n",
       "      <td>81764</td>\n",
       "      <td>105423</td>\n",
       "      <td>45157</td>\n",
       "      <td>104693</td>\n",
       "      <td>84267</td>\n",
       "      <td>111599</td>\n",
       "      <td>74906</td>\n",
       "      <td>108567</td>\n",
       "      <td>78086</td>\n",
       "      <td>114495</td>\n",
       "      <td>57964</td>\n",
       "      <td>62216</td>\n",
       "      <td>88242</td>\n",
       "      <td>113655</td>\n",
       "      <td>54437</td>\n",
       "      <td>82515</td>\n",
       "      <td>87541</td>\n",
       "      <td>83457</td>\n",
       "      <td>87061</td>\n",
       "      <td>103142</td>\n",
       "      <td>115059</td>\n",
       "      <td>85254</td>\n",
       "      <td>100215</td>\n",
       "      <td>114085</td>\n",
       "      <td>69793</td>\n",
       "      <td>110077</td>\n",
       "      <td>91715</td>\n",
       "      <td>41170</td>\n",
       "      <td>87710</td>\n",
       "      <td>66873</td>\n",
       "      <td>72203</td>\n",
       "      <td>90813</td>\n",
       "      <td>76215</td>\n",
       "      <td>82904</td>\n",
       "      <td>73584</td>\n",
       "      <td>99030</td>\n",
       "      <td>82737</td>\n",
       "      <td>102129</td>\n",
       "      <td>62151</td>\n",
       "      <td>88528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>108324</td>\n",
       "      <td>78086</td>\n",
       "      <td>97032</td>\n",
       "      <td>57964</td>\n",
       "      <td>103279</td>\n",
       "      <td>84166</td>\n",
       "      <td>101669</td>\n",
       "      <td>96910</td>\n",
       "      <td>88242</td>\n",
       "      <td>103630</td>\n",
       "      <td>102924</td>\n",
       "      <td>111599</td>\n",
       "      <td>100215</td>\n",
       "      <td>81764</td>\n",
       "      <td>105886</td>\n",
       "      <td>62216</td>\n",
       "      <td>111751</td>\n",
       "      <td>105423</td>\n",
       "      <td>113567</td>\n",
       "      <td>84267</td>\n",
       "      <td>82737</td>\n",
       "      <td>104693</td>\n",
       "      <td>108567</td>\n",
       "      <td>82138</td>\n",
       "      <td>73584</td>\n",
       "      <td>54437</td>\n",
       "      <td>103142</td>\n",
       "      <td>85254</td>\n",
       "      <td>74906</td>\n",
       "      <td>87061</td>\n",
       "      <td>90217</td>\n",
       "      <td>72203</td>\n",
       "      <td>79932</td>\n",
       "      <td>105411</td>\n",
       "      <td>109136</td>\n",
       "      <td>82850</td>\n",
       "      <td>45157</td>\n",
       "      <td>85564</td>\n",
       "      <td>82419</td>\n",
       "      <td>109182</td>\n",
       "      <td>108938</td>\n",
       "      <td>91715</td>\n",
       "      <td>44185</td>\n",
       "      <td>113655</td>\n",
       "      <td>82515</td>\n",
       "      <td>78229</td>\n",
       "      <td>82109</td>\n",
       "      <td>52268</td>\n",
       "      <td>62151</td>\n",
       "      <td>83661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    item_0  item_1  item_2 item_3  item_4  item_5  item_6  item_7  item_8  item_9 item_10 item_11 item_12 item_13 item_14 item_15 item_16 item_17 item_18 item_19 item_20 item_21 item_22 item_23 item_24 item_25 item_26 item_27 item_28 item_29 item_30 item_31 item_32 item_33 item_34 item_35 item_36 item_37 item_38 item_39 item_40 item_41 item_42 item_43 item_44 item_45 item_46 item_47 item_48 item_49\n",
       "1   108007  102958   84267  94926   98806  108288   93960  105496   30956   76595  105886   64109  109322  104436   74204   63609  108151  109416  109735   75446  102650   99058   83204   80776   61628  114125  112806   87570  105116   94961  111560   88947   95690  112555  113094   79754  105590   99470   47192   93538   90602   80555  113708   93116   72782  102274   96063   95743   66688   96481\n",
       "2   102958   94926  108007  98806   93960  109322   76595   94961  105496  109416  104436   96063   74204  105116  113115   84267   30956  111318  113361  114125  113708   95965   99058   63609  111560  111751   93538   88947  112959  112806   73835  104747  105886  113467   75326  112555  108151   80555   55823   81311  108010  105590  110033   94632  104880   56287  104900   75446  102650   90602\n",
       "11  112351   88002  102863  98129   82194  110355   69793   53751  103411  113976  108948   25095  115250  116130   78213  106023   82044   39704  109136   53888  106286  102399  111215   48349   89888  104693   82138   52766   69911  105224  100821   88528   93379   96137   48051   19758   92218  110903   32990   28014  114125  101938  116778   94797   41138   80976   85899  101321  116697  108782\n",
       "13  108324  101669  102924  84166  103279   97032  111751  105886  108938  103630   81764  105423   45157  104693   84267  111599   74906  108567   78086  114495   57964   62216   88242  113655   54437   82515   87541   83457   87061  103142  115059   85254  100215  114085   69793  110077   91715   41170   87710   66873   72203   90813   76215   82904   73584   99030   82737  102129   62151   88528\n",
       "22  108324   78086   97032  57964  103279   84166  101669   96910   88242  103630  102924  111599  100215   81764  105886   62216  111751  105423  113567   84267   82737  104693  108567   82138   73584   54437  103142   85254   74906   87061   90217   72203   79932  105411  109136   82850   45157   85564   82419  109182  108938   91715   44185  113655   82515   78229   82109   52268   62151   83661"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    1,     2,    11,    13,    22,    34,    35,    44,    45,\n",
       "               55,\n",
       "            ...\n",
       "            35345, 35354, 35355, 35365, 35366, 35388, 35389, 35398, 35421,\n",
       "            35422],\n",
       "           dtype='int64', length=5079)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfr.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7848         1\n",
       "3756         2\n",
       "15374       11\n",
       "16425       13\n",
       "1168        22\n",
       "         ...  \n",
       "11625    35388\n",
       "14663    35389\n",
       "13316    35398\n",
       "16941    35421\n",
       "16203    35422\n",
       "Name: user_id, Length: 5079, dtype: int64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_click_val.user_id.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   item_0  item_1  item_2  item_3  item_4  item_5  item_6  item_7  item_8  item_9 item_10 item_11 item_12 item_13 item_14 item_15 item_16 item_17 item_18 item_19 item_20 item_21 item_22 item_23 item_24 item_25 item_26 item_27 item_28 item_29 item_30 item_31 item_32 item_33 item_34 item_35 item_36 item_37 item_38 item_39 item_40 item_41 item_42 item_43 item_44 item_45 item_46 item_47 item_48 item_49\n",
      "1  102958   94961  113361  105886  105496  105116  111560   88947  113115  109322   89975   45530  114125   74204  113094   81353   99058   93960  113708  107900  105012  108007   88936  113467  108621   91838  112562  104818   73689   84267   77012  109416   89082   95171   81715  104436   95965  102263   95743  110740   47192   52995  101509   90726   88931  104731  103740   98370   59702   97400\n",
      "2   94961  102958  113361  113115  105116  113467  112562  111560   88947  109322  105496  114125  108621  113708  112959  116114  103605  109121  110838  101975  115874   95965   81353   74204  109228   93960  107900  102956  109416   52995   42884   99794  107322   91838   72547   98370  101256  102349  113655  115202   99058  106289  101829  109284  113094   45070  105167   78532   56287   35234\n"
     ]
    }
   ],
   "source": [
    "print(dfr.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-243d8a632151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbe_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf14/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapplymap\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   6942\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6944\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6946\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf14/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6877\u001b[0m         )\n\u001b[0;32m-> 6878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6880\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf14/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf14/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 result = libreduction.compute_reduction(\n\u001b[0;32m--> 296\u001b[0;31m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf14/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   6940\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6941\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf14/lib/python3.7/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m    288\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;31m# inverse transform of empty array is empty array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf14/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad input shape ()"
     ]
    }
   ],
   "source": [
    "\n",
    "dfr = dfr.applymap(lambda: lbe_list[0].inverse_transform)\n",
    "print(dfr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
