{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple itemCF Baseline, score:0.1169(phase0-2) by青禹小生\n",
    "\n",
    "\n",
    "https://tianchi.aliyun.com/forum/postDetail?postId=103530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File underexpose_train/underexpose_train_click-0.csv does not exist: 'underexpose_train/underexpose_train_click-0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fe861b1dfbc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow_phase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phase:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mclick_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/underexpose_train_click-{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mclick_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/underexpose_test_click-{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File underexpose_train/underexpose_train_click-0.csv does not exist: 'underexpose_train/underexpose_train_click-0.csv'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python  \n",
    "# -*- coding:utf-8 -*-  \n",
    "  \n",
    "import pandas as pd  \n",
    "from tqdm import tqdm  \n",
    "from collections import defaultdict  \n",
    "import math  \n",
    "  \n",
    "def get_sim_item(df, user_col, item_col, use_iif=False):  \n",
    "    user_item_ = df.groupby(user_col)[item_col].agg(set).reset_index()  \n",
    "    user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))  \n",
    "  \n",
    "    sim_item = {}  \n",
    "    item_cnt = defaultdict(int)  \n",
    "    for user, items in tqdm(user_item_dict.items()):  \n",
    "        for i in items:  \n",
    "            item_cnt[i] += 1  \n",
    "            sim_item.setdefault(i, {})  \n",
    "            for relate_item in items:  \n",
    "                if i == relate_item:  \n",
    "                    continue  \n",
    "                sim_item[i].setdefault(relate_item, 0)  \n",
    "                if not use_iif:  \n",
    "                    sim_item[i][relate_item] += 1  \n",
    "                else:  \n",
    "                    sim_item[i][relate_item] += 1 / math.log(1 + len(items))  \n",
    "    sim_item_corr = sim_item.copy()  \n",
    "    for i, related_items in tqdm(sim_item.items()):  \n",
    "        for j, cij in related_items.items():  \n",
    "            sim_item_corr[i][j] = cij/math.sqrt(item_cnt[i]*item_cnt[j])  \n",
    "  \n",
    "    return sim_item_corr, user_item_dict  \n",
    "  \n",
    "\n",
    "def recommend(sim_item_corr, user_item_dict, user_id, top_k, item_num):  \n",
    "    rank = {}  \n",
    "    interacted_items = user_item_dict[user_id]  \n",
    "    for i in interacted_items:  \n",
    "        for j, wij in sorted(sim_item_corr[i].items(), reverse=True)[0:top_k]:  \n",
    "            if j not in interacted_items:  \n",
    "                rank.setdefault(j, 0)  \n",
    "                rank[j] += wij  \n",
    "    return sorted(rank.items(), key=lambda d: d[1], reverse=True)[:item_num]  \n",
    "  \n",
    "\n",
    "# fill user to 50 items  \n",
    "def get_predict(df, pred_col, top_fill):  \n",
    "    top_fill = [int(t) for t in top_fill.split(',')]  \n",
    "    scores = [-1 * i for i in range(1, len(top_fill) + 1)]  \n",
    "    ids = list(df['user_id'].unique())  \n",
    "    fill_df = pd.DataFrame(ids * len(top_fill), columns=['user_id'])  \n",
    "    fill_df.sort_values('user_id', inplace=True)  \n",
    "    fill_df['item_id'] = top_fill * len(ids)  \n",
    "    fill_df[pred_col] = scores * len(ids)  \n",
    "    df = df.append(fill_df)  \n",
    "    df.sort_values(pred_col, ascending=False, inplace=True)  \n",
    "    df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')  \n",
    "    df['rank'] = df.groupby('user_id')[pred_col].rank(method='first', ascending=False)  \n",
    "    df = df[df['rank'] <= 50]  \n",
    "    df = df.groupby('user_id')['item_id'].apply(lambda x: ','.join([str(i) for i in x])).str.split(',', expand=True).reset_index()  \n",
    "    return df  \n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    now_phase = 1  \n",
    "    train_path = 'underexpose_train'  \n",
    "    test_path = 'underexpose_test'  \n",
    "    recom_item = []  \n",
    "  \n",
    "    whole_click = pd.DataFrame()  \n",
    "    for c in range(now_phase + 1):  \n",
    "        print('phase:', c)  \n",
    "        click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "        click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "  \n",
    "        all_click = click_train.append(click_test)  \n",
    "        whole_click = whole_click.append(all_click)  \n",
    "        item_sim_list, user_item = get_sim_item(all_click, 'user_id', 'item_id', use_iif=True)  \n",
    "  \n",
    "        for i in tqdm(click_test['user_id'].unique()):  \n",
    "            rank_item = recommend(item_sim_list, user_item, i, 500, 50)  \n",
    "            for j in rank_item:  \n",
    "                recom_item.append([i, j[0], j[1]])  \n",
    "    # find most popular items  \n",
    "    top50_click = whole_click['item_id'].value_counts().index[:50].values  \n",
    "    top50_click = ','.join([str(i) for i in top50_click])  \n",
    "  \n",
    "    recom_df = pd.DataFrame(recom_item, columns=['user_id', 'item_id', 'sim'])  \n",
    "    result = get_predict(recom_df, 'sim', top50_click)  \n",
    "    result.to_csv('baseline.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开源版本 0.2\n",
    "https://mp.weixin.qq.com/s/GbnhlATPGHwVsmF069VpHw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"该赛道的数据集强调电商推荐系统的公平性，尤其是流量较少的广大中小商家所面临的“有好货缺无人问津”的困境。数据横跨十余天，中间还穿插了某次全网促销活动，涵盖了一些商品从上新时无人问津、到逐渐成为高潜力爆款的历程。欲获胜的队伍需格外关注曝光不足的商品上的推荐准确度，需探索“如何抵消掉历史点击数据的选择性偏差以便避免只推爆款”、“如何注意数据分布随时间的变化以便及时发现高潜力冷门好货”、“如何利用多模态图文商品信息来辅助商品冷启动”等重要课题。\"--解题关键\n",
    "\n",
    "       首先感谢青禹小生的开源，本文是在其基础上进行更多的细化， 也将对更多优化方向和建模思路进行简单介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_click = whole_click.drop_duplicates(subset=['user_id','item_id','time'],keep='last')\n",
    "whole_click = whole_click.sort_values('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关联商品打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_item(df_, user_col, item_col, use_iif=False):\n",
    "\n",
    "    df = df_.copy()\n",
    "    user_item_ = df.groupby(user_col)[item_col].agg(list).reset_index()\n",
    "    user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))\n",
    "\n",
    "    user_time_ = df.groupby(user_col)['time'].agg(list).reset_index() # 引入时间因素\n",
    "    user_time_dict = dict(zip(user_time_[user_col], user_time_['time']))\n",
    "\n",
    "    sim_item = {}  \n",
    "    item_cnt = defaultdict(int)  # 商品被点击次数\n",
    "    for user, items in tqdm(user_item_dict.items()):  \n",
    "        for loc1, item in enumerate(items):  \n",
    "            item_cnt[item] += 1  \n",
    "            sim_item.setdefault(item, {})  \n",
    "            for loc2, relate_item in enumerate(items):  \n",
    "                if item == relate_item:  \n",
    "                    continue  \n",
    "                t1 = user_time_dict[user][loc1] # 点击时间提取\n",
    "                t2 = user_time_dict[user][loc2]\n",
    "                sim_item[item].setdefault(relate_item, 0)  \n",
    "                if not use_iif:  \n",
    "                    if loc1-loc2>0:\n",
    "                        sim_item[item][relate_item] += 1 * 0.7 * (0.8**(loc1-loc2-1)) * (1 - (t1 - t2) * 10000) / math.log(1 + len(items)) # 逆向\n",
    "                    else:\n",
    "                        sim_item[item][relate_item] += 1 * 1.0 * (0.8**(loc2-loc1-1)) * (1 - (t2 - t1) * 10000) / math.log(1 + len(items)) # 正向\n",
    "                else:  \n",
    "                    sim_item[item][relate_item] += 1 / math.log(1 + len(items))\n",
    "\n",
    "    sim_item_corr = sim_item.copy() # 引入AB的各种被点击次数  \n",
    "    for i, related_items in tqdm(sim_item.items()):  \n",
    "        for j, cij in related_items.items():  \n",
    "            sim_item_corr[i][j] = cij / ((item_cnt[i] * item_cnt[j]) ** 0.2)\n",
    "\n",
    "    return sim_item_corr, user_item_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里在原有基础上考虑了两点因素，关联位置因素和关联时间因素。强关联的发生是有向的、有位置的和有时间的。\n",
    "\n",
    "比如我们先买了手机，那下一次买手机壳的关联，和先买手机壳再买手机的关联，这两种很明显，A到B大于B到A，这是有向性；\n",
    "\n",
    "我先买了手机，然后买了手机壳，又买了耳机，很明显，手机和手机壳的关联性大于手机与耳机的关联性，这是位置性；\n",
    "\n",
    "那么如果再加上时间这层因素，时间相隔越远的关联性肯定是不高的。\n",
    "\n",
    "这三点因素就可以组成我们的优化思路，有向性打分__位置打分__时间打分，得到最终关联打分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交互行为打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(sim_item_corr, user_item_dict, user_id, top_k, item_num):\n",
    "\n",
    "    rank = {}  \n",
    "    interacted_items = user_item_dict[user_id] \n",
    "    interacted_items = interacted_items[::-1]\n",
    "    for loc, i in enumerate(interacted_items):  \n",
    "        for j, wij in sorted(sim_item_corr[i].items(), reverse=True)[0:top_k]:  \n",
    "            if j not in interacted_items:  \n",
    "                rank.setdefault(j, 0)  \n",
    "                rank[j] += wij * (0.7**loc) \n",
    "                \n",
    "    return sorted(rank.items(), key=lambda d: d[1], reverse=True)[:item_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的优化也很符合我们的主观认识，距离下次点击月近的行为，相关性越接近，所有可以根据位置远近考虑重要性，添加权重因子。当然还可以添加时间权重因子。\n",
    "\n",
    "\n",
    "\n",
    "## 优化方向\n",
    "\n",
    "切勿陷入思维定势，也许我的优化方向和baseline会使大家产生一个误区。就像之前安泰杯的比赛，决赛中评委也说到了baseline，很大程度影响的大家的思维方向。没有说baseline不好，而这只能作为无数解题思路中的一小部分，一个分支。不是沿着这个分支走下去，而是去创造更多分支。__这就如同多路召回，之将其当作融合的一部分罢了__。\n",
    "\n",
    "目前只是从关联的角度解决问题，这个角度可以做的更细。也可以考虑其它方向，目前大家做的还都是起点，而不是终点。向量召回、模型召回，以及后面的排序都需要进一步尝试。\n",
    "\n",
    "\n",
    "\n",
    "## 建模思路\n",
    "\n",
    "没有相关经验的同学可能会问\n",
    "### 正负样本怎么来，\n",
    "那其实很简单，需要我们去构造label。这里分为两步：\n",
    "\n",
    "#### 样本提取。\n",
    "我们线下验证的时候，一般是用户最后一次点击进行验证，会进行召回50个商品，然后观察召回率。这样的50个商品及对应的用户就是样本数据。\n",
    "\n",
    "#### 样本打标。\n",
    "召回的50个商品中是最后一次点击的商品labael是1，反之为0。\n",
    "\n",
    "这样我们就能得到训练集，测试集构造方式一样，只不过需要去预测其label，最后将label的概率进行排序，top50就是最终建模得到的结果。\n",
    "\n",
    "\n",
    "\n",
    "最后我们就\n",
    "#### 可以像一般的二分类问题进行解题了。\n",
    "为了防止数据泄露，构造特征时一定要用历史的点击行为进行构造。\n",
    "\n",
    "根据历史数据，给定上一次点击数据情况下。用户点击过某商品则label=1,其他皆为负样本。负样本可抽样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Recall Method based on Network-based Inference by BruceQD\n",
    "https://tianchi.aliyun.com/forum/postDetail?spm=5176.12281915.0.0.50cc29c82GYcQg&postId=104936\n",
    "\n",
    "开源的recall思路\n",
    "\n",
    "\n",
    "召回阶段很重要，可尝试多种方式、方法。下面给出一种基于二分网络的召回方法，该方法来自于电子科技大学周涛教授的代表作“Bipartite network projection and personal recommendation”。其核心思想在于下图，建议感兴趣的同学去读读这篇论文。\n",
    "\n",
    "思路还很多, local random walk、common neighbors based on bayesian之类的, 基于里面分享的论文，可以去查查引用它的文献\n",
    "\n",
    "\n",
    "核心代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_item(df, user_col, item_col, use_iif=False):  \n",
    "    user_item_ = df.groupby(user_col)[item_col].agg(set).reset_index()  \n",
    "    user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))  \n",
    "    \n",
    "    item_user_ = whole_click.groupby(item_col)[user_col].agg(set).reset_index()  \n",
    "    item_user_dict = dict(zip(item_user_[item_col], item_user_[user_col]))    \n",
    "\n",
    "    item_cnt = defaultdict(int)  \n",
    "    for user, items in tqdm(user_item_dict.items()):  \n",
    "        for i in items:  \n",
    "            item_cnt[i] += 1  \n",
    "\n",
    "    sim_item = {}\n",
    "\n",
    "    for item, users in tqdm(item_user_dict.items()):\n",
    "    \n",
    "        sim_item.setdefault(item, {}) \n",
    "    \n",
    "        for u in users:\n",
    "        \n",
    "            tmp_len = len(user_item_dict[u])\n",
    "        \n",
    "            for relate_item in user_item_dict[u]:\n",
    "                sim_item[item].setdefault(relate_item, 0)\n",
    "                sim_item[item][relate_item] += 1/ (math.log(len(users)+1) * math.log(tmp_len+1))\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "    return sim_item, user_item_dict  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他部分可参考青羽小生开源的baseline。\n",
    "\n",
    "## 召回阶段的效果决定了排序后的天花板，所以要多尝试些思路提高召回效果，尽可能保证真正点击的商品在召回列表中。之后，再做重排序（第二阶段），甚至精排序（第三阶段）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipartite network projection and personal recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abstract\n",
    "One-mode projecting is extensively used to compress bipartite networks. Since one-mode projection is always less informative than the bipartite representation, a proper weighting method is required to better retain the original information. In this article, inspired by the network-based resource-allocation dynamics, we raisea weighting method which can be directly applied in extracting the hidden information of networks, with remarkably better performance than the widely used global ranking method as well as collaborative filtering. This work not only provides a creditable method for compressing bipartite networks, but also highlights apossible way for the better solution of a long-standing challenge in modern information science: How to do a personal recommendation.\n",
    "\n",
    "单模投影被广泛用于压缩两方网络。 由于单模投影的信息量总是少于二分法表示，因此需要一种适当的加权方法以更好地保留原始信息。 在本文中，基于网络资源分配动态的启发，我们提出了一种加权方法，该方法可直接用于提取网络的隐藏信息，其性能要比广泛使用的全局排名方法和协作过滤明显好。 不仅提供了一种可靠的压缩双向网络的方法，而且还强调了一种可行的方法，可以更好地解决现代信息科学中长期存在的挑战：如何进行个人推荐。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "A particular class of networks is \n",
    "#### the bipartite networks, whose nodes are divided into two sets XandY, and only the con-nection between two nodes in different sets is allowed\u0002as illustrated in Fig.1\u0004a\n",
    "![](img/fig01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the convenience of directly showing the relations among a particular set of nodes, the bipartite network is usu-ally compressed by one-mode projecting. The one-mode pro-jection onto X--X projection for short means a network containing only X nodes, where two Xnodes are connectedwhen they have at least one common neighboring Y node.\n",
    "\n",
    "The simplest way is to project the bipartite   network   onto   an   unweighted   network, without taking into account the frequency that a collaboration has been repeatedn, the loss of information is obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bipartite network projection\n",
    "Without loss of generality, we discuss how to determinethe edge weight in Xprojection, where the weight w_ij can be considered as the importance of node i in j’s sense, and it is generally not equal to w_ji. \n",
    "\n",
    "For example, in the book projec-tion of a customer-book opinion network, the weight wij be-tween two books i and j contributes to the strength of book i recommendation to a customer provided he has bought book j. \n",
    "\n",
    "为了得出wij的解析表达式，我们回到二分表示法。 由于双向网络本身未加权，因此，任意Xnode中的资源应平均分配给Y中的邻居。 类似地，anyYnode中的资源应平均分配到其Xneighbors。 如图2a所示，最初为这三个X节点分配了权重x，y和z。 资源分配过程包括两个步骤： 首先从XtoY，然后回到X。 每个步骤之后的资源量分别在图2b和2c中标记。 将这两个步骤合并为一个\n",
    "\n",
    "\n",
    "![](img/img1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
