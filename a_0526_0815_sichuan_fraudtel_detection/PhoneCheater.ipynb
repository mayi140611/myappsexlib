{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.stats import entropy, pearsonr, stats\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "path = \"./data/\"\n",
    "user_data = \"./user_data/\"\n",
    "\n",
    "\n",
    "def get_app_feats(df):\n",
    "    \"\"\"\n",
    "    构造app特征\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(df.head())\n",
    "    print(df[\"busi_name\"].value_counts())\n",
    "    phones_app = df[[\"phone_no_m\"]].copy()\n",
    "    phones_app = phones_app.drop_duplicates(subset=['phone_no_m'], keep='last')\n",
    "    # 1 对原表按照['phone_no_m']groupby,对app的name进行unique记录一个电话使用多少APP\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"busi_name\"].agg(busi_count=\"nunique\")\n",
    "    # 将原表的phone_no_m进行记录 然后与之前的 app_count merge\n",
    "    phones_app = phones_app.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \"\"\"使用的流量统计\n",
    "    \"\"\"\n",
    "    # 2 对每个电话号码使用流量的数值特征进行统计\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"flow\"].agg(flow_mean=\"mean\",\n",
    "                                               flow_median=\"median\",\n",
    "                                               flow_min=\"min\",\n",
    "                                               flow_max=\"max\",\n",
    "                                               flow_var=\"var\",\n",
    "                                               flow_sum=\"sum\")\n",
    "\n",
    "    phones_app = phones_app.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    # 3 对每个电话号码流量的活跃月份进行个数统计\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"month_id\"].agg(month_ids=\"nunique\")\n",
    "    phones_app = phones_app.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    # 月流量使用统计\n",
    "    phones_app[\"flow_month\"] = phones_app[\"flow_sum\"] / phones_app[\"month_ids\"]\n",
    "\n",
    "    return phones_app\n",
    "\n",
    "\n",
    "def get_voc_feat(df):\n",
    "    \"\"\"\n",
    "    构造通话行为特征\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df[\"start_datetime\"] = pd.to_datetime(df['start_datetime'])\n",
    "    df[\"hour\"] = df['start_datetime'].dt.hour\n",
    "    df[\"day\"] = df['start_datetime'].dt.day\n",
    "    print(df.head())\n",
    "    phone_no_m = df[[\"phone_no_m\"]].copy()\n",
    "    phone_no_m = phone_no_m.drop_duplicates(subset=['phone_no_m'], keep='last')\n",
    "    # 总通话话次数以及总通话的不同对端号码的人数统计\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"opposite_no_m\"].agg(opposite_count=\"count\", opposite_unique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \"\"\"主叫通话\n",
    "    \"\"\"\n",
    "    # 主叫通话的不同加密设备个数 以及主叫通话的次数\n",
    "    df_call = df[df[\"calltype_id\"] == 1].copy()\n",
    "    tmp = df_call.groupby(\"phone_no_m\")[\"imei_m\"].agg(voccalltype1=\"count\", imeis=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    # 主叫通话次数/通话总次数\n",
    "    phone_no_m[\"voc_calltype1\"] = phone_no_m[\"voccalltype1\"] / phone_no_m[\"opposite_count\"]\n",
    "\n",
    "    # 主叫通话的收费号码所在城市的unique\n",
    "    tmp = df_call.groupby(\"phone_no_m\")[\"city_name\"].agg(city_name_call=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    # 主叫通话的收费号码所在区县的uniques\n",
    "    tmp = df_call.groupby(\"phone_no_m\")[\"county_name\"].agg(county_name_call=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "\n",
    "    \"\"\"和固定通话者的对话统计\n",
    "    \"\"\"\n",
    "    # 与固定通话者通话次数和通话总时长的统计  然后再对phone_no_m groupby统计求每一个收费号码对所有对端电话通话次数的最大值 均值 中位数\n",
    "    tmp = df.groupby([\"phone_no_m\", \"opposite_no_m\"])[\"call_dur\"].agg(count=\"count\", sum=\"sum\")\n",
    "    # 与所有通话者总次数的中位数 最大 均值\n",
    "    phone2opposite = tmp.groupby(\"phone_no_m\")[\"count\"].agg(phone2opposite_mean=\"mean\", phone2opposite_median=\"median\",\n",
    "                                                            phone2opposite_max=\"max\")\n",
    "    phone_no_m = phone_no_m.merge(phone2opposite, on=\"phone_no_m\", how=\"left\")\n",
    "    # 与所有通话者总时间的 均值 中位数 最大值\n",
    "    phone2opposite = tmp.groupby(\"phone_no_m\")[\"sum\"].agg(phone2oppo_sum_mean=\"mean\", phone2oppo_sum_median=\"median\",\n",
    "                                                          phone2oppo_sum_max=\"max\")\n",
    "    phone_no_m = phone_no_m.merge(phone2opposite, on=\"phone_no_m\", how=\"left\")\n",
    "\n",
    "    \"\"\"通话时间长短统计\n",
    "    \"\"\"\n",
    "    # 所有通话的总时长 的均值 最大值 中位数 最小值\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"call_dur\"].agg(call_dur_mean=\"mean\", call_dur_median=\"median\", call_dur_max=\"max\",\n",
    "                                                   call_dur_min=\"min\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "\n",
    "    # 所有收费号码 通话所在地市名称的个数\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"city_name\"].agg(city_name_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    # 所有收费号码通话所在县区的个数\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"county_name\"].agg(county_name_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    # 所有收费号码通话种类的个数\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"calltype_id\"].agg(calltype_id_unique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "\n",
    "    \"\"\"通话时间点偏好\n",
    "    \"\"\"\n",
    "    # 某一电话通话时间点 出现最多的hour 以及出现最多的次数以及不同时间点的个数\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"hour\"].agg(voc_hour_mode=lambda x: stats.mode(x)[0][0],\n",
    "                                               voc_hour_mode_count=lambda x: stats.mode(x)[1][0],\n",
    "                                               voc_hour_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    # 某一电话通话时间点 出现最多的day 以及出现最多的次数以及不同时间点的个数\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"day\"].agg(voc_day_mode=lambda x: stats.mode(x)[0][0],\n",
    "                                              voc_day_mode_count=lambda x: stats.mode(x)[1][0],\n",
    "                                              voc_day_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    return phone_no_m\n",
    "\n",
    "\n",
    "def get_sms_feats(df):\n",
    "    \"\"\"\n",
    "    构造 短信行为特征\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(df.head())\n",
    "    df['request_datetime'] = pd.to_datetime(df['request_datetime'])\n",
    "    df[\"hour\"] = df['request_datetime'].dt.hour\n",
    "    df[\"day\"] = df['request_datetime'].dt.day\n",
    "    #     df[\"month\"] = df['request_datetime'].dt.month\n",
    "\n",
    "    phone_no_m = df[[\"phone_no_m\"]].copy()\n",
    "    phone_no_m = phone_no_m.drop_duplicates(subset=['phone_no_m'], keep='last')\n",
    "    # 短信通信对话次数  通信对话人数\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"opposite_no_m\"].agg(sms_count=\"count\", sms_nunique=\"nunique\")\n",
    "    # 与人之间的短信通信比(看是否和一个人发了多个短信)\n",
    "    tmp[\"sms_rate\"] = tmp[\"sms_count\"] / tmp[\"sms_nunique\"]\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "\n",
    "    \"\"\"短信下行比例\n",
    "    \"\"\"\n",
    "    calltype2 = df[df[\"calltype_id\"] == 2].copy()\n",
    "    # 短信下行次数\n",
    "    calltype2 = calltype2.groupby(\"phone_no_m\")[\"calltype_id\"].agg(calltype_2=\"count\")\n",
    "    phone_no_m = phone_no_m.merge(calltype2, on=\"phone_no_m\", how=\"left\")\n",
    "    # 短信下行比例(短信下行次数/短信总次数)\n",
    "    phone_no_m[\"calltype_rate\"] = phone_no_m[\"calltype_2\"] / phone_no_m[\"sms_count\"]\n",
    "\n",
    "    \"\"\"短信时间\n",
    "    \"\"\"\n",
    "    # 发送/接受 短信最多的时间点 最多时间点的短信的次数(可以改成 下行短信的时间特征)\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"hour\"].agg(hour_mode=lambda x: stats.mode(x)[0][0],\n",
    "                                               hour_mode_count=lambda x: stats.mode(x)[1][0],\n",
    "                                               hour_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    # 发送/接受 短信最多的天数 以及那天发了多少短信\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"day\"].agg(day_mode=lambda x: stats.mode(x)[0][0],\n",
    "                                              day_mode_count=lambda x: stats.mode(x)[1][0],\n",
    "                                              day_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "\n",
    "    return phone_no_m\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    test_app_feat = pd.read_csv(user_data + 'test_app_feat.csv')\n",
    "    test_voc_feat = pd.read_csv(user_data + 'test_voc_feat.csv')\n",
    "    test_sms_feat = pd.read_csv(user_data + \"test_sms_feat.csv\")\n",
    "    test_user = pd.read_csv(path + 'test/test_user.csv')\n",
    "    test_user = test_user.merge(test_app_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    test_user = test_user.merge(test_voc_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    test_user = test_user.merge(test_sms_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    # 有问题\n",
    "    train_user = pd.read_csv(path + 'train/train_user.csv')\n",
    "    le_city = LabelEncoder()\n",
    "    city_name = pd.concat([test_user[\"city_name\"].astype(np.str), train_user[\"city_name\"].astype(np.str)])\n",
    "    le_city.fit(city_name)\n",
    "    le_county = LabelEncoder()\n",
    "    country_name = pd.concat([test_user[\"county_name\"].astype(np.str), train_user[\"county_name\"].astype(np.str)])\n",
    "    le_county.fit(country_name)\n",
    "\n",
    "    test_user[\"city_name\"] = le_city.transform(test_user[\"city_name\"].astype(np.str))\n",
    "    test_user[\"county_name\"] = le_county.transform(test_user[\"county_name\"].astype(np.str))\n",
    "\n",
    "    train_app_feat = pd.read_csv(user_data + \"train_app_feat.csv\")\n",
    "    train_voc_feat = pd.read_csv(user_data + \"train_voc_feat.csv\")\n",
    "    train_sms_feat = pd.read_csv(user_data + \"train_sms_feat.csv\")\n",
    "    train_user = pd.read_csv(path + 'train/train_user.csv')\n",
    "    # 把3月份之前的消费信息全部删除 用3月作4月\n",
    "    drop_r = [\"arpu_201908\", \"arpu_201909\", \"arpu_201910\", \"arpu_201911\", \"arpu_201912\", \"arpu_202001\", \"arpu_202002\"]\n",
    "    train_user.drop(drop_r, axis=1, inplace=True)\n",
    "    train_user.rename(columns={\"arpu_202003\": \"arpu_202004\"}, inplace=True)\n",
    "    train_user = train_user.merge(train_app_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    train_user = train_user.merge(train_voc_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    train_user = train_user.merge(train_sms_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    # 有问题\n",
    "    train_user[\"city_name\"] = le_city.fit_transform(train_user[\"city_name\"].astype(np.str))\n",
    "    train_user[\"county_name\"] = le_county.transform(train_user[\"county_name\"].astype(np.str))\n",
    "\n",
    "    sub = test_user[[\"phone_no_m\"]].copy()\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'num_leaves': 31,\n",
    "        'max_bin': 50,\n",
    "        'max_depth': 6,\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"colsample_bytree\": 0.8,  # 每次迭代中随机选择特征的比例\n",
    "        \"bagging_fraction\": 0.8,  # 每次迭代时用的数据比例\n",
    "        'min_child_samples': 25,\n",
    "        'n_jobs': -1,\n",
    "        'silent': True,  # 信息输出设置成1则没有信息输出\n",
    "        'seed': 1000,\n",
    "    }\n",
    "    train_label = train_user[[\"label\"]].copy()\n",
    "    print(np.sum(train_label), train_label.shape, train_label.head())\n",
    "\n",
    "    test_user.drop([\"phone_no_m\"], axis=1, inplace=True)\n",
    "    train_user.drop([\"phone_no_m\", \"label\"], axis=1, inplace=True)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1024)\n",
    "    test_p = []\n",
    "    threshold = 0\n",
    "    scores = 0\n",
    "    for i, (train_index, vaild_index) in enumerate(kf.split(train_user, train_label[\"label\"])):\n",
    "        #         print('第{}次训练...'.format(i+1))\n",
    "        train_x = train_user.iloc[train_index]\n",
    "        train_y = train_label.iloc[train_index]\n",
    "        valid_x = train_user.iloc[vaild_index]\n",
    "        valid_y = train_label.iloc[vaild_index]\n",
    "\n",
    "        cat = [\"city_name\", \"county_name\"]\n",
    "        lgb_train = lgb.Dataset(train_x, train_y, categorical_feature=cat, silent=True)\n",
    "        lgb_eval = lgb.Dataset(valid_x, valid_y, reference=lgb_train, categorical_feature=cat, silent=True)\n",
    "\n",
    "        gbm = lgb.train(params, lgb_train, num_boost_round=1000, valid_sets=[lgb_train, lgb_eval], verbose_eval=100,\n",
    "                        early_stopping_rounds=200)\n",
    "\n",
    "        vaild_preds = gbm.predict(valid_x, num_iteration=gbm.best_iteration)\n",
    "        bp, bestf1 = score_vail(vaild_preds, valid_y)\n",
    "        # bp 最佳划分阈值 bestf1最佳分数\n",
    "        print(\"score: \", bestf1, bp)\n",
    "        scores += bestf1\n",
    "        threshold += bp\n",
    "        #         sauc = myFevalAuc(vaild_preds, valid_y)\n",
    "        #         importance = gbm.feature_importance()\n",
    "        #         feature_name = gbm.feature_name()\n",
    "        #         feature_importance = pd.DataFrame({'feature_name':feature_name,'importance':importance} ).sort_values(by='importance',ascending=False)\n",
    "        #         feature_importance.to_csv('feature_importance_{}.csv'.format(idx+1),index=None,encoding='utf-8-sig')\n",
    "        test_pre = gbm.predict(test_user, num_iteration=gbm.best_iteration)\n",
    "        test_pre = MinMaxScaler().fit_transform(test_pre.reshape(-1, 1))\n",
    "        test_pre = test_pre.reshape(-1, )\n",
    "        test_p.append(test_pre)\n",
    "    threshold = threshold / len(test_p)\n",
    "    print(threshold)\n",
    "    print(\"五折平均分数: \", scores / len(test_p))\n",
    "    test_p = np.array(test_p)\n",
    "    test_p = test_p.mean(axis=0)\n",
    "    sub[\"prob\"] = test_p\n",
    "    sub[\"label\"] = sub[\"prob\"] > round(np.percentile(sub[\"prob\"], threshold), 4)\n",
    "    sub[[\"phone_no_m\", \"label\"]].to_csv('submission.csv', index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "def score_vail(vaild_preds, real):\n",
    "    \"\"\"\n",
    "\n",
    "    :param vaild_preds: 模型预测结果\n",
    "    :param real: 模型标签\n",
    "    :return: 最佳分位数 最佳f1\n",
    "    \"\"\"\n",
    "    #     import matplotlib.pylab as plt\n",
    "    #     plt.figure(figsize=(16,5*10))\n",
    "    best = 0\n",
    "    bp = 0\n",
    "    score = []\n",
    "    for i in range(600):\n",
    "        p = 32 + i * 0.08\n",
    "        threshold_test = round(np.percentile(vaild_preds, p), 4)\n",
    "        pred_int = vaild_preds > threshold_test\n",
    "        ff = f1_score(pred_int, real)\n",
    "        score.append(ff)\n",
    "\n",
    "        if ff >= best:\n",
    "            best = ff\n",
    "            bp = p\n",
    "    #     plt.plot(range(len(score)), score)\n",
    "    #     plt.show()\n",
    "    return bp, best\n",
    "\n",
    "\n",
    "def feats():\n",
    "    # path = \"./data/\"\n",
    "    # user_data = \"./user_data/\"\n",
    "    test_voc = pd.read_csv(path + 'test/test_voc.csv', )\n",
    "    test_voc_feat = get_voc_feat(test_voc)\n",
    "    test_voc_feat.to_csv(user_data + 'test_voc_feat.csv', index=False)\n",
    "\n",
    "    test_app = pd.read_csv(path + 'test/test_app.csv', )\n",
    "    test_app_feat = get_app_feats(test_app)\n",
    "    test_app_feat.to_csv(user_data + \"test_app_feat.csv\", index=False)\n",
    "\n",
    "    test_sms = pd.read_csv(path + 'test/test_sms.csv', )\n",
    "    test_sms_feat = get_sms_feats(test_sms)\n",
    "    test_sms_feat.to_csv(user_data + \"test_sms_feat.csv\", index=False)\n",
    "\n",
    "    train_voc = pd.read_csv(path + 'train/train_voc.csv', )\n",
    "    train_voc_feat = get_voc_feat(train_voc)\n",
    "    train_voc_feat.to_csv(user_data + \"train_voc_feat.csv\", index=False)\n",
    "\n",
    "    train_app = pd.read_csv(path + 'train/train_app.csv', )\n",
    "    train_app_feat = get_app_feats(train_app)\n",
    "    train_app_feat.to_csv(user_data + \"train_app_feat.csv\", index=False)\n",
    "\n",
    "    train_sms = pd.read_csv(path + 'train/train_sms.csv', )\n",
    "    train_sms_feat = get_sms_feats(train_sms)\n",
    "    train_sms_feat.to_csv(user_data + \"train_sms_feat.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    1962\n",
      "dtype: int64 (6106, 1)    label\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyl/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/wyl/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:842: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  .format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.981495\tvalid_1's auc: 0.968091\n",
      "[200]\ttraining's auc: 0.991874\tvalid_1's auc: 0.967428\n",
      "[300]\ttraining's auc: 0.997155\tvalid_1's auc: 0.964769\n",
      "Early stopping, best iteration is:\n",
      "[131]\ttraining's auc: 0.985384\tvalid_1's auc: 0.96953\n",
      "score:  0.9037433155080212 70.96000000000001\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyl/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/wyl/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:842: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  .format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.983327\tvalid_1's auc: 0.950463\n",
      "[200]\ttraining's auc: 0.992969\tvalid_1's auc: 0.95139\n",
      "[300]\ttraining's auc: 0.998119\tvalid_1's auc: 0.952228\n",
      "Early stopping, best iteration is:\n",
      "[150]\ttraining's auc: 0.989131\tvalid_1's auc: 0.953176\n",
      "score:  0.873972602739726 72.48\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyl/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/wyl/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:842: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  .format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.982525\tvalid_1's auc: 0.960208\n",
      "[200]\ttraining's auc: 0.991436\tvalid_1's auc: 0.963483\n",
      "[300]\ttraining's auc: 0.997544\tvalid_1's auc: 0.964129\n",
      "[400]\ttraining's auc: 0.999701\tvalid_1's auc: 0.963806\n",
      "Early stopping, best iteration is:\n",
      "[286]\ttraining's auc: 0.996987\tvalid_1's auc: 0.96415\n",
      "score:  0.9045092838196287 70.4\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyl/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/wyl/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:842: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  .format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.980398\tvalid_1's auc: 0.963449\n",
      "[200]\ttraining's auc: 0.99098\tvalid_1's auc: 0.962689\n",
      "[300]\ttraining's auc: 0.9969\tvalid_1's auc: 0.960159\n",
      "Early stopping, best iteration is:\n",
      "[127]\ttraining's auc: 0.984331\tvalid_1's auc: 0.963935\n",
      "score:  0.8939974457215837 68.0\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyl/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/wyl/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:842: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  .format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.98343\tvalid_1's auc: 0.954171\n",
      "[200]\ttraining's auc: 0.992076\tvalid_1's auc: 0.952227\n",
      "[300]\ttraining's auc: 0.996425\tvalid_1's auc: 0.950674\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttraining's auc: 0.984575\tvalid_1's auc: 0.95426\n",
      "score:  0.8913342503438789 72.56\n",
      "70.88000000000001\n",
      "五折平均分数:  0.8935113796265677\n"
     ]
    }
   ],
   "source": [
    "# feats()\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_voc_feat.to_csv(user_data+ 'test_voc_feat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./user_data/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
