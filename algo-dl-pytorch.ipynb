{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp algo.dl.pytorch\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algo-dl-pytorch\n",
    "PyTorch是由Facebook的人工智能部门开发的机器学习和深度学习工具。 \n",
    "\n",
    "它是使用Python和C ++语言编写的。 \n",
    "\n",
    "It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "    A replacement for NumPy to use the power of GPUs\n",
    "    a deep learning research platform that provides maximum flexibility and speed\n",
    "    \n",
    "您可以在需要时重用自己喜欢的Python软件包（例如NumPy，SciPy和Cython）来扩展PyTorch。\n",
    "\n",
    "https://pytorch.org/\n",
    "\n",
    "https://github.com/pytorch/pytorch\n",
    "\n",
    "https://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch处理序列任务示例\n",
    "[pytorch官方示例](demo_seq_cls/pytorch官方示例.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spectra-torch==0.4.0\r\n",
      "torch==1.6.0\r\n",
      "torchaudio==0.5.1\r\n",
      "torchsummary==1.5.1\r\n",
      "torchtext==0.7.0\r\n",
      "torchvision==0.4.2\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch -U #-i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "!pip freeze | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "None == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "None != None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuda相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch中使用指定的GPU\n",
    "1.1 直接终端中设定：\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1 python my_script.py\n",
    "\n",
    "1.2 python代码中设定：\n",
    "\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "使用函数 set_device \n",
    "\n",
    "过官方建议使用CUDA_VISIBLE_DEVICES，不建议使用 set_device 函数。\n",
    "\n",
    "    import torch\n",
    "    torch.cuda.set_device(id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch处理框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据迭代器\n",
    "# 构建MyDataset实例\n",
    "train_data = RMBDataset(data_dir=train_dir, transform=train_transform)\n",
    "valid_data = RMBDataset(data_dir=valid_dir, transform=valid_transform)\n",
    "\n",
    "# 构建DataLoder\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=BATCH_SIZE)\n",
    "# 构建网络并初始化\n",
    "net = LeNet(classes=2)\n",
    "net.initialize_weights()\n",
    "# 初始化loss\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "# 初始化optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)            \n",
    "# 初始化lr_scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计分类情况\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "        # 打印训练信息\n",
    "        loss_mean += loss.item()\n",
    "        train_curve.append(loss.item())\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_loader), loss_mean, correct / total))\n",
    "            loss_mean = 0.\n",
    "\n",
    "    scheduler.step()  # 更新学习率\n",
    "\n",
    "    # validate the model\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "\n",
    "        correct_val = 0.\n",
    "        total_val = 0.\n",
    "        loss_val = 0.\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(valid_loader):\n",
    "                inputs, labels = data\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "                loss_val += loss.item()\n",
    "\n",
    "            valid_curve.append(loss_val)\n",
    "            print(\"Valid:\\t Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, j+1, len(valid_loader), loss_val, correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition: syntax 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7032, -1.2390,  1.5754],\n",
       "        [-1.3122, -0.1154, -0.2655],\n",
       "        [ 1.1924,  1.6143,  0.1924],\n",
       "        [-0.5584, -1.9609,  3.2666],\n",
       "        [-0.1971,  1.6196, -0.4365]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition: syntax 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7032, -1.2390,  1.5754],\n",
       "        [-1.3122, -0.1154, -0.2655],\n",
       "        [ 1.1924,  1.6143,  0.1924],\n",
       "        [-0.5584, -1.9609,  3.2666],\n",
       "        [-0.1971,  1.6196, -0.4365]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition: providing an output tensor as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7032, -1.2390,  1.5754],\n",
       "        [-1.3122, -0.1154, -0.2655],\n",
       "        [ 1.1924,  1.6143,  0.1924],\n",
       "        [-0.5584, -1.9609,  3.2666],\n",
       "        [-0.1971,  1.6196, -0.4365]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)  # 这样的好处是节约内存\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition: in-place\n",
    "Any operation that mutates a tensor in-place is post-fixed with an `_`. For example: `x.copy_(y), x.t_()`, will change x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7032, -1.2390,  1.5754],\n",
       "        [-1.3122, -0.1154, -0.2655],\n",
       "        [ 1.1924,  1.6143,  0.1924],\n",
       "        [-0.5584, -1.9609,  3.2666],\n",
       "        [-0.1971,  1.6196, -0.4365]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以使用标准的与numty类似的索引来实现所有功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0605, -0.5069,  1.2580, -2.2096,  1.2931])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resizing: If you want to resize/reshape tensor, you can use torch.view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7351,  1.3273,  0.6753,  1.5732, -1.6490, -1.0935,  0.2658,  0.8934],\n",
       "        [ 0.3362, -0.4389,  0.5252, -1.4052,  1.5856,  2.4997, -0.8547,  1.3175]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a one element tensor, use .item() to get the value as a Python number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3911])\n",
      "-0.3911091685295105\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Bridge\n",
    "\n",
    "Converting a Torch Tensor to a NumPy array and vice versa is a breeze.\n",
    "\n",
    "The Torch Tensor and NumPy array will share their underlying memory locations (if the Torch Tensor is on CPU), and changing one will change the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting a Torch Tensor to a NumPy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting NumPy Array to Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensors\n",
    "ensors can be moved onto any device using the .to method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out:\n",
    "\n",
    "    tensor([-0.2550], device='cuda:0')\n",
    "    tensor([-0.2550], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd: Automatic Differentiation\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\n",
    "\n",
    "PyTorch中所有神经网络的核心是autograd软件包。 让我们先简要地介绍一下，然后再训练第一个神经网络。\n",
    "\n",
    "autograd软件包为Tensor上的所有操作提供自动区分。 这是一个按运行定义的框架，这意味着您的backprop是由代码的运行方式定义的，并且每次迭代都可以不同。\n",
    "\n",
    "让我们通过一些示例以更简单的方式看待这一点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "Torch.Tensor是程序包的中心类。 如果将其属性`.requires_grad`设置为True，它将开始跟踪对其的所有操作。 \n",
    "\n",
    "完成计算后，您可以调用`.backward（）`并自动计算所有gradients。 该张量的梯度将累积到.grad属性中。\n",
    "\n",
    "要停止张量跟踪历史记录，可以调用`.detach（）`将其与计算历史记录分离，并防止跟踪将来的计算。\n",
    "\n",
    "为了防止跟踪历史记录（和使用内存），您还可以使用`torch.no_grad（）`：包装代码块。 这在评估模型时特别有用，因为模型可能具有可训练的参数，且`require_grad=True`，但我们不需要gradients。\n",
    "\n",
    "There’s one more class which is very important for autograd implementation - a Function.\n",
    "\n",
    "Tensor and Function are interconnected and build up an acyclic graph(无环图), that encodes a complete history of computation. 每个张量都有一个.grad_fn属性，该属性引用创建了张量的函数（用户创建的张量除外-它们的grad_fn为None）。\n",
    "\n",
    "如果要计算导数，可以在Tensor上调用.backward（）。 如果Tensor是标量（即，它包含一个元素数据），则无需为Backward（）指定任何参数，但是，如果Tensor具有更多元素，则需要指定渐变参数，该参数是形状匹配的张量 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x11d8d82e8>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".requires_grad_( ... ) changes an existing Tensor’s requires_grad flag in-place. The input flag defaults to False if not given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x120f0cd30>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "Let’s backprop now. Because out contains a single scalar, out.backward() is equivalent to out.backward(torch.tensor(1.))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print gradients d(out)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(2., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(1.)\n",
    "x2 = torch.tensor(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = a * x1\n",
    "y2 = a * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "\n",
    "可以使用torch.nn包构建神经网络。\n",
    "\n",
    "现在您已经了解了autograd，nn依靠autograd定义模型并对其进行区分。 nn.Module包含图层以及返回输出的方法forward（input）。\n",
    "\n",
    "这是一个简单的前馈网络。 它获取输入，将其一层又一层地馈入，然后最终给出输出。\n",
    "\n",
    "神经网络的典型训练过程如下：\n",
    "\n",
    "定义具有一些可学习参数（或权重）的神经网络\n",
    "\n",
    "遍历输入数据集\n",
    "\n",
    "通过网络处理输入\n",
    "\n",
    "计算损失（输出正确的距离有多远）\n",
    "\n",
    "将渐变传播回网络参数\n",
    "\n",
    "通常使用简单的更新规则来更新网络的权重：权重=权重-learning_rate *梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function.\n",
    "\n",
    "The learnable parameters of a model are returned by net.parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.1999,  0.2530, -0.2831],\n",
       "          [-0.0439, -0.1528, -0.0559],\n",
       "          [ 0.0339,  0.3140,  0.0404]]],\n",
       "\n",
       "\n",
       "        [[[-0.2021, -0.2145, -0.2619],\n",
       "          [ 0.1274, -0.0834, -0.2485],\n",
       "          [ 0.0629,  0.1619,  0.0973]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0882, -0.2897,  0.3128],\n",
       "          [-0.1747,  0.2634,  0.1911],\n",
       "          [ 0.2923,  0.0829,  0.1996]]],\n",
       "\n",
       "\n",
       "        [[[-0.2798, -0.1275, -0.0258],\n",
       "          [-0.1741, -0.1414,  0.2522],\n",
       "          [-0.2951,  0.1101, -0.0925]]],\n",
       "\n",
       "\n",
       "        [[[-0.2216,  0.0712,  0.0358],\n",
       "          [-0.0395, -0.0431,  0.2562],\n",
       "          [ 0.1782,  0.1885,  0.1876]]],\n",
       "\n",
       "\n",
       "        [[[-0.0326, -0.3080,  0.0519],\n",
       "          [ 0.3257, -0.1819, -0.1857],\n",
       "          [ 0.2332, -0.3239, -0.0098]]]], requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们尝试一个32x32随机输入。 注意：该网络的预期输入大小（LeNet）为32x32。 要在MNIST数据集上使用此网络，请将图像从数据集中调整为32x32。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0713,  0.1089, -0.0452, -0.0585, -0.0935,  0.0098, -0.2002, -0.0618,\n",
      "         -0.0777,  0.0434]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将梯度缓冲区归零， 用随机梯度进行反向传播：\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0047, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x11de072b0>\n",
      "<AddmmBackward object at 0x11de07dd8>\n",
      "<AccumulateGrad object at 0x11de072b0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "\n",
    "To backpropagate the error all we have to do is to loss.backward(). You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
    "\n",
    "Now we shall call loss.backward(), and have a look at conv1’s bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0125,  0.0180, -0.0034, -0.0196,  0.0072,  0.0050])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，在使用神经网络时，您希望使用各种不同的更新规则，例如SGD，Nesterov-SGD，Adam，RMSProp等。为实现此目的，我们构建了一个小程序包：torch.optim，它实现了所有这些方法。 使用它非常简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Classifier\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
    "\n",
    "For this tutorial, we will use the CIFAR10 dataset. It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and normalizing CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a762cea7046c4ff5b6471c7729b9a02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cat   dog  frog  frog\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.172\n",
      "[1,  4000] loss: 1.842\n",
      "[1,  6000] loss: 1.678\n",
      "[1,  8000] loss: 1.596\n",
      "[1, 10000] loss: 1.516\n",
      "[1, 12000] loss: 1.474\n",
      "[2,  2000] loss: 1.389\n",
      "[2,  4000] loss: 1.377\n",
      "[2,  6000] loss: 1.371\n",
      "[2,  8000] loss: 1.330\n",
      "[2, 10000] loss: 1.315\n",
      "[2, 12000] loss: 1.308\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:    cat  ship  ship plane\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lib 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "https://pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "张量是一个多维数组，它是标量、向量、矩阵的高维拓展\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
    "## Tensor and Variable\n",
    "Variable是torch.autograd中的数据类型 主要用于封装Tensor，进行自动求导。在pytorch0.4.0以后将Variable并入Tensor\n",
    "\n",
    "    data: 被包装的Tensor\n",
    "    grad: data的梯度\n",
    "    grad_fn: 创建Tensor的Function，是自动 求导的关键\n",
    "    requires_grad: 指示是否需要梯度 \n",
    "    is_leaf: 指示是否是叶子结点(张量)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(6).view(-1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get its size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(1)  # 获取某个维度的dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gen Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• data: 数据, 可以是list, numpy\n",
    "• dtype : 数据类型，默认与data的一致 \n",
    "    • device : 所在设备, cuda/cpu\n",
    "• requires_grad:是否需要梯度\n",
    "• pin_memory:是否存于锁页内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_numpy(ndarray) -> Tensor\n",
    "从torch.from_numpy创建的tensor于原ndarray共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.array([1,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依据数值创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/TensorFactories.cpp:361: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5., 5., 5.],\n",
       "        [5., 5., 5.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2, 3), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  3.2500,  5.5000,  7.7500, 10.0000])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+01, 1.2328e+01, 1.5199e+01, 1.8738e+01, 2.3101e+01, 2.8480e+01,\n",
       "        3.5112e+01, 4.3288e+01, 5.3367e+01, 6.5793e+01, 8.1113e+01, 1.0000e+02,\n",
       "        1.2328e+02, 1.5199e+02, 1.8738e+02, 2.3101e+02, 2.8480e+02, 3.5112e+02,\n",
       "        4.3288e+02, 5.3367e+02, 6.5793e+02, 8.1113e+02, 1.0000e+03, 1.2328e+03,\n",
       "        1.5199e+03, 1.8738e+03, 2.3101e+03, 2.8480e+03, 3.5112e+03, 4.3288e+03,\n",
       "        5.3367e+03, 6.5793e+03, 8.1113e+03, 1.0000e+04, 1.2328e+04, 1.5199e+04,\n",
       "        1.8738e+04, 2.3101e+04, 2.8480e+04, 3.5112e+04, 4.3288e+04, 5.3367e+04,\n",
       "        6.5793e+04, 8.1113e+04, 1.0000e+05, 1.2328e+05, 1.5199e+05, 1.8738e+05,\n",
       "        2.3101e+05, 2.8480e+05, 3.5112e+05, 4.3288e+05, 5.3367e+05, 6.5793e+05,\n",
       "        8.1113e+05, 1.0000e+06, 1.2328e+06, 1.5199e+06, 1.8738e+06, 2.3101e+06,\n",
       "        2.8480e+06, 3.5112e+06, 4.3288e+06, 5.3367e+06, 6.5793e+06, 8.1113e+06,\n",
       "        1.0000e+07, 1.2328e+07, 1.5199e+07, 1.8738e+07, 2.3101e+07, 2.8480e+07,\n",
       "        3.5112e+07, 4.3288e+07, 5.3367e+07, 6.5793e+07, 8.1113e+07, 1.0000e+08,\n",
       "        1.2328e+08, 1.5199e+08, 1.8738e+08, 2.3101e+08, 2.8480e+08, 3.5112e+08,\n",
       "        4.3288e+08, 5.3367e+08, 6.5793e+08, 8.1113e+08, 1.0000e+09, 1.2328e+09,\n",
       "        1.5199e+09, 1.8738e+09, 2.3101e+09, 2.8480e+09, 3.5112e+09, 4.3288e+09,\n",
       "        5.3367e+09, 6.5793e+09, 8.1113e+09, 1.0000e+10])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建对数均分的1维张量\n",
    "torch.logspace(1, 10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct a randomly initialized matrix:\n",
    "声明了一个未初始化的矩阵，但在使用前不包含确定的已知值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.2870e+21, 5.2059e+22, 3.3091e+21],\n",
       "        [4.2931e-08, 1.6800e-07, 3.1369e+27],\n",
       "        [7.0800e+31, 3.1095e-18, 7.7052e+31],\n",
       "        [1.9447e+31, 2.0558e+32, 1.8755e+28],\n",
       "        [3.1093e-18, 2.9934e+32, 1.8528e+28]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依概率分布创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0179,  0.8916, -0.1481],\n",
       "        [ 1.3215, -2.5404,  1.0174]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正态分布\n",
    "torch.normal(mean=1, std=2, size=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0839, 0.7643, 0.4382],\n",
       "        [0.3033, 0.8193, 0.1281],\n",
       "        [0.8989, 0.9061, 0.7433],\n",
       "        [0.7744, 0.4719, 0.5584],\n",
       "        [0.0186, 0.3912, 0.1602]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 均匀分布\n",
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3886,  0.8938,  0.8417],\n",
       "        [-0.6967,  1.2211, -0.8665],\n",
       "        [ 0.4069,  0.8723,  0.5643],\n",
       "        [ 0.0948, -0.1088,  0.2056],\n",
       "        [-1.0987,  0.0246, -0.9158]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标准正态分布\n",
    "torch.randn([5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 8, 4, 7, 5, 2, 3, 1, 0, 9])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成生成从0到n-1的随机排列\n",
    "torch.randperm(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bernoulli(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-9bba74c58950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: bernoulli(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "torch.bernoulli(0.5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 10])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  2,  2],\n",
       "        [ 0,  0,  2],\n",
       "        [ 0,  8,  8],\n",
       "        [ 0, 10,  8]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a & b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = np.array([[0,10,10],[10,10,10],[7,8,9],[0,10,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  2],\n",
       "       [ 0,  0,  2],\n",
       "       [ 7,  8,  9],\n",
       "       [ 0, 10, 12]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa & bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  2],\n",
       "       [ 0,  0,  2],\n",
       "       [ 0,  8,  8],\n",
       "       [ 0, 10,  8]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa & bb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]]) \n",
      "\n",
      "tensor([[ 0, 10, 10],\n",
      "        [10, 10, 10],\n",
      "        [ 7,  8,  9],\n",
      "        [ 0, 10, 12]]) \n",
      "\n",
      "tensor([[ 0,  2,  2],\n",
      "        [ 0,  0,  2],\n",
      "        [ 7,  8,  9],\n",
      "        [ 0, 10, 12]]) \n",
      "\n",
      "tensor([[ 0,  2,  2],\n",
      "        [ 0,  0,  2],\n",
      "        [ 7,  8,  9],\n",
      "        [ 0, 10, 12]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.from_numpy(np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]]))\n",
    "print(a,'\\n')\n",
    "b = torch.from_numpy(np.array([[0,10,10],[10,10,10],[7,8,9],[0,10,12]]))\n",
    "print(b,'\\n')\n",
    "print(a & b,'\\n')\n",
    "print(b & a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 强制类型转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type_as(tensor) → Tensor\n",
    "\n",
    "    Returns this tensor cast to the type of the given tensor.\n",
    "\n",
    "    This is a no-op if the tensor is already of the correct type. This is equivalent to self.type(tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量拼接与切分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 0, 1, 2],\n",
       "        [3, 4, 5, 3, 4, 5]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, a], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2],\n",
       "         [3, 4, 5]],\n",
       "\n",
       "        [[0, 1, 2],\n",
       "         [3, 4, 5]]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2],\n",
       "         [0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5],\n",
       "         [3, 4, 5]]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a, a], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.chunk()\n",
    "功能:将张量按维度dim进行平均切分\n",
    "返回值:张量列表\n",
    "注意事项:若不能整除，最后一份张量小于 其他张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [3]]), tensor([[1],\n",
       "         [4]]), tensor([[2],\n",
       "         [5]]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(a, 3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [3, 4]]), tensor([[2],\n",
       "         [5]]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(a, 2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.split(tensor, split_size_or_sections, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2]]), tensor([[3, 4, 5]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2]]), tensor([[3, 4, 5]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [3]]), tensor([[1],\n",
       "         [4]]), tensor([[2],\n",
       "         [5]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split(1, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### index_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4, 5]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.index_select(0, torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.index_select(0, torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2],\n",
       "        [3, 5]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(a, 1, torch.tensor([0,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.masked_select()\n",
    "功能:按mask中的True进行索引\n",
    "\n",
    "返回值:一维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True, False,  True]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randint(0, 9, size=(3, 3))\n",
    "mask = t.le(5)  # le: <=\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 1, 3, 4, 1, 0])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.masked_select(t, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.where(\n",
    "    condition,  # condition (BoolTensor): When True (nonzero), yield x, otherwise yield y\n",
    "    x, \n",
    "    y\n",
    ") -> Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.reshape()\n",
    "功能:变换张量形状\n",
    "\n",
    "注意事项:当张量在内存中是连续时，新张 量与input共享数据内存\n",
    "\n",
    "• input: 要变换的张量 \n",
    "• shape: 新张量的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:tensor([1, 2, 3, 5, 7, 4, 6, 0])\n",
      "t_reshape:\n",
      "tensor([[[1, 2],\n",
      "         [3, 5]],\n",
      "\n",
      "        [[7, 4],\n",
      "         [6, 0]]])\n",
      "t:tensor([1024,    2,    3,    5,    7,    4,    6,    0])\n",
      "t_reshape:\n",
      "tensor([[[1024,    2],\n",
      "         [   3,    5]],\n",
      "\n",
      "        [[   7,    4],\n",
      "         [   6,    0]]])\n",
      "t.data 内存地址:6481941704\n",
      "t_reshape.data 内存地址:6481941704\n"
     ]
    }
   ],
   "source": [
    "t = torch.randperm(8)\n",
    "t_reshape = torch.reshape(t, (-1, 2, 2))    # -1\n",
    "print(\"t:{}\\nt_reshape:\\n{}\".format(t, t_reshape))\n",
    "\n",
    "t[0] = 1024\n",
    "print(\"t:{}\\nt_reshape:\\n{}\".format(t, t_reshape))\n",
    "print(\"t.data 内存地址:{}\".format(id(t.data)))\n",
    "print(\"t_reshape.data 内存地址:{}\".format(id(t_reshape.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.transpose()\n",
    "功能:交换张量的两个维度 \n",
    "\n",
    "• input: 要变换的张量\n",
    "• dim0: 要交换的维度\n",
    "• dim1: 要交换的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t shape:torch.Size([2, 3, 4])\n",
      "t_transpose shape: torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand((2, 3, 4))\n",
    "t_transpose = torch.transpose(t, dim0=1, dim1=2)    # c*h*w     h*w*c\n",
    "print(\"t shape:{}\\nt_transpose shape: {}\".format(t.shape, t_transpose.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5767, 0.6817, 0.6154, 0.1999],\n",
       "         [0.4417, 0.4720, 0.9115, 0.0849],\n",
       "         [0.0277, 0.7797, 0.1309, 0.2334]],\n",
       "\n",
       "        [[0.8531, 0.4334, 0.9189, 0.6155],\n",
       "         [0.1755, 0.4395, 0.0836, 0.7417],\n",
       "         [0.6370, 0.1990, 0.6498, 0.9281]]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5767, 0.4417, 0.0277],\n",
       "         [0.6817, 0.4720, 0.7797],\n",
       "         [0.6154, 0.9115, 0.1309],\n",
       "         [0.1999, 0.0849, 0.2334]],\n",
       "\n",
       "        [[0.8531, 0.1755, 0.6370],\n",
       "         [0.4334, 0.4395, 0.1990],\n",
       "         [0.9189, 0.0836, 0.6498],\n",
       "         [0.6155, 0.7417, 0.9281]]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.t() \n",
    "功能:2维张量转置，对矩阵而言，等价于 torch.transpose(input, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.squeeze()\n",
    "功能:压缩长度为1的维度(轴)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1954],\n",
       "          [0.2219],\n",
       "          [0.4094]],\n",
       "\n",
       "         [[0.2999],\n",
       "          [0.9619],\n",
       "          [0.3831]]]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand((1, 2, 3, 1))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1954, 0.2219, 0.4094],\n",
       "        [0.2999, 0.9619, 0.3831]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_sq = torch.squeeze(t)\n",
    "t_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1954],\n",
       "         [0.2219],\n",
       "         [0.4094]],\n",
       "\n",
       "        [[0.2999],\n",
       "         [0.9619],\n",
       "         [0.3831]]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_0 = torch.squeeze(t, dim=0)\n",
    "t_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1954],\n",
       "          [0.2219],\n",
       "          [0.4094]],\n",
       "\n",
       "         [[0.2999],\n",
       "          [0.9619],\n",
       "          [0.3831]]]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1 = torch.squeeze(t, dim=1)\n",
    "t_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 1])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3, 1])\n",
      "torch.Size([1, 2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t_sq.shape)\n",
    "print(t_0.shape)\n",
    "print(t_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.unsqueeze() \n",
    "功能:依据dim扩展维度 • dim: 扩展的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1954],\n",
       "          [0.2219],\n",
       "          [0.4094]],\n",
       "\n",
       "         [[0.2999],\n",
       "          [0.9619],\n",
       "          [0.3831]]]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_0.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量数学运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4198, 0.2774, 0.0937],\n",
       "        [0.6961, 0.0774, 0.7155]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000],\n",
       "        [0.7000, 0.6799, 0.7000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(a, 0.5, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gather\n",
    "torch.gather 函数用于从参数 t 选择性输出特定 index 的矩阵，输出矩阵的大小跟 index 的大小是一样的，torch.gather 的 dim 参数用来选择 index 作用的 axis。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gather(input, dim, index)\n",
    "\n",
    "index的维度要和input相同\n",
    "\n",
    "dim表示index要选择的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用场景, 自己实现cross entropyloss，或focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经过softmax的输出值\n",
    "preds = torch.tensor([[0.4996, 0.5004],\n",
    "        [0.4944, 0.5056],\n",
    "        [0.5954, 0.4046],\n",
    "        [0.5711, 0.4289],\n",
    "        [0.6190, 0.3810],\n",
    "        [0.5797, 0.4203],\n",
    "        [0.5441, 0.4559]])\n",
    "# groud truth\n",
    "labels = torch.tensor([0, 0, 1, 1, 0, 1, 0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4996],\n",
       "        [0.4944],\n",
       "        [0.4046],\n",
       "        [0.4289],\n",
       "        [0.6190],\n",
       "        [0.4203],\n",
       "        [0.5441]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 需要把相应的类别的y_pred选出来\n",
    "y_pred = torch.gather(preds, 1, labels.unsqueeze(1))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6939],\n",
       "        [0.7044],\n",
       "        [0.9049],\n",
       "        [0.8465],\n",
       "        [0.4796],\n",
       "        [0.8668],\n",
       "        [0.6086]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss = -torch.log(y_pred)\n",
    "cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7293])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取均值\n",
    "cross_entropy_loss.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.4198, 0.7155]),\n",
       "indices=tensor([0, 2]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, e = a.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4198, 0.7155])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2637, 0.4963])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4198, 0.2774, 0.0937],\n",
       "        [0.6961, 0.0774, 0.7155]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### masked_fill_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_mask = torch.tensor([\n",
    "    [1, 1, 0],\n",
    "    [1, 0, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True],\n",
       "        [False,  True,  True]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_mask==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.1978e-01, 2.7744e-01, 1.0000e-09],\n",
       "        [6.9611e-01, 1.0000e-09, 1.0000e-09]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.masked_fill_(a_mask==0, 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### masked_select\n",
    "Returns a new 1-D tensor which indexes the :attr:`input` tensor according to\n",
    "the boolean mask :attr:`mask` which is a `BoolTensor`.\n",
    "\n",
    "The shapes of the :attr:`mask` tensor and the :attr:`input` tensor don't need\n",
    "to match, but they must be :ref:`broadcastable <broadcasting-semantics>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4198, 0.2774, 0.6961])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.masked_select(a, a_mask!=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.bmm(input, mat2, out=None) → Tensor\n",
    "\n",
    "    Performs a batch matrix-matrix product of matrices stored in input and mat2.\n",
    "\n",
    "    input and mat2 must be 3-D tensors each containing the same number of matrices.\n",
    "\n",
    "    If input is a (b×n×m)(b \\times n \\times m)(b×n×m) tensor, mat2 is a (b×m×p)(b \\times m \\times p)(b×m×p) tensor, out will be a (b×n×p)(b \\times n \\times p)(b×n×p) tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.6, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8000, 2.4000])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add(a, alpha=2)  # a + a*alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2364, 0.4633, 0.3512],\n",
       "        [0.2279, 0.2244, 0.4190]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2364, 0.4633, 0.3512],\n",
       "        [0.2279, 0.2244, 0.4190]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(a, b).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0802, 0.5049, 0.9776],\n",
       "        [0.9938, 0.2298, 0.2362]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(2, 3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5037)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5209, 0.4866])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5209],\n",
       "        [0.4866]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor.contiguous()\n",
    "https://blog.csdn.net/gdymind/article/details/82662502\n",
    "\n",
    "在pytorch中，只有很少几个操作是不改变tensor的内容本身，而只是重新定义下标与元素的对应关系的。换句话说，这种操作不进行数据拷贝和数据的改变，变的是元数据。\n",
    "\n",
    "这些操作是：\n",
    "\n",
    "    narrow()，view()，expand()和transpose()\n",
    "\n",
    "举个栗子，在使用transpose()进行转置操作时，pytorch并不会创建新的、转置后的tensor，而是修改了tensor中的一些属性（也就是元数据），使得此时的offset和stride是与转置tensor相对应的。转置的tensor和原tensor的内存是共享的！\n",
    "\n",
    "也就是说，经过上述操作后得到的tensor，它内部数据的布局方式和从头开始创建一个这样的常规的tensor的布局方式是不一样的！于是…这就有contiguous()的用武之地了。\n",
    "\n",
    "在上面的例子中，x是contiguous的，但y不是（因为内部数据不是通常的布局方式）。注意不要被contiguous的字面意思“连续的”误解，tensor中数据还是在内存中一块区域里，只是布局的问题！\n",
    "\n",
    "当调用contiguous()时，会强制拷贝一份tensor，让它的布局和从头创建的一毛一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(6).reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5615395320"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5614952664"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0, 0] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100,   1,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.transpose(0, 1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "c [0, 0] = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100,   1,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.matmul(input, other, out=None) → Tensor\n",
    "\n",
    "    Matrix product of two tensors.\n",
    "\n",
    "    The behavior depends on the dimensionality of the tensors as follows:\n",
    "\n",
    "        If both tensors are 1-dimensional, the dot product (scalar) is returned.\n",
    "\n",
    "        If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
    "\n",
    "        If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.\n",
    "\n",
    "        If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.\n",
    "\n",
    "        If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a batched matrix multiply is returned. \n",
    "        If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. \n",
    "        If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). \n",
    "        For example, if input is a (j×1×n×m)(j \\times 1 \\times n \\times m)(j×1×n×m) tensor and other is a (k×m×p)(k \\times m \\times p)(k×m×p) tensor, out will be an (j×k×n×p)(j \\times k \\times n \\times p)(j×k×n×p) tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine_similarity\n",
    "$$\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [\n",
    "    torch.tensor([[1., 2., 1.]]),\n",
    "    torch.tensor([[1., 0., 1.]]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.tensor([\n",
    "    [1., 2., 1.],\n",
    "    [1., 0., 1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 1.],\n",
       "       [1., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = embeddings.numpy()\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57735026"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[0].dot(arr[1])/np.sqrt(arr[0].dot(arr[0])*arr[1].dot(arr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5774])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(embeddings[0].view(1, -1), embeddings[1].view(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchvision:计算机视觉工具包\n",
    "* torchvision.transforms : 常用的图像预处理方法\n",
    "* torchvision.datasets : 常用数据集的dataset实现，MNIST，CIFAR-10，ImageNet等\n",
    "* torchvision.model : 常用的模型预训练，AlexNet，VGG， ResNet，GoogLeNet等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchvision.transforms : 常用的图像预处理方法 \n",
    "    • 数据中心化\n",
    "    • 数据标准化\n",
    "    • 缩放\n",
    "    • 裁剪\n",
    "    • 旋转\n",
    "    • 翻转\n",
    "    • 填充\n",
    "    • 噪声添加\n",
    "    • 灰度变换\n",
    "    • 线性变换\n",
    "    • 仿射变换\n",
    "    • 亮度、饱和度及对比度变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforms.Normalize\n",
    "功能:逐channel的对图像进行标准化 output = (input - mean) / std\n",
    "\n",
    "    • mean:各通道的均值\n",
    "    • std:各通道的标准差\n",
    "    • inplace:是否原地操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchvision.models\n",
    "载入预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.resnet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.utils.data.DataLoader \n",
    "功能:构建可迭代的数据装载器\n",
    "\n",
    "主要解决以下几个问题:\n",
    "1. 读哪些数据?\n",
    "Sampler输出的Index\n",
    "2. 从哪读数据?\n",
    "Dataset中的data_dir\n",
    "3. 怎么读数据?\n",
    "Dataset中的getitem\n",
    "\n",
    "\n",
    "    • dataset: Dataset类，决定数据从哪读取 及如何读取\n",
    "    • batchsize : 批大小\n",
    "    • num_works: 是否多进程读取数据\n",
    "    • shuffle: 每个epoch是否乱序\n",
    "    • drop_last:当样本数不能被batchsize整 除时，是否舍弃最后一批数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.utils.data.Dataset\n",
    "功能:Dataset抽象类，所有自定义的 Dataset需要继承它，并且复写 `__getitem__()`\n",
    "\n",
    "getitem : 接收一个索引，返回一个样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn & torch.nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn各个功能模块\n",
    "* nn.Parameter：张量子类，表示可学习参数，如weight, bias\n",
    "* nn.Module：所有网络层基类，管理网络属性\n",
    "* nn.functional：函数具体实现，如卷积，池化，激活函数等\n",
    "* nn.init：参数初始化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型创建步骤\n",
    "* 数据准备\n",
    "* 模型构建\n",
    "* 损失函数\n",
    "* 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建两要素\n",
    "### 构建子模块: `__init__()`\n",
    "就是把该Module有的layers有塞进来，不必考虑顺序\n",
    "### 拼接子模块: `forward()`\n",
    "通过 连接 input output 运算等 把`__init__()`中的layers构成一个有机的整体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module\n",
    "    • 一个module可以包含多个子module\n",
    "    • 一个module相当于一个运算，必须实现forward()函数 \n",
    "    • 每个module都有8个字典管理它的属性\n",
    "    \n",
    "![](img/nnm01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using Sequential\n",
    "model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取模型内部的子module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of Sequential(\n",
       "  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (3): ReLU()\n",
       ")>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取模型内部的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 5, 5])\n",
      "torch.Size([20])\n",
      "torch.Size([64, 20, 5, 5])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 容器类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Sequential\n",
    "按顺序包装多个网络层\n",
    "\n",
    "    • 顺序性:各网络层之间严格按照顺序构建\n",
    "    • 自带forward():自带的forward里，通过for循环依次执行前向传播运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using Sequential\n",
    "model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "# Example of using Sequential with OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-af9c1f5e508d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m_get_item_by_idx\u001b[0;34m(self, iterator, idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;34m\"\"\"Get the idx-th item of the iterator\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index {} is out of range'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "model['conv1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.ModuleList\n",
    "像python的list一样包装多个网络层，以迭代方式调用网络层 \n",
    "\n",
    "主要方法:\n",
    "\n",
    "    • append():在ModuleList后面添加网络层\n",
    "    • extend():拼接两个ModuleList\n",
    "    • insert():指定在ModuleList中位置插入网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = self.linears[i // 2](x) + l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.ModuleDict\n",
    "像python的dict一样包装多个网络层, ModuleDict is an ordered dictionary that respects. 以索引方式调用网络层 \n",
    "\n",
    "主要方法:\n",
    "\n",
    "    • clear():清空ModuleDict\n",
    "    • items():返回可迭代的键值对(key-value pairs)\n",
    "    • keys():返回字典的键(key)\n",
    "    • values():返回字典的值(value)\n",
    "    • pop():返回一对键值，并从字典中删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.choices = nn.ModuleDict({\n",
    "                'conv': nn.Conv2d(10, 10, 3),\n",
    "                'pool': nn.MaxPool2d(3)\n",
    "        })\n",
    "        self.activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['prelu', nn.PReLU()]\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, choice, act):\n",
    "        x = self.choices[choice](x)\n",
    "        x = self.activations[act](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 容器总结\n",
    "* nn.Sequential:顺序性，各网络层之间严格按顺序执行，常用于block构建\n",
    "* nn.ModuleList:迭代性，常用于大量重复网构建，通过for循环实现重复构建 \n",
    "* nn.ModuleDict:索引性，常用于可选择的网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相关类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Parameter\n",
    "Parameters are Tensor subclasses\n",
    "\n",
    "通过data属性获取到 parameter tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(20, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 20])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight.shape  # equal m._parameters['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m._parameters['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 权值初始化\n",
    "initrange = 0.5\n",
    "m.weight.data.uniform_(-initrange, initrange)\n",
    "m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 30])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(128, 20)\n",
    "output = m(x)  # forward\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.LSTM\n",
    "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n",
    "\n",
    "Inputs: input, (h_0, c_0)\n",
    "\n",
    "If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.\n",
    "\n",
    "Outputs: output, (h_n, c_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 1)  # input_size, hidden_size, num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = torch.randn(5, 3, 10)  #  seq_len, batch, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "h0 = torch.randn(1, 3, 20)  # \n",
    "c0 = torch.randn(1, 3, 20)\n",
    "output, (hn, cn) = rnn(input1, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 20])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape  # 所有seq element的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 20])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1244, -0.0423, -0.0715,  0.0993,  0.0076, -0.0306, -0.2079, -0.1731,\n",
       "         -0.0650, -0.1120, -0.1716,  0.0862,  0.1526,  0.0210,  0.2429, -0.3801,\n",
       "          0.0815,  0.3206,  0.0126,  0.0161],\n",
       "        [ 0.0132, -0.0114, -0.1110,  0.1884, -0.1763,  0.1574, -0.1779, -0.0043,\n",
       "          0.0380, -0.0192, -0.1365,  0.0278,  0.1410,  0.0814,  0.1697,  0.0294,\n",
       "          0.0606,  0.1546,  0.0837, -0.0532],\n",
       "        [-0.0419, -0.0079, -0.1726,  0.0810, -0.1047, -0.1129, -0.0092, -0.0764,\n",
       "         -0.0376, -0.0481, -0.1735,  0.0239,  0.0308, -0.0315,  0.0165, -0.0323,\n",
       "         -0.0621, -0.0889,  0.0618,  0.0536]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1244, -0.0423, -0.0715,  0.0993,  0.0076, -0.0306, -0.2079,\n",
       "          -0.1731, -0.0650, -0.1120, -0.1716,  0.0862,  0.1526,  0.0210,\n",
       "           0.2429, -0.3801,  0.0815,  0.3206,  0.0126,  0.0161],\n",
       "         [ 0.0132, -0.0114, -0.1110,  0.1884, -0.1763,  0.1574, -0.1779,\n",
       "          -0.0043,  0.0380, -0.0192, -0.1365,  0.0278,  0.1410,  0.0814,\n",
       "           0.1697,  0.0294,  0.0606,  0.1546,  0.0837, -0.0532],\n",
       "         [-0.0419, -0.0079, -0.1726,  0.0810, -0.1047, -0.1129, -0.0092,\n",
       "          -0.0764, -0.0376, -0.0481, -0.1735,  0.0239,  0.0308, -0.0315,\n",
       "           0.0165, -0.0323, -0.0621, -0.0889,  0.0618,  0.0536]]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn  # 最后一个seq element的输出，equal to output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "birnn = nn.LSTM(10, 20, 1, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (hn, cn) = birnn(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 40])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层\n",
    "卷积维度: 一般情况下，卷积核在几个维度上滑动，就是几维卷积。\n",
    "![](img/conv01.png)\n",
    "padding\n",
    "![](img/conv02.png)\n",
    "孔洞卷积\n",
    "![](img/conv03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积尺寸计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 简化版\n",
    "$$out_{size}=\\frac{in_{size}-kernel_{size}}{stride}+1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 完整版\n",
    "$$out_{size}=\\frac{in_{size}+2*padding-dilation*(kernel_{size}-1)-1}{stride}+1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Conv2d(\n",
    "    in_channels,\n",
    "    out_channels,  # Number of channels produced by the convolution。等于卷积核的个数\n",
    "    kernel_size,\n",
    "    stride=1,\n",
    "    padding=0,  # Zero-padding added to both sides of the input. Default: 0。填充个数\n",
    "    dilation=1,  # 孔洞卷积大小。常用于图像分割任务，主要作用是提升感受野\n",
    "    groups=1,  # 分组卷积设置。主要用于模型轻量化。\n",
    "    bias=True,\n",
    "    padding_mode='zeros',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转置卷积 Transpose Convolution\n",
    "转置卷积又称为反卷积(Deconvolution)和部分跨越卷积(Fractionally- strided Convolution) ，用于对图像进行上采样(UpSample)\n",
    "\n",
    "为什么称为转置卷积? 假设图像尺寸为4\\*4，卷积核为3\\*3，padding=0，stride=1 \n",
    "\n",
    "#### 正常卷积:\n",
    "\n",
    "图像:$I_{𝟏𝟔∗𝟏}$ 卷积核: $K_{𝟒∗𝟏𝟔}$ 输出:$O_{𝟒∗𝟏} = K_{𝟒∗𝟏𝟔} ∗ I_{𝟏𝟔∗𝟏}$\n",
    "#### 转置卷积:\n",
    "假设图像尺寸为2\\*2，卷积核为3\\*3，padding=0，stride=1 \n",
    "\n",
    "图像:$I_{4∗𝟏}$ 卷积核: $K_{16∗4}$ 输出:$O_{16∗𝟏} = K_{16∗4} ∗ I_{4∗𝟏}$\n",
    "\n",
    "![](img/conv05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 池化层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.MaxPool1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool of size=3, stride=2\n",
    "m = nn.MaxPool1d(3, stride=3)\n",
    "input1 = torch.randn(20, 16, 50)  # batch, channel, Lin\n",
    "output = m(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 16])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MaxPool2d(\n",
    "    kernel_size,\n",
    "    stride=None,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    return_indices=False,\n",
    "    ceil_mode=False,  # when True, will use `ceil` instead of `floor` to compute the output shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AvgPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.AvgPool2d(\n",
    "    kernel_size,\n",
    "    stride=None,\n",
    "    padding=0,\n",
    "    ceil_mode=False,\n",
    "    count_include_pad=True,  # 计算均值时是否考虑填充值\n",
    "    divisor_override=None,  # if specified, it will be used as divisor, otherwise attr:`kernel_size` will be used\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxUnpool2d\n",
    "功能:对二维信号(图像)进行最大值池化 上采样\n",
    "![](img/conv06.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PositionalEncoding\n",
    "### 无可学习参数的PositionEncoding层\n",
    "\n",
    "无参数的PositionEncoding计算速度快，还可以减小整个模型的尺寸，据说在有些任务中，效果与有参数的接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model,dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有可学习参数的PositionEncoding层\n",
    "\n",
    "我曾在一个序列预测任务（非NLP）里面对比过两种PositionEncoding层，发现带有参数的PositionEncoding层效果明显比没有参数的PositionEncoding要好。\n",
    "\n",
    "带参数的PositionEncoding层的定义更为简单，直接继承一个nn.Embedding，再续上一个dropout就可以了。因为nn.Embedding中包含了一个可以按索引取向量的权重矩阵weight。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionEncoding(nn.Embedding):\n",
    "    def __init__(self,d_model, dropout = 0.1,max_len = 5000):\n",
    "        super().__init__(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        weight = self.weight.data.unsqueeze(1)\n",
    "        x = x + weight[:x.size(0),:]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Transformer(\n",
    "    d_model=512, \n",
    "    nhead=8, \n",
    "    num_encoder_layers=6, \n",
    "    num_decoder_layers=6, \n",
    "    dim_feedforward=2048, \n",
    "    dropout=0.1, activation='relu', custom_encoder=None, custom_decoder=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(\n",
    "    src: torch.Tensor, \n",
    "    tgt: torch.Tensor, \n",
    "    src_mask: Optional[torch.Tensor] = None, \n",
    "    tgt_mask: Optional[torch.Tensor] = None, \n",
    "    memory_mask: Optional[torch.Tensor] = None, \n",
    "    src_key_padding_mask: Optional[torch.Tensor] = None, \n",
    "    tgt_key_padding_mask: Optional[torch.Tensor] = None, \n",
    "    memory_key_padding_mask: Optional[torch.Tensor] = None) → torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer中有两种类型的mask\n",
    "https://luozhouyang.github.io/transformer/\n",
    "\n",
    "#### Padding mask\n",
    "什么是padding mask呢？回想一下，我们的每个批次输入序列长度是不一样的！也就是说，我们要对输入序列进行对齐！具体来说，就是给在较短的序列后面填充0。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。\n",
    "\n",
    "\n",
    "src_key_padding_mask: (N,S) # batch_size, seq_len\n",
    "\n",
    "tgt_key_padding_mask: (N,T) # batch_size, seq_len\n",
    "\n",
    "memory_key_padding_mask: (N,S) # batch_size, seq_len\n",
    "\n",
    "\n",
    "[src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. \n",
    "* If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. \n",
    "* If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged.\n",
    "\n",
    "#### Sequence mask\n",
    "文章前面也提到，sequence mask是为了使得decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。\n",
    "\n",
    "那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为1，下三角的值权威0，对角线也是0。把这个矩阵作用在每一个序列上，就可以达到我们的目的啦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))  # seq_len, batch, embeding_size\n",
    "tgt = torch.rand((20, 32, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model.generate_square_subsequent_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = transformer_model(src, tgt)\n",
    "\n",
    "out.shape  # equal to tgt.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerEncoderLayer\n",
    "TransformerEncoderLayer is made up of self-attn and feedforward network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerEncoder\n",
    "TransformerEncoder is a stack of N encoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = transformer_encoder(src)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward,dropout)\n",
    "encoder_norm = nn.LayerNorm(d_model)\n",
    "self.encoder = nn.TransformerEncoder(encoder_layer,num_encoder_layers,encoder_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence2sequence模型\n",
    "https://www.jianshu.com/p/23b527e0f696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2sTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,position_enc,d_model = 512,nhead = 8,num_encoder_layers=6,\n",
    "                 num_decoder_layers=6,dim_feedforward=2048,dropout=0.1):\n",
    "        super(S2sTransformer,self).__init__()\n",
    "\n",
    "        # Preprocess\n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "        self.pos_encoder_src = position_enc(d_model=512)\n",
    "        # tgt\n",
    "        self.pos_encoder_tgt = position_enc(d_model=512)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward,dropout)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer,num_encoder_layers,encoder_norm)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model,nhead,dim_feedforward,dropout)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer,num_decoder_layers,decoder_norm)\n",
    "        self.output_layer = nn.Linear(d_model,vocab_size)\n",
    "\n",
    "        self._reset_parameters()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "\n",
    "    def forward(self, src,tgt,src_mask = None,tgt_mask = None,\n",
    "                memory_mask = None,src_key_padding_mask = None,\n",
    "                tgt_key_padding_mask = None,memory_key_padding_mask = None):\n",
    "\n",
    "        # word embedding\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "\n",
    "        # shape check\n",
    "        if src.size(1) != tgt.size(1):\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "        if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n",
    "            raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
    "\n",
    "        # position encoding\n",
    "        src = self.pos_encoder_src(src)\n",
    "        tgt = self.pos_encoder_tgt(tgt)\n",
    "\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        output = self.output_layer(output)\n",
    "        # return output\n",
    "        return softmax(output,dim = 2)\n",
    "\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.LayerNorm(\n",
    "    normalized_shape: Union[int, List[int], torch.Size], \n",
    "    eps: float = 1e-05, \n",
    "    elementwise_affine: bool = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> input = torch.randn(20, 5, 10, 10)\n",
    ">>> # With Learnable Parameters\n",
    ">>> m = nn.LayerNorm(input.size()[1:])\n",
    ">>> # Without Learnable Parameters\n",
    ">>> m = nn.LayerNorm(input.size()[1:], elementwise_affine=False)\n",
    ">>> # Normalize over last two dimensions\n",
    ">>> m = nn.LayerNorm([10, 10])\n",
    ">>> # Normalize over last dimension of size 10\n",
    ">>> m = nn.LayerNorm(10)\n",
    ">>> # Activating the module\n",
    ">>> output = m(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sigmoid\n",
    "$$y=\\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.tanh\n",
    "$$y=\\frac{\\sin x}{\\cos x}=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.LeakyReLU\n",
    "negative_slope: 负半轴斜率(固定，很小如0.01\n",
    "### nn.PReLU\n",
    "init: 可学习斜率\n",
    "### nn.RReLU(random，负半轴斜率每次都从均匀分布采样\n",
    "• lower: 均匀分布下限\n",
    "\n",
    "• upper:均匀分布上限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权值初始化\n",
    "假设当前层的输入和权值都服从方差为1，均值为0的分布，假设当前层的神经元个数为N。\n",
    "\n",
    "那么当前层的输出的方差会是输入方差的N倍。\n",
    "\n",
    "如果层数很深，则会随着前向传播而造成尺度爆炸，无法完成前向传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier初始化\n",
    "方差一致性:保持数据尺度维持在恰当范围，通常方差为1 \n",
    "\n",
    "激活函数:饱和函数，如Sigmoid，Tanh\n",
    "\n",
    "$$W\\sim U[-\\frac{\\sqrt 6}{\\sqrt{n_i, n_{i+1}}}, \\frac{\\sqrt 6}{\\sqrt{n_i, n_{i+1}}}]$$\n",
    "## Kaiming初始化\n",
    "激活函数:ReLU及其变种\n",
    "\n",
    "$$std(W)=\\sqrt{\\frac{2}{(1+a^2)*n_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 十种初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Xavier均匀分布 2. Xavier正态分布 3. Kaiming均匀分布 4. Kaiming正态分布 5. 均匀分布\n",
    "6. 正态分布\n",
    "7. 常数分布\n",
    "8. 正交矩阵初始化 9. 单位矩阵初始化 10. 稀疏矩阵初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.init.calculate_gain\n",
    "主要功能:计算激活函数的方差变化尺度(输入数据的标准差/输出数据的标准差)\n",
    "\n",
    "主要参数\n",
    "\n",
    "• nonlinearity: 激活函数名称\n",
    "\n",
    "• param: 激活函数的参数，如Leaky ReLU 的negative_slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6666666666666667"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.calculate_gain('tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.calculate_gain('relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        # nn.init.uniform_(self.fc.weight.data, -initrange, initrange)\n",
    "        # nn.init.xavier_normal_(self.fc.weight.data)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.optim\n",
    "https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "pytorch的优化器:管理并更新模型中可学习参数的值，使得模型输出更接近 真实标签\n",
    "\n",
    "    导数:函数在指定坐标轴上的变化率 \n",
    "    方向导数:指定方向上的变化率 \n",
    "    梯度:一个向量，方向为方向导数 取得最大值的方向\n",
    "    \n",
    "## 基本属性\n",
    "    • defaults:优化器超参数\n",
    "    • state:参数的缓存，如momentum的缓存\n",
    "    • params_groups:管理的参数组\n",
    "    • _step_count:记录更新次数，学习率调整中使用\n",
    "\n",
    "## 基本方法\n",
    "    • zero_grad():清空所管理参数的梯度\n",
    "    • step():执行一步更新\n",
    "    • add_param_group():添加参数组\n",
    "    • state_dict():获取优化器当前状态信息字典 \n",
    "    • load_state_dict() :加载状态信息字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. optim.SGD:随机梯度下降法\n",
    "2. optim.Adagrad:自适应学习率梯度下降法 \n",
    "3. optim.RMSprop: Adagrad的改进  \n",
    "http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "4. optim.Adadelta: Adagrad的改进\n",
    "5. optim.Adam:RMSprop结合Momentum   \n",
    "《Adam: A Method for Stochastic Optimization》\n",
    "6. optim.Adamax:Adam增加学习率上限\n",
    "7. optim.SparseAdam:稀疏版的Adam\n",
    "8. optim.ASGD:随机平均梯度下降\n",
    "9. optim.Rprop:弹性反向传播\n",
    "10. optim.LBFGS:BFGS的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习率调整\n",
    "## class _LRScheduler \n",
    "\n",
    "主要属性:\n",
    "\n",
    "    • optimizer:关联的优化器 \n",
    "    • last_epoch:记录epoch数 \n",
    "    • base_lrs:记录初始学习率\n",
    "    \n",
    "## pytorch提供的lr调整策略\n",
    "### StepLR \n",
    "功能:等间隔调整学习率 \n",
    "\n",
    "主要参数:\n",
    "\n",
    "    • step_size:调整间隔数 \n",
    "    • gamma:调整系数\n",
    "\n",
    "调整方式:lr = lr * gamma\n",
    "\n",
    "### MultiStepLR 功能:按给定间隔调整学习率 主要参数:\n",
    "• milestones:设定调整时刻数 \n",
    "\n",
    "• gamma:调整系数\n",
    "\n",
    "调整方式:lr = lr * gamma\n",
    "\n",
    "### ExponentialLR 功能:按指数衰减调整学习率 主要参数:\n",
    "• gamma:指数的底\n",
    "调整方式:lr = lr * gamma ** epoch\n",
    "\n",
    "### CosineAnnealingLR 功能:余弦周期调整学习率 主要参数:\n",
    "• T_max:下降周期\n",
    "\n",
    "• eta_min:学习率下限\n",
    "\n",
    "### ReduceLRonPlateau\n",
    "功能:监控指标，当指标不再变化则调整\n",
    "主要参数:\n",
    "• mode:min/max 两种模式\n",
    "• factor:调整系数\n",
    "• patience:“耐心”，接受几次不变化\n",
    "• cooldown:“冷却时间”，停止监控一段时间 • verbose:是否打印日志\n",
    "• min_lr:学习率下限\n",
    "• eps:学习率衰减最小值\n",
    "\n",
    "### LambdaLR 功能:自定义调整策略 主要参数:\n",
    "• lr_lambda:function or list\n",
    "\n",
    "学习率调整小结\n",
    "1. 有序调整:Step、MultiStep、Exponential 和 CosineAnnealing\n",
    "2. 自适应调整:ReduceLROnPleateau \n",
    "3. 自定义调整:Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finetune\n",
    "模型微调训练方法:\n",
    "1. 固定预训练的参数(requires_grad =False;lr=0) \n",
    "2. Features Extractor较小学习率(params_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 法1 : 冻结卷积层\n",
    "flag_m1 = 0\n",
    "# flag_m1 = 1\n",
    "if flag_m1:\n",
    "    for param in resnet18_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"conv1.weights[0, 0, ...]:\\n {}\".format(resnet18_ft.conv1.weight[0, 0, ...]))\n",
    "\n",
    "# 法2 : conv 小学习率\n",
    "# flag = 0\n",
    "flag = 1\n",
    "if flag:\n",
    "    fc_params_id = list(map(id, resnet18_ft.fc.parameters()))     # 返回的是parameters的 内存地址\n",
    "    base_params = filter(lambda p: id(p) not in fc_params_id, resnet18_ft.parameters())\n",
    "    optimizer = optim.SGD([\n",
    "        {'params': base_params, 'lr': LR*0.1},   # 0\n",
    "        {'params': resnet18_ft.fc.parameters(), 'lr': LR}], momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.)\n",
    "a = torch.tensor(1., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "c = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = a**2*x + b*x + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.), tensor(1.), tensor(1.))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() \n",
    "a.grad, b.grad, c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.), tensor(1.), tensor(1.))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or \n",
    "autograd.grad(y, [a,b,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch api\n",
    "https://pytorch.org/docs/stable/torch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "https://pytorch.org/docs/stable/nn.html#utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clip_grad_value_\n",
    "https://zhuanlan.zhihu.com/p/99953668\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_\n",
    "\n",
    "梯度裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.utils.clip_grad_value_(parameters, clip_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code\n",
    "def clip_grad_value_(parameters, clip_value):\n",
    "    r\"\"\"Clips gradient of an iterable of parameters at specified value.\n",
    "    Gradients are modified in-place.\n",
    "\n",
    "    Arguments:\n",
    "        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n",
    "            single Tensor that will have gradients normalized\n",
    "        clip_value (float or int): maximum allowed value of the gradients.\n",
    "            The gradients are clipped in the range\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    clip_value = float(clip_value)\n",
    "    for p in filter(lambda p: p.grad is not None, parameters):\n",
    "        p.grad.data.clamp_(min=-clip_value, max=clip_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, features_in=1, features_out=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(features_in, features_out)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        nn.init.constant_(self.linear.weight, val=1)\n",
    "        nn.init.constant_(self.linear.bias, val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义\n",
    "net = LinearNet()\n",
    "mse_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "# 网络输入和标签\n",
    "x = torch.FloatTensor([120])\n",
    "target_value = torch.FloatTensor([2])\n",
    "# loss计算\n",
    "predict = net(x)\n",
    "loss = mse_fn(predict, target_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([120.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(118., grad_fn=<L1LossBackward>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad before clip:tensor([[120.]])\n",
      "grad after clip:tensor([[1.1000]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(\"grad before clip:\"+str(net.linear.weight.grad))\n",
    "nn.utils.clip_grad_value_(net.linear.weight, clip_value=1.1)\n",
    "print(\"grad after clip:\"+str(net.linear.weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pad_sequence\n",
    "https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad_sequence to convert variable length sequences to same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5,  18,  29,   0],\n",
       "        [ 32, 100,   0,   0],\n",
       "        [699,   6,   9,  17]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "x_seq = [\n",
    "    torch.tensor([5, 18, 29]), \n",
    "    torch.tensor([32, 100]), \n",
    "    torch.tensor([699, 6, 9, 17])\n",
    "]\n",
    "x_padded = pad_sequence(x_seq, batch_first=True, padding_value=0)\n",
    "x_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For batch processing, a typical pattern is to use this with Pytorch’s DataLoader and Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "## refer to pytorch tutorials on how to inherit from Dataset class\n",
    "dataset = Dataset(...)\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "def pad_collate(batch):\n",
    "  (xx, yy) = zip(*batch)\n",
    "  x_lens = [len(x) for x in xx]\n",
    "  y_lens = [len(y) for y in yy]\n",
    "\n",
    "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "  yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "\n",
    "  return xx_pad, yy_pad, x_lens, y_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert padded sequences to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "for (x_padded, y_padded, x_lens, y_lens) in enumerate(data_loader):\n",
    "  x_embed = embedding(x_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 5\n",
    "vob_size = 1000\n",
    "embedding = nn.Embedding(vob_size, embedding_dim)\n",
    "x_embed = embedding(x_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.5252,  0.8362,  1.1393,  0.3071, -0.3895],\n",
       "        [ 0.6688, -0.6191,  0.8454, -0.3163, -0.6356],\n",
       "        [-0.0812, -1.1087,  2.4557, -1.0208,  2.2698],\n",
       "        [-0.3954,  0.1666, -0.4051, -0.4502,  0.2692],\n",
       "        [-2.2914,  0.3807,  1.3536, -0.5610,  0.1074],\n",
       "        [-0.3659, -0.1681, -0.1752,  1.1853, -0.9145],\n",
       "        [-1.4882,  0.6698,  1.1393,  0.0200, -0.8313],\n",
       "        [-0.0207,  0.5845,  1.0133, -1.8247, -2.8377],\n",
       "        [-0.5514, -1.0893, -0.7097,  1.7510, -1.6252]],\n",
       "       grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([3, 3, 2, 1]), sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_packed = pack_padded_sequence(x_embed, [3, 2, 4], batch_first=True, enforce_sorted=False)\n",
    "x_packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim = 10\n",
    "n_layers = 1\n",
    "rnn = nn.GRU(embedding_dim, h_dim, n_layers, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.zeros((1, 3, 10))\n",
    "output_packed, hidden = rnn(x_packed, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.0563,  0.1575, -0.0926, -0.1861, -0.0318,  0.2794,  0.0795,  0.1051,\n",
       "          0.1107,  0.0018],\n",
       "        [-0.0420, -0.0289,  0.2141, -0.1463, -0.1701,  0.1181,  0.2335, -0.1500,\n",
       "         -0.1298,  0.0032],\n",
       "        [ 0.2677, -0.1951, -0.0147, -0.4717, -0.2592,  0.3746,  0.0140,  0.3341,\n",
       "          0.1634,  0.2654],\n",
       "        [ 0.1960, -0.1801, -0.2302, -0.1667, -0.1394,  0.3125,  0.1049,  0.1050,\n",
       "          0.0325, -0.0654],\n",
       "        [ 0.1856, -0.3599, -0.3315, -0.1492,  0.0558,  0.1694,  0.2388,  0.2492,\n",
       "          0.3313,  0.5142],\n",
       "        [ 0.0650, -0.2714, -0.2284, -0.1213, -0.3386,  0.2859,  0.1472,  0.0692,\n",
       "         -0.2625,  0.2844],\n",
       "        [ 0.2055, -0.2502, -0.4189, -0.0743,  0.0574,  0.3002,  0.1425,  0.2216,\n",
       "          0.1985,  0.3188],\n",
       "        [ 0.0353,  0.0804, -0.0954, -0.0091,  0.2367,  0.1685,  0.4627, -0.2842,\n",
       "          0.1381,  0.0260],\n",
       "        [-0.1437, -0.3091, -0.3145,  0.1768, -0.3265,  0.0643,  0.3470, -0.2125,\n",
       "         -0.4193,  0.3295]], grad_fn=<CatBackward>), batch_sizes=tensor([3, 3, 2, 1]), sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_packed_sequence on our packed RNN output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "output_padded, output_lengths = pad_packed_sequence(output_packed, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 10])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0420, -0.0289,  0.2141, -0.1463, -0.1701,  0.1181,  0.2335,\n",
       "          -0.1500, -0.1298,  0.0032],\n",
       "         [ 0.1856, -0.3599, -0.3315, -0.1492,  0.0558,  0.1694,  0.2388,\n",
       "           0.2492,  0.3313,  0.5142],\n",
       "         [ 0.0353,  0.0804, -0.0954, -0.0091,  0.2367,  0.1685,  0.4627,\n",
       "          -0.2842,  0.1381,  0.0260],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.2677, -0.1951, -0.0147, -0.4717, -0.2592,  0.3746,  0.0140,\n",
       "           0.3341,  0.1634,  0.2654],\n",
       "         [ 0.0650, -0.2714, -0.2284, -0.1213, -0.3386,  0.2859,  0.1472,\n",
       "           0.0692, -0.2625,  0.2844],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0563,  0.1575, -0.0926, -0.1861, -0.0318,  0.2794,  0.0795,\n",
       "           0.1051,  0.1107,  0.0018],\n",
       "         [ 0.1960, -0.1801, -0.2302, -0.1667, -0.1394,  0.3125,  0.1049,\n",
       "           0.1050,  0.0325, -0.0654],\n",
       "         [ 0.2055, -0.2502, -0.4189, -0.0743,  0.0574,  0.3002,  0.1425,\n",
       "           0.2216,  0.1985,  0.3188],\n",
       "         [-0.1437, -0.3091, -0.3145,  0.1768, -0.3265,  0.0643,  0.3470,\n",
       "          -0.2125, -0.4193,  0.3295]]], grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "386.85px",
    "left": "973px",
    "right": "20px",
    "top": "108px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
