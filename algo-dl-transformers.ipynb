{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp algo.dl.transformers\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algo-dl-transformers\n",
    "\n",
    "https://huggingface.co/\n",
    "\n",
    "https://github.com/huggingface/transformers\n",
    "\n",
    "doc: https://huggingface.co/transformers/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==3.0.2\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers -U\n",
    "!pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ê≥®: ÈªòËÆ§ÊåâÁÖßÁöÑtokenizers‰ºöÊä•Èîô\n",
    "    https://github.com/huggingface/tokenizers/issues/321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers==0.8.1\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install tokenizers==0.8.1\n",
    "!pip freeze | grep tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 10:46:43.570719 140736034558848 file_utils.py:39] PyTorch version 1.6.0 available.\n",
      "I0813 10:46:45.364997 140736034558848 file_utils.py:55] TensorFlow version 2.2.0 available.\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data-tokenizer\n",
    "https://huggingface.co/transformers/preprocessing.html\n",
    "\n",
    "In this tutorial, we‚Äôll explore how to preprocess your data using ü§ó Transformers. \n",
    "## tokenizer‰ΩøÁî®\n",
    "The main tool for this is what we call a tokenizer. You can build one using the tokenizer class associated to the model you would like to use, or directly with the AutoTokenizer class.\n",
    "\n",
    "As we saw in the quicktour, \n",
    "\n",
    "    the tokenizer will first split a given text in words (or part of words, punctuation symbols, etc.) usually called tokens. \n",
    "    Then it will convert those tokens into numbers, to be able to build a tensor out of them and feed them to the model. \n",
    "    It will also add any additional inputs the model might expect to work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 10:51:57.931643 140736034558848 filelock.py:274] Lock 5582576552 acquired on /Users/luoyonggui/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock\n",
      "I0813 10:51:57.933714 140736034558848 file_utils.py:748] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /Users/luoyonggui/.cache/torch/transformers/tmpm3cprjbs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105c65167dae4a72be7152469fb31132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=433, style=ProgressStyle(description_width=‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 10:51:59.628379 140736034558848 file_utils.py:752] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json in cache at /Users/luoyonggui/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "I0813 10:51:59.629653 140736034558848 file_utils.py:755] creating metadata file for /Users/luoyonggui/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "I0813 10:51:59.631561 140736034558848 filelock.py:318] Lock 5582576552 released on /Users/luoyonggui/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock\n",
      "I0813 10:51:59.632812 140736034558848 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/luoyonggui/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "I0813 10:51:59.634166 140736034558848 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 10:52:01.275471 140736034558848 filelock.py:274] Lock 5582576552 acquired on /Users/luoyonggui/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\n",
      "I0813 10:52:01.276754 140736034558848 file_utils.py:748] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /Users/luoyonggui/.cache/torch/transformers/tmpie9rxvue\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef11670ee08428dbd2196d348471d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=213450, style=ProgressStyle(description_wid‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 10:52:04.300662 140736034558848 file_utils.py:752] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt in cache at /Users/luoyonggui/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0813 10:52:04.302201 140736034558848 file_utils.py:755] creating metadata file for /Users/luoyonggui/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0813 10:52:04.303758 140736034558848 filelock.py:318] Lock 5582576552 released on /Users/luoyonggui/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\n",
      "I0813 10:52:04.304719 140736034558848 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/luoyonggui/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÁºñÁ†Å‰∏Ä‰∏™Âè•Â≠ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ëß£Á†Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Hello, I'm a single sentence! [SEP]\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÊâπÈáèÁºñÁ†ÅÂè•Â≠ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102], [101, 1262, 1330, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                   \"And another sentence\",\n",
    "                   \"And the very very last one\"]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad & truncate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the purpose of sending several sentences at a time to the tokenizer is to build a batch to feed the model, you will probably want:\n",
    "\n",
    "* To pad each sentence to the maximum length there is in your batch.\n",
    "\n",
    "* To truncate each sentence to the maximum length the model can accept (if applicable).\n",
    "\n",
    "* To return tensors.\n",
    "\n",
    "You can do all of this by using the following options when feeding your list of sentences to the tokenizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everything you always wanted to know about padding and truncation\n",
    "https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation\n",
    "\n",
    "* no truncation\n",
    "\n",
    "    * no padding:   \n",
    "    tokenizer(batch_sentences)\n",
    "    * padding to max sequence in batch:   \n",
    "    tokenizer(batch_sentences, padding=True) or tokenizer(batch_sentences, padding='longest')\n",
    "    * padding to max model input length   \n",
    "    tokenizer(batch_sentences, padding='max_length')\n",
    "\n",
    "    * padding to specific length:   \n",
    "    tokenizer(batch_sentences, padding='max_length', max_length=42)\n",
    "\n",
    "* truncation to max model input length\n",
    "\n",
    "    * no padding:   \n",
    "    tokenizer(batch_sentences, truncation=True) or tokenizer(batch_sentences, truncation=STRATEGY)\n",
    "    * padding to max sequence in batch:   \n",
    "    tokenizer(batch_sentences, padding=True, truncation=True) or tokenizer(batch_sentences, padding=True, truncation=STRATEGY)\n",
    "    * padding to max model input length:   \n",
    "    tokenizer(batch_sentences, padding='max_length', truncation=True) or tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY)\n",
    "\n",
    "    * padding to specific length:   \n",
    "    Not possible\n",
    "    \n",
    "* truncation to specific length\n",
    "\n",
    "    * no padding:   \n",
    "    tokenizer(batch_sentences, truncation=True, max_length=42) or tokenizer(batch_sentences, truncation=STRATEGY, max_length=42)\n",
    "    * padding to max sequence in batch:   \n",
    "    tokenizer(batch_sentences, padding=True, truncation=True, max_length=42) or tokenizer(batch_sentences, padding=True, truncation=STRATEGY, max_length=42)\n",
    "    * padding to max model input length:   \n",
    "    Not possible\n",
    "\n",
    "    * padding to specific length:   \n",
    "    tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=42) or tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY, max_length=42)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 8667,  146,  112,  182,  170, 1423, 5650,  102],\n",
      "        [ 101, 1262, 1330, 5650,  102,    0,    0,    0,    0],\n",
      "        [ 101, 1262, 1103, 1304, 1304, 1314, 1141,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Â§ÑÁêÜÂè•Â≠êÂØπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1731, 1385, 1132, 1128, 136, 102, 146, 112, 182, 127, 1201, 1385, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"How old are you?\", \"I'm 6 years old\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] How old are you? [SEP] I'm 6 years old [SEP]\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102, 146, 112, 182, 170, 5650, 1115, 2947, 1114, 1103, 1148, 5650, 102], [101, 1262, 1330, 5650, 102, 1262, 146, 1431, 1129, 12544, 1114, 1103, 1248, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102, 1262, 146, 1301, 1114, 1103, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                   \"And another sentence\",\n",
    "                   \"And the very very last one\"]\n",
    "batch_of_second_sentences = [\"I'm a sentence that goes with the first sentence\",\n",
    "                             \"And I should be encoded with the second sentence\",\n",
    "                             \"And I go with the very last one\"]\n",
    "encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Hello I'm a single sentence [SEP] I'm a sentence that goes with the first sentence [SEP]\n",
      "[CLS] And another sentence [SEP] And I should be encoded with the second sentence [SEP]\n",
      "[CLS] And the very very last one [SEP] And I go with the very last one [SEP]\n"
     ]
    }
   ],
   "source": [
    "for ids in encoded_inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  8667,   146,   112,   182,   170,  1423,  5650,   102,   146,\n",
       "           112,   182,   170,  5650,  1115,  2947,  1114,  1103,  1148,  5650,\n",
       "           102],\n",
       "        [  101,  1262,  1330,  5650,   102,  1262,   146,  1431,  1129, 12544,\n",
       "          1114,  1103,  1248,  5650,   102,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1262,  1103,  1304,  1304,  1314,  1141,   102,  1262,   146,\n",
       "          1301,  1114,  1103,  1304,  1314,  1141,   102,     0,     0,     0,\n",
       "             0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-tokenized inputs\n",
    "The tokenizer also accept pre-tokenized inputs. This is particularly useful when you want to compute labels and extract predictions in named entity recognition (NER) or part-of-speech tagging (POS tagging).\n",
    "\n",
    "If you want to use pre-tokenized inputs, just set is_pretokenized=True when passing your inputs to the tokenizer. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 146, 112, 182, 170, 1423, 5650, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer([\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"], is_pretokenized=True)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Hello I'm a single sentence [SEP]\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = [[\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"],\n",
    "                   [\"And\", \"another\", \"sentence\"],\n",
    "                   [\"And\", \"the\", \"very\", \"very\", \"last\", \"one\"]]\n",
    "encoded_inputs = tokenizer(batch_sentences, is_pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_of_second_sentences = [[\"I'm\", \"a\", \"sentence\", \"that\", \"goes\", \"with\", \"the\", \"first\", \"sentence\"],\n",
    "                             [\"And\", \"I\", \"should\", \"be\", \"encoded\", \"with\", \"the\", \"second\", \"sentence\"],\n",
    "                             [\"And\", \"I\", \"go\", \"with\", \"the\", \"very\", \"last\", \"one\"]]\n",
    "encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences, is_pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(batch_sentences,\n",
    "                  batch_of_second_sentences,\n",
    "                  is_pretokenized=True,\n",
    "                  padding=True,\n",
    "                  truncation=True,\n",
    "                  return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## three main different kinds of tokenizers used in Transformers: \n",
    "https://huggingface.co/transformers/tokenizer_summary.html\n",
    "\n",
    "* Byte-Pair Encoding (BPE), \n",
    "* WordPiece and \n",
    "* SentencePiece, \n",
    "\n",
    "\n",
    "### Introduction to tokenization\n",
    "Splitting a text in smaller chunks is a task that‚Äôs harder than it looks, and there are multiple ways of doing it. For instance, let‚Äôs look at the sentence ‚ÄúDon‚Äôt you love ü§ó Transformers? We sure do.‚Äù A first simple way of tokenizing this text is just to split it by spaces„ÄÇ\n",
    "\n",
    "word-levelÂàÜËØçÁöÑÁº∫ÁÇπÊòØÂÆπÊòìÈÄ†ÊàêvocabularyËøáÂ§ßÔºõ\n",
    "\n",
    "char-levelÂàÜËØçÁöÑÁº∫ÁÇπÊòØÂ§±ÂéªwordÁöÑËØ≠‰πâ‰ø°ÊÅØÔºõ\n",
    "\n",
    "‰∏ÄÁßçÊäò‰∏≠ÁöÑÂàÜËØçÊñπÊ≥ï: subword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword tokenization\n",
    "Subword tokenization algorithms rely on the principle that most common words should be left as is, but rare words should be decomposed in meaningful subword units. For instance ‚Äúannoyingly‚Äù might be considered a rare word and decomposed as ‚Äúannoying‚Äù and ‚Äúly‚Äù. This is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together some subwords.\n",
    "\n",
    "This allows the model to keep a reasonable vocabulary while still learning useful representations for common words or subwords. This also gives the ability to the model to process words it has never seen before, by decomposing them into subwords it knows. For instance, the base BertTokenizer will tokenize ‚ÄúI have a new GPU!‚Äù like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 11:29:43.441022 140736034558848 filelock.py:274] Lock 5615081120 acquired on /Users/luoyonggui/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "I0813 11:29:43.442437 140736034558848 file_utils.py:748] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /Users/luoyonggui/.cache/torch/transformers/tmppel_0_gq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c12158d1e404df4913d02d55167d36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 11:29:45.972033 140736034558848 file_utils.py:752] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /Users/luoyonggui/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0813 11:29:45.973664 140736034558848 file_utils.py:755] creating metadata file for /Users/luoyonggui/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0813 11:29:45.975466 140736034558848 filelock.py:318] Lock 5615081120 released on /Users/luoyonggui/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "I0813 11:29:45.976834 140736034558848 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/luoyonggui/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'have', 'a', 'new', 'gp', '##u', '!']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.tokenize(\"I have a new GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 11:31:18.174586 140736034558848 filelock.py:274] Lock 5613630800 acquired on /Users/luoyonggui/.cache/torch/transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8.lock\n",
      "I0813 11:31:18.175970 140736034558848 file_utils.py:748] https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model not found in cache or force_download set to True, downloading to /Users/luoyonggui/.cache/torch/transformers/tmp0eqzw_v5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaae35b0901492195d3b2b075f04785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=798011, style=ProgressStyle(description_wid‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0813 11:31:21.261960 140736034558848 file_utils.py:752] storing https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model in cache at /Users/luoyonggui/.cache/torch/transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n",
      "I0813 11:31:21.262964 140736034558848 file_utils.py:755] creating metadata file for /Users/luoyonggui/.cache/torch/transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n",
      "I0813 11:31:21.265434 140736034558848 filelock.py:318] Lock 5613630800 released on /Users/luoyonggui/.cache/torch/transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8.lock\n",
      "I0813 11:31:21.266402 140736034558848 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model from cache at /Users/luoyonggui/.cache/torch/transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['‚ñÅDon',\n",
       " \"'\",\n",
       " 't',\n",
       " '‚ñÅyou',\n",
       " '‚ñÅlove',\n",
       " '‚ñÅ',\n",
       " 'ü§ó',\n",
       " '‚ñÅ',\n",
       " 'Transform',\n",
       " 'ers',\n",
       " '?',\n",
       " '‚ñÅWe',\n",
       " '‚ñÅsure',\n",
       " '‚ñÅdo',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "tokenizer.tokenize(\"Don't you love ü§ó Transformers? We sure do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-level BPE\n",
    "To deal with the fact the base vocabulary needs to get all base characters, which can be quite big if one allows for all unicode characters, the GPT-2 paper introduces a clever trick, which is to use bytes as the base vocabulary (which gives a size of 256). With some additional rules to deal with punctuation, this manages to be able to tokenize every text without needing an unknown token. For instance, the GPT-2 model has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned with 50,000 merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece\n",
    "WordPiece is the subword tokenization algorithm used for BERT (as well as DistilBERT and Electra) and was outlined in this paper. It relies on the same base as BPE, which is to initialize the vocabulary to every character present in the corpus and progressively learn a given number of merge rules, the difference is that it doesn‚Äôt choose the pair that is the most frequent but the one that will maximize the likelihood on the corpus once merged.\n",
    "\n",
    "What does this mean? Well, in the previous example, it means we would only merge ‚Äòu‚Äô and ‚Äòg‚Äô if the probability of having ‚Äòug‚Äô divided by the probability of having ‚Äòu‚Äô then ‚Äòg‚Äô is greater than for any other pair of symbols. It‚Äôs subtly different from what BPE does in the sense that it evaluates what it ‚Äúloses‚Äù by merging two symbols and makes sure it‚Äôs worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram\n",
    "Unigram is a subword tokenization algorithm introduced in this paper. Instead of starting with a group of base symbols and learning merges with some rule, like BPE or WordPiece, it starts from a large vocabulary (for instance, all pretokenized words and the most common substrings) that it will trim down progressively. It‚Äôs not used directly for any of the pretrained models in the library, but it‚Äôs used in conjunction with SentencePiece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece\n",
    "All the methods we have been looking at so far required some from of pretrokenization, which has a central problem: not all languages use spaces to separate words. This is a problem XLM solves by using specific pretokenizers for each of those languages (in this case, Chinese, Japanese and Thai). \n",
    "\n",
    "To solve this problem, SentencePiece (introduced in this paper) treats the input as a raw stream, includes the space in the set of characters to use, then uses BPE or unigram to construct the appropriate vocabulary.\n",
    "\n",
    "That‚Äôs why in the example we saw before using XLNetTokenizer (which uses SentencePiece), we had some ‚Äò‚ñÅ‚Äô characters, that represent spaces. Decoding a tokenized text is then super easy: we just have to concatenate all of them together and replace those ‚Äò‚ñÅ‚Äô by spaces.\n",
    "\n",
    "All transformers models in the library that use SentencePiece use it with unigram. Examples of models using it are ALBERT, XLNet or the Marian framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer-api\n",
    "https://huggingface.co/transformers/main_classes/tokenizer.html\n",
    "\n",
    "A tokenizer is in charge of preparing the inputs for a model. \n",
    "\n",
    "The library comprise tokenizers for all the models. \n",
    "\n",
    "Most of the tokenizers are available in two flavors: \n",
    "* a full python implementation and \n",
    "* a ‚ÄúFast‚Äù implementation based on the Rust library tokenizers. \n",
    "\n",
    "The ‚ÄúFast‚Äù implementations allows \n",
    "* (1) a significant speed-up in particular when doing batched tokenization and \n",
    "* (2) additional methods to map between the original string (character and words) and the token space (e.g. getting the index of the token comprising a given character or the span of characters corresponding to a given token).   \n",
    "Currently no ‚ÄúFast‚Äù implementation is available for the SentencePiece-based tokenizers (for T5, ALBERT, CamemBERT, XLMRoBERTa and XLNet models).\n",
    "\n",
    "The base classes PreTrainedTokenizer and PreTrainedTokenizerFast implements the common methods for encoding string inputs in model inputs (see below) and instantiating/saving python and ‚ÄúFast‚Äù tokenizers either from a local file or directory or from a pretrained tokenizer provided by the library (downloaded from HuggingFace‚Äôs AWS S3 repository).\n",
    "\n",
    "PreTrainedTokenizer and PreTrainedTokenizerFast thus implements the main methods for using all the tokenizers:\n",
    "\n",
    "* tokenizing (spliting strings in sub-word token strings), converting tokens strings to ids and back, and encoding/decoding (i.e. tokenizing + convert to integers),\n",
    "\n",
    "* adding new tokens to the vocabulary in a way that is independant of the underlying structure (BPE, SentencePiece‚Ä¶),\n",
    "\n",
    "* managing special tokens like mask, beginning-of-sentence, etc tokens (adding them, assigning them to attributes in the tokenizer for easy access and making sure they are not split during tokenization)\n",
    "\n",
    "`BatchEncoding` holds the output of the tokenizer‚Äôs encoding methods (`__call__`, encode_plus and batch_encode_plus) and is derived from a Python dictionary. \n",
    "* When the tokenizer is a pure python tokenizer, this class behave just like a standard python dictionary and hold the various model inputs computed by these methodes (input_ids, attention_mask‚Ä¶). \n",
    "* When the tokenizer is a ‚ÄúFast‚Äù tokenizer (i.e. backed by HuggingFace tokenizers library), this class provides in addition several advanced alignement methods which can be used to map between the original string (character and words) and the token space (e.g. getting the index of the token comprising a given character or the span of characters corresponding to a given token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script('.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
