{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from tqdm import tqdm  \n",
    "import scipy.sparse as sp\n",
    "import pickle\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "class PickleWrapper(object):\n",
    "\n",
    "    @classmethod\n",
    "    def loadFromFile(cls, file, mode='rb'):\n",
    "        with open(file, mode) as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    @classmethod\n",
    "    def dump2File(cls, o, file, mode='wb'):\n",
    "        with open(file, mode) as f:\n",
    "            pickle.dump(o, f)\n",
    "def get_sim_item(df_, user_col, item_col, use_iif=False): \n",
    "\n",
    "    df = df_.copy()\n",
    "    user_item_ = df.groupby(user_col)[item_col].agg(list).reset_index()\n",
    "    user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))\n",
    "    \n",
    "    user_time_ = df.groupby(user_col)['time'].agg(list).reset_index() # 引入时间因素\n",
    "    user_time_dict = dict(zip(user_time_[user_col], user_time_['time']))\n",
    "    \n",
    "    sim_item = {}  \n",
    "    item_cnt = defaultdict(int)  # 商品被点击次数\n",
    "    for user, items in tqdm(user_item_dict.items()):  \n",
    "        for loc1, item in enumerate(items):  \n",
    "            item_cnt[item] += 1  \n",
    "            sim_item.setdefault(item, {})  \n",
    "            for loc2, relate_item in enumerate(items):  \n",
    "                if item == relate_item:  \n",
    "                    continue  \n",
    "                t1 = user_time_dict[user][loc1] # 点击时间提取\n",
    "                t2 = user_time_dict[user][loc2]\n",
    "                sim_item[item].setdefault(relate_item, 0)  \n",
    "                if not use_iif:  \n",
    "                    if loc1-loc2>0:\n",
    "                        sim_item[item][relate_item] += 1 * 0.7 * (0.8**(loc1-loc2-1)) * (1 - (t1 - t2) * 10000) / math.log(1 + len(items)) # 逆向\n",
    "                    else:\n",
    "                        sim_item[item][relate_item] += 1 * 1.0 * (0.8**(loc2-loc1-1)) * (1 - (t2 - t1) * 10000) / math.log(1 + len(items)) # 正向\n",
    "#                 else:  \n",
    "#                     sim_item[item][relate_item] += 1 / math.log(1 + len(items))  \n",
    "\n",
    "    sim_item_corr = sim_item.copy() # 引入AB的各种被点击次数  \n",
    "    for i, related_items in tqdm(sim_item.items()):  \n",
    "        for j, cij in related_items.items():  \n",
    "#             sim_item_corr[i][j] = cij / ((item_cnt[i] * item_cnt[j]) ** 0.2)  \n",
    "            sim_item_corr[i][j] = cij / (np.log(item_cnt[i]+1) * np.log(item_cnt[j]+1))\n",
    "    return sim_item_corr, user_item_dict  \n",
    "\n",
    "\n",
    "def recommend(sim_item_corr, user_item_dict, user_id, top_k, item_num):  \n",
    "    '''\n",
    "    input:item_sim_list, user_item, uid, 500, 50\n",
    "    # 用户历史序列中的所有商品均有关联商品,整合这些关联商品,进行相似性排序\n",
    "    '''\n",
    "    rank = {}  \n",
    "    interacted_items = user_item_dict[user_id] \n",
    "    interacted_items = interacted_items[::-1]\n",
    "    for loc, i in enumerate(interacted_items):  \n",
    "        for j, wij in sorted(sim_item_corr[i].items(), key=lambda d: d[1], reverse=True)[0:top_k]:  \n",
    "            if j not in interacted_items:  \n",
    "                rank.setdefault(j, 0)  \n",
    "                rank[j] += wij * (0.7**loc) \n",
    "\n",
    "    return sorted(rank.items(), key=lambda d: d[1], reverse=True)[:item_num]  \n",
    "\n",
    "# fill user to 50 items  \n",
    "def get_predict(df, pred_col, top_fill):  \n",
    "    top_fill = [int(t) for t in top_fill.split(',')]  \n",
    "    scores = [-1 * i for i in range(1, len(top_fill) + 1)]  \n",
    "    ids = list(df['user_id'].unique())  \n",
    "    fill_df = pd.DataFrame(ids * len(top_fill), columns=['user_id'])  \n",
    "    fill_df.sort_values('user_id', inplace=True)  \n",
    "    fill_df['item_id'] = top_fill * len(ids)  \n",
    "    fill_df[pred_col] = scores * len(ids)  \n",
    "    df = df.append(fill_df)  \n",
    "    df.sort_values(pred_col, ascending=False, inplace=True)  \n",
    "    df = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')  \n",
    "    df['rank'] = df.groupby('user_id')[pred_col].rank(method='first', ascending=False)  \n",
    "    df = df[df['rank'] <= 50]  \n",
    "    df = df.groupby('user_id')['item_id'].apply(lambda x: ','.join([str(i) for i in x])).str.split(',', expand=True).reset_index()  \n",
    "    return df  \n",
    "\n",
    "now_phase = 9\n",
    "train_path = '../data/underexpose_train'  \n",
    "test_path = '../data/underexpose_test'  \n",
    "\n",
    "\n",
    "whole_click = pd.DataFrame()  \n",
    "r = [] \n",
    "for c in range(7, now_phase + 1):  \n",
    "    recom_item = []  \n",
    "    print('phase:', c)  \n",
    "    click_train = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'], converters={'user_id':str, 'item_id':str})  \n",
    "    click_test = pd.read_csv(test_path + '/underexpose_test_click-{}/underexpose_test_click-{}.csv'.format(c,c), header=None,  names=['user_id', 'item_id', 'time'], converters={'user_id':str, 'item_id':str})  \n",
    "\n",
    "    all_click = click_train.append(click_test)  \n",
    "    whole_click = whole_click.append(all_click)  \n",
    "    whole_click = whole_click.drop_duplicates(subset=['user_id','item_id','time'],keep='last')\n",
    "    whole_click = whole_click.sort_values('time')\n",
    "\n",
    "    item_sim_list, user_item = get_sim_item(whole_click, 'user_id', 'item_id', use_iif=False)  \n",
    "\n",
    "    for i in tqdm(click_test['user_id'].unique()):  \n",
    "        rank_item = recommend(item_sim_list, user_item, i, 500, 500)  \n",
    "        rank = 1\n",
    "        for j in rank_item:  \n",
    "            \n",
    "            recom_item.append([i, j[0], j[1], rank])  \n",
    "            rank += 1\n",
    "            \n",
    "    dft = pd.DataFrame(recom_item)\n",
    "\n",
    "    dft.columns = 'user_id item_id_pred score rank'.split()\n",
    "    r.append(dft)\n",
    "    \n",
    "PickleWrapper.dump2File(r, '../user_data/tmp_data/r_list_itemcf_op_test_789.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def load_click_data_per_phase(now_phase):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    train_path = '../data/underexpose_train'\n",
    "    test_path = '../data/underexpose_test'\n",
    "\n",
    "     \n",
    "    all_click_df = []\n",
    "    for c in range(now_phase + 1):  \n",
    "        print(f'phase: {c}')  \n",
    "        cols_str = 'user_id item_id time'.split()\n",
    "        click_train1 = pd.read_csv(train_path + '/underexpose_train_click-{}.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'], converters={c: str for c in cols_str})  \n",
    "        click_test1 = pd.read_csv(test_path + '/underexpose_test_click-{}/underexpose_test_click-{}.csv'.format(c, c), header=None,  names=['user_id', 'item_id', 'time'], converters={c: str for c in cols_str}) \n",
    "        test_qtime1 = pd.read_csv(test_path + '/underexpose_test_click-{}/underexpose_test_qtime-{}.csv'.format(c, c), header=None,  names=['user_id','time'], converters={c: str for c in cols_str})  \n",
    "        click_test1_val = click_test1.sort_values(['user_id', 'time']).drop_duplicates(subset=['user_id'],keep='last')\n",
    "        \n",
    "        click_test1 = click_test1[~click_test1.index.isin(click_test1_val.index)]\n",
    "        all_click = click_train1.append(click_test1).drop_duplicates().sort_values('time')\n",
    "        \n",
    "        all_click_df.append((all_click, click_test1_val, test_qtime1))\n",
    "        print(f'all_click: {all_click.shape}, click_test1_val: {click_test1_val.shape}, test_qtime1: {test_qtime1.shape}')\n",
    "    return all_click_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_item_data():\n",
    "    train_item_df = pd.read_csv('../data/underexpose_train/underexpose_item_feat.csv', sep=r',\\s+|,\\[|\\],\\[',\n",
    "                                names=['item_id']+list(range(256)),\n",
    "                                converters={'item_id':str})\n",
    "    train_item_df.iloc[:, -1] = train_item_df.iloc[:, -1].str.replace(']', '').map(float)\n",
    "    return train_item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "            \n",
    "def build_co_occurance_matrix(items_list,\n",
    "                              window=9999,\n",
    "                              penalty1=False,\n",
    "                              penalty2=False,\n",
    "                              penalty3=1,\n",
    "                              penalty4=False,\n",
    "                              save_dir=None):\n",
    "    from multiprocessing.dummy import Pool, Lock\n",
    "    from collections import Counter\n",
    "\n",
    "    pool = Pool()\n",
    "    mutex = Lock()\n",
    "    item_list_flat = [ii for i in items_list for ii in i]\n",
    "    item_num_dict = dict(Counter(item_list_flat))  # 每个item出现次数的字典\n",
    "\n",
    "    items = pd.Series(list(item_num_dict.keys()))\n",
    "    item2id = pd.Series(items.index, items)\n",
    "\n",
    "    n_items = items.shape[0]\n",
    "    print(f'n_items: {n_items}')\n",
    "    train_data_matrix = sp.lil_matrix((n_items, n_items), dtype=np.float)\n",
    "    def t(items_):\n",
    "#     for items_ in tqdm(items_list):\n",
    "        for i, item in enumerate(items_):\n",
    "            for j, related_item in enumerate(items_):\n",
    "                distance = np.abs(i-j)\n",
    "                if (item != related_item) and (distance<window):\n",
    "                    vt = 1\n",
    "                    if penalty1:\n",
    "#                         print('对距离惩罚，距离越远，相关性越小...')\n",
    "                        vt /= np.sqrt(np.log2(distance+1))\n",
    "                    if penalty2:\n",
    "#                         print('对list长度惩罚，长度越长，对共现的价值越小...')\n",
    "                        vt /= np.log10(len(items_)+9)\n",
    "                    if i < j:\n",
    "                        vt *= penalty3\n",
    "                    mutex.acquire()\n",
    "                    train_data_matrix[item2id.loc[item], item2id.loc[related_item]] += vt\n",
    "                    mutex.release()\n",
    "    pool.map(t, items_list)\n",
    "    if penalty4:\n",
    "        print('对item出现次数做惩罚...')\n",
    "        def t(r):\n",
    "#         for r in tqdm(range(train_data_matrix.shape[0])):\n",
    "            for c in train_data_matrix.rows[r]:\n",
    "                mutex.acquire()\n",
    "                train_data_matrix[r,c] /= (np.log(item_num_dict[items[r]]+1)*np.log(item_num_dict[items[c]]+1))\n",
    "                mutex.release()\n",
    "        pool.map(t, range(train_data_matrix.shape[0]))\n",
    "    if save_dir:\n",
    "        if not os.path.exists(save_dir):\n",
    "            print(f'create matrix dir {save_dir}')\n",
    "            os.mkdir(save_dir)\n",
    "        items.to_pickle(os.path.join(save_dir, f'id2item_series_{penalty1}_{penalty2}_{penalty3}.pkl'))\n",
    "        item2id.to_pickle(os.path.join(save_dir, f'item2id_series_{penalty1}_{penalty2}_{penalty3}.pkl'))\n",
    "        sp.save_npz(os.path.join(save_dir, f'item_item_matrix_{penalty1}_{penalty2}_{penalty3}.npz'), train_data_matrix.tocsc())\n",
    "        print(f'save matrix to {save_dir}, finished')\n",
    "    return train_data_matrix, items, item2id            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def recall_from_cocur_matr(matr, topk, user_pred_list, user_items, item2id, id2item):\n",
    "    def t(u):\n",
    "#         items = user_items[u][-30:]\n",
    "        items = user_items[u]\n",
    "        len_items = len(items)\n",
    "#         print(len_items)\n",
    "        sr = pd.Series()\n",
    "        for i, item in enumerate(items):\n",
    "#             sr = sr.append(pd.Series(matr[item2id[item]].toarray()[0]).sort_values()[-100:] * (len_items+i))\n",
    "            sr = sr.append(pd.Series(matr[item2id[item]].toarray()[0]).sort_values()[-500:] * (len_items//2+1+i))\n",
    "        sr = sr.reset_index()\n",
    "        sr.columns = ['id', 'score']\n",
    "\n",
    "        sr = sr.groupby('id')['score'].sum().reset_index()\n",
    "        sr['item_id_pred'] = sr['id'].map(id2item)\n",
    "        \n",
    "        sr = sr[~sr.item_id_pred.isin(user_items[u])]\n",
    "        sr = sr.sort_values('score', ascending=False)[:topk]\n",
    "        sr['user_id'] = u\n",
    "        sr['rank'] = range(1, sr.shape[0]+1)\n",
    "#         rs.loc[u] = sr.sort_values('score', ascending=False)[:topk]['item_id score'.split()].values.tolist()\n",
    "#         return u, ['item_id score'.split()].values.tolist()\n",
    "        return sr['user_id item_id_pred score rank'.split()]\n",
    "    rs = pd.Series()\n",
    "    from multiprocessing.dummy import Pool\n",
    "    pool = Pool(16)\n",
    "    rs = pool.map(t, user_pred_list)\n",
    "    df = pd.DataFrame()\n",
    "    for d in rs:\n",
    "        df = df.append(d)\n",
    "#     rs = pd.Series(dict(rs))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def recall_from_bipartite_network(df,test_qtime, user_col, item_col):  \n",
    "    user_item_ = df.groupby(user_col)[item_col].agg(set).reset_index()  \n",
    "    user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))  \n",
    "    \n",
    "    item_user_ = df.groupby(item_col)[user_col].agg(set).reset_index()  \n",
    "    item_user_dict = dict(zip(item_user_[item_col], item_user_[user_col]))    \n",
    "\n",
    "    item_cnt = defaultdict(int)  \n",
    "    for user, items in tqdm(user_item_dict.items()):  \n",
    "        for i in items:  \n",
    "            item_cnt[i] += 1  \n",
    "\n",
    "    sim_item = {}\n",
    "\n",
    "    for item, users in tqdm(item_user_dict.items()):\n",
    "    \n",
    "        sim_item.setdefault(item, {}) \n",
    "    \n",
    "        for u in users:\n",
    "        \n",
    "            tmp_len = len(user_item_dict[u])\n",
    "        \n",
    "            for relate_item in user_item_dict[u]:\n",
    "                sim_item[item].setdefault(relate_item, 0)\n",
    "                sim_item[item][relate_item] += 1/ (math.log(len(users)+1) * math.log(tmp_len+1))\n",
    "\n",
    "    def recommend(sim_item_corr, user_item_dict, user_id, top_k):  \n",
    "        rank = {}  \n",
    "        interacted_items = user_item_dict[user_id]  \n",
    "        for i in interacted_items:  \n",
    "            for j, wij in sorted(sim_item_corr[i].items(), key=lambda d: d[1], reverse=True)[0:100]:  \n",
    "                if j not in interacted_items:  \n",
    "                    rank.setdefault(j, 0)  \n",
    "                    rank[j] += wij  \n",
    "        return sorted(rank.items(), key=lambda d: d[1], reverse=True)[:top_k]  \n",
    "\n",
    "    recom_item = []        \n",
    "    for i in tqdm(test_qtime['user_id'].unique()):  \n",
    "        rank_item = recommend(sim_item, user_item_dict , i, 500)  \n",
    "        rank = 1\n",
    "        for j in rank_item:  \n",
    "            recom_item.append([i, j[0], j[1], rank])  # user_id, item_id, sim\n",
    "            rank += 1\n",
    "    \n",
    "    dfr = pd.DataFrame(recom_item)\n",
    "\n",
    "    dfr.columns = 'user_id item_id_pred score rank'.split()\n",
    "    \n",
    "#     dfr['item_id_score'] = dfr.item_id.map(lambda x: [x]) + dfr.score.map(lambda x: [x])\n",
    "#     srr = dfr.sort_values('score', ascending=False).groupby('user_id')['item_id_score'].agg(list)\n",
    "            \n",
    "    return dfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def recall_from_word2vec(\n",
    "                        topk, user_pred_list, user_items, phase\n",
    "                        ,dim=88\n",
    "                        ,epochs=30\n",
    "                        ,learning_rate=0.5, mode='train'\n",
    "                        ):\n",
    "    \n",
    "    import psutil\n",
    "    import gensim\n",
    "    \n",
    "    list_data = user_items.tolist()\n",
    "#     print(len(list_data), list_data[:5])\n",
    "    fp = os.path.join(f'../user_data/model_data/wvmodel_{mode}_{phase}')\n",
    "    if os.path.exists(fp):\n",
    "        print(f'load w2v..._{mode}_{phase}')\n",
    "        model = gensim.models.Word2Vec.load(fp)\n",
    "    else:\n",
    "        print(f'train w2v..._{mode}_{phase}')\n",
    "        model = gensim.models.Word2Vec(\n",
    "                        list_data,\n",
    "                        size=dim,\n",
    "                        alpha=learning_rate,\n",
    "                        window=999999,\n",
    "                        min_count=1,\n",
    "                        workers=psutil.cpu_count(),\n",
    "                        compute_loss=True,\n",
    "                        iter=epochs,\n",
    "                        hs=0,\n",
    "                        sg=1,\n",
    "                        seed=42\n",
    "                    )\n",
    "        model.save(fp)\n",
    "    print('------- word2vec 召回 ---------')\n",
    "    def ft(u):  \n",
    "        items = user_items[u]\n",
    "        len_items = len(items)\n",
    "        sr = pd.Series()\n",
    "        for i, item in enumerate(items):\n",
    "            try:\n",
    "                sr = sr.append(pd.Series(dict(model.wv.most_similar(item, topn=500)))* (len_items//2+1+i))\n",
    "            except:\n",
    "                continue\n",
    "        sr = sr.reset_index()\n",
    "        sr.columns = ['item_id_pred', 'score']\n",
    "\n",
    "        sr = sr.groupby('item_id_pred')['score'].sum().reset_index()\n",
    "        \n",
    "        sr = sr[~sr.item_id_pred.isin(user_items[u])]\n",
    "        sr = sr.sort_values('score', ascending=False)[:topk]\n",
    "        sr['user_id'] = u\n",
    "        sr['rank'] = range(1, sr.shape[0]+1)\n",
    "#         rs.loc[u] = sr.sort_values('score', ascending=False)[:topk]['item_id score'.split()].values.tolist()\n",
    "#         return u, ['item_id score'.split()].values.tolist()\n",
    "#         print(sr.shape)\n",
    "        return sr['user_id item_id_pred score rank'.split()]\n",
    "    rs = pd.Series()\n",
    "    from multiprocessing.dummy import Pool\n",
    "    pool = Pool(16)\n",
    "    rs = pool.map(ft, user_pred_list)\n",
    "    df = pd.DataFrame()\n",
    "    for d in rs:\n",
    "        df = df.append(d)\n",
    "#     rs = pd.Series(dict(rs))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def t():\n",
    "    phases = [7,8,9]\n",
    "    r1 = []\n",
    "    r2 = []\n",
    "    r3 = []\n",
    "    mode = 'test'\n",
    "    window = 20\n",
    "    for phase in phases:\n",
    "        user_items = all_click_df[phase][0].append(all_click_df[phase][1]).groupby('user_id')['item_id'].agg(list)\n",
    "        df_ = all_click_df[phase][0].append(all_click_df[phase][1])\n",
    "\n",
    "        save_dir=None\n",
    "        print(f'build cocur_matr {phase}')\n",
    "        train_data_matrix, id2item, item2id = build_co_occurance_matrix(user_items.tolist(), window=window, \n",
    "                                                          penalty1=True, penalty2=True,penalty4=True,\n",
    "                                                          penalty3=0.9, save_dir=save_dir)\n",
    "        \n",
    "        print(f'recall_from_cocur_matr {mode} {phase}')\n",
    "        r1.append(recall_from_cocur_matr(train_data_matrix, 500, all_click_df[phase][2].user_id.tolist(), user_items, item2id, id2item))\n",
    "\n",
    "        print(f'recall_from_word2vec {mode} {phase}')\n",
    "        r2.append(recall_from_word2vec(\n",
    "                        500, all_click_df[phase][2].user_id.tolist(), user_items,phase\n",
    "                        ,dim=128\n",
    "                        ,epochs=60\n",
    "                        ,learning_rate=0.025, mode=mode\n",
    "                        ))\n",
    "        \n",
    "        print(f'recall_from_bipartite_network {mode} {phase}')\n",
    "        r3.append(recall_from_bipartite_network(df_,all_click_df[phase][2], 'user_id', 'item_id'))\n",
    "\n",
    "    PickleWrapper.dump2File(r1, '../user_data/tmp_data/r_list_itemcf_test_789.pkl')\n",
    "    PickleWrapper.dump2File(r2, '../user_data/tmp_data/r_list_w2v_test_789.pkl')\n",
    "    PickleWrapper.dump2File(r3, '../user_data/tmp_data/r_list_binn_test_789.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load match result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_match_items789():\n",
    "    r_itemcf = PickleWrapper.loadFromFile('../user_data/tmp_data/r_list_itemcf_test_789.pkl')\n",
    "\n",
    "    r_binn = PickleWrapper.loadFromFile('../user_data/tmp_data/r_list_binn_test_789.pkl')\n",
    "    \n",
    "    r_itemcf_yl = PickleWrapper.loadFromFile('../user_data/tmp_data/r_list_itemcf_op_test_789.pkl')\n",
    "    r_list_w2v = PickleWrapper.loadFromFile('../user_data/tmp_data/r_list_w2v_test_789.pkl')\n",
    "\n",
    "    return r_itemcf, r_binn, r_itemcf_yl, r_list_w2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def data_preporcess(recall_list, match_num, phase, mode='train'):\n",
    "    \n",
    "    itemcf, r_binn, r_itemcf_yl, r_list_w2v = recall_list\n",
    "    df1 = pd.DataFrame()\n",
    "    df2 = pd.DataFrame()\n",
    "    df3 = pd.DataFrame()\n",
    "    df4 = pd.DataFrame()\n",
    "    for i in range(len(phase)):\n",
    "        r_itemcf[i]['phase'] = phase[i]\n",
    "        r_binn[i]['phase'] = phase[i]\n",
    "        r_itemcf_yl[i]['phase'] = phase[i]\n",
    "        r_list_w2v[i]['phase'] = phase[i]\n",
    "        df1 = df1.append(r_itemcf[i])\n",
    "        df2 = df2.append(r_binn[i])\n",
    "        df3 = df3.append(r_itemcf_yl[i])\n",
    "        df4 = df4.append(r_list_w2v[i])\n",
    "\n",
    "    df1_ = df1[df1['rank']<(match_num+1)]\n",
    "    df2_ = df2[df2['rank']<(match_num+1)]\n",
    "    df3_ = df3[df3['rank']<(match_num+1)]\n",
    "    df4_ = df4[df4['rank']<(match_num+1)]\n",
    "\n",
    "    print('merge Multi-channel recall...')\n",
    "    cols = 'user_id item_id_pred phase'.split()\n",
    "    \n",
    "    if mode == 'train':\n",
    "        cols = 'user_id item_id_pred item_id_true phase'.split()\n",
    "    df = pd.merge(pd.merge(pd.merge(df1_, \n",
    "                                    df2_, on=cols, how='outer'), \n",
    "                                      df3_, on=cols, how='outer'), \n",
    "                                      df4_, on=cols, how='outer')\n",
    "\n",
    "    df = df.fillna(0)\n",
    "\n",
    "\n",
    "    dft = pd.DataFrame()\n",
    "    for p in tqdm(phase):\n",
    "        temp_ = all_click_df[p][0][all_click_df[p][0].user_id.isin(all_click_df[p][1].user_id)].groupby('user_id')['item_id'].agg(list)\n",
    "        temp_ = pd.DataFrame(temp_)\n",
    "        for i in range(0, 3):\n",
    "            temp_[f'last_{i+1}'] = temp_.item_id.str.get(-(i+1))\n",
    "        dft = dft.append(temp_.reset_index())\n",
    "        t_ = all_click_df[p][0].groupby('item_id')['user_id'].count()\n",
    "        df.loc[df.phase==p, 'item_cnt'] = df.item_id_pred.map(lambda x: t_[x] if x in t_ else 0)\n",
    "\n",
    "    df = pd.merge(df, dft.drop(columns='item_id'))\n",
    "    df.columns = ['user_id', 'item_id_pred', 'score_1', 'rank_1', 'phase', 'score_2',\n",
    "       'rank_2', 'score_3', 'rank_3', 'score_4', 'rank_4', 'item_cnt',\n",
    "       'last_1', 'last_2', 'last_3']\n",
    "    df['item_id_pred_text_vec'] = df.item_id_pred.map(lambda x: item_feat['text_vec'][x] if x in item_feat['text_vec'] else np.zeros(128))\n",
    "\n",
    "    df['item_id_pred_img_vec'] = df.item_id_pred.map(lambda x: item_feat['img_vec'][x] if x in item_feat['img_vec'] else np.arange(128))\n",
    "\n",
    "    for c in tqdm('last_1 last_2 last_3'.split()):\n",
    "        df[f'{c}_text_vec'] = df[c].map(lambda x: item_feat['text_vec'][x] if x in item_feat['text_vec'] else np.zeros(128))\n",
    "\n",
    "        df[f'{c}_img_vec'] = df[c].map(lambda x: item_feat['img_vec'][x] if x in item_feat['img_vec'] else np.arange(128))\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def fe(df):\n",
    "    for i in tqdm('rank score'.split()):\n",
    "        df[f'{i}1_sub_{i}2'] = df[f'{i}_1']-df[f'{i}_2']\n",
    "        df[f'{i}1_add_{i}2'] = df[f'{i}_1']+df[f'{i}_2']\n",
    "        df[f'{i}1_mul_{i}2'] = df[f'{i}_1']*df[f'{i}_2']\n",
    "\n",
    "        df[f'{i}1_sub_{i}3'] = df[f'{i}_1']-df[f'{i}_3']\n",
    "        df[f'{i}1_add_{i}3'] = df[f'{i}_1']+df[f'{i}_3']\n",
    "        df[f'{i}1_mul_{i}3'] = df[f'{i}_1']*df[f'{i}_3']\n",
    "\n",
    "        df[f'{i}1_sub_{i}4'] = df[f'{i}_1']-df[f'{i}_4']\n",
    "        df[f'{i}1_add_{i}4'] = df[f'{i}_1']+df[f'{i}_4']\n",
    "        df[f'{i}1_mul_{i}4'] = df[f'{i}_1']*df[f'{i}_4']\n",
    "\n",
    "        df[f'{i}2_sub_{i}3'] = df[f'{i}_2']-df[f'{i}_3']\n",
    "        df[f'{i}2_add_{i}3'] = df[f'{i}_2']+df[f'{i}_3']\n",
    "        df[f'{i}2_mul_{i}3'] = df[f'{i}_2']*df[f'{i}_3']\n",
    "\n",
    "        df[f'{i}2_sub_{i}4'] = df[f'{i}_2']-df[f'{i}_4']\n",
    "        df[f'{i}2_add_{i}4'] = df[f'{i}_2']+df[f'{i}_4']\n",
    "        df[f'{i}2_mul_{i}4'] = df[f'{i}_2']*df[f'{i}_4']\n",
    "\n",
    "        df[f'{i}3_sub_{i}4'] = df[f'{i}_3']-df[f'{i}_4']\n",
    "        df[f'{i}3_add_{i}4'] = df[f'{i}_3']+df[f'{i}_4']\n",
    "        df[f'{i}3_mul_{i}4'] = df[f'{i}_3']*df[f'{i}_4']\n",
    "        \n",
    "        df[f'{i}1_add_{i}2_add_{i}3'] = df[f'{i}_1']+df[f'{i}_2']+df[f'{i}_3']\n",
    "        df[f'{i}1_mul_{i}2_mul_{i}3'] = df[f'{i}_1']*df[f'{i}_2']*df[f'{i}_3']\n",
    "        \n",
    "        df[f'{i}1_add_{i}2_add_{i}4'] = df[f'{i}_1']+df[f'{i}_2']+df[f'{i}_4']\n",
    "        df[f'{i}1_mul_{i}2_mul_{i}4'] = df[f'{i}_1']*df[f'{i}_2']*df[f'{i}_4']\n",
    "        \n",
    "        df[f'{i}4_add_{i}2_add_{i}3'] = df[f'{i}_4']+df[f'{i}_2']+df[f'{i}_3']\n",
    "        df[f'{i}4_mul_{i}2_mul_{i}3'] = df[f'{i}_4']*df[f'{i}_2']*df[f'{i}_3']\n",
    "        \n",
    "        df[f'{i}1_add_{i}2_add_{i}3_add_{i}4'] = df[f'{i}_1']+df[f'{i}_2']+df[f'{i}_3']+df[f'{i}_4']\n",
    "        df[f'{i}1_mul_{i}2_mul_{i}3_mul_{i}4'] = df[f'{i}_1']*df[f'{i}_2']*df[f'{i}_3']*df[f'{i}_4']\n",
    "        \n",
    "\n",
    "    df['sim1_text'] = (df['item_id_pred_text_vec'] * df['last_1_text_vec']).map(sum)\n",
    "\n",
    "    df['sim1_img'] = (df['item_id_pred_img_vec'] * df['last_1_img_vec']).map(sum)\n",
    "\n",
    "    df['sim2_text'] = (df['item_id_pred_text_vec'] * df['last_2_text_vec']).map(sum)\n",
    "    df['sim2_img'] = (df['item_id_pred_img_vec'] * df['last_2_img_vec']).map(sum)\n",
    "\n",
    "    df['sim3_text'] = (df['item_id_pred_text_vec'] * df['last_3_text_vec']).map(sum)\n",
    "    df['sim3_img'] = (df['item_id_pred_img_vec'] * df['last_3_img_vec']).map(sum)\n",
    "    \n",
    "    df['sim1_text_img'] = df['sim1_text'] *  df['sim1_img']\n",
    "    df['sim2_text_img'] = df['sim2_text'] *  df['sim2_img']\n",
    "    df['sim3_text_img'] = df['sim3_text'] *  df['sim3_img']\n",
    "\n",
    "    df['sim12_text'] = df['sim1_text'] + df['sim2_text']\n",
    "    df['sim123_text'] = df['sim1_text'] + df['sim2_text'] + df['sim3_text']\n",
    "\n",
    "    df['sim12_img'] = df['sim1_img'] + df['sim2_img']\n",
    "    df['sim123_img'] = df['sim1_img'] + df['sim2_img'] + df['sim3_img']\n",
    "\n",
    "    df['sim12_text_img'] = df['sim1_text_img'] + df['sim2_text_img']\n",
    "    df['sim123_text_img'] = df['sim1_text_img'] + df['sim2_text_img'] + df['sim3_text_img']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_model_input(df, mode='train'):\n",
    "    from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "    cols = ['score_1', 'rank_1',\n",
    "       'score_2', 'rank_2', 'score_3', 'rank_3', 'score_4', 'rank_4',\n",
    "       'item_cnt', 'last_1', 'last_2', 'last_3', 'sim1_text', 'sim1_img',\n",
    "       'sim2_text', 'sim2_img', 'sim3_text', 'sim3_img', 'rank1_sub_rank2',\n",
    "       'rank1_add_rank2', 'rank1_mul_rank2', 'rank1_sub_rank3',\n",
    "       'rank1_add_rank3', 'rank1_mul_rank3', 'rank1_sub_rank4',\n",
    "       'rank1_add_rank4', 'rank1_mul_rank4', 'rank2_sub_rank3',\n",
    "       'rank2_add_rank3', 'rank2_mul_rank3', 'rank2_sub_rank4',\n",
    "       'rank2_add_rank4', 'rank2_mul_rank4', 'rank3_sub_rank4',\n",
    "       'rank3_add_rank4', 'rank3_mul_rank4', 'score1_sub_score2',\n",
    "       'score1_add_score2', 'score1_mul_score2', 'score1_sub_score3',\n",
    "       'score1_add_score3', 'score1_mul_score3', 'score1_sub_score4',\n",
    "       'score1_add_score4', 'score1_mul_score4', 'score2_sub_score3',\n",
    "       'score2_add_score3', 'score2_mul_score3', 'score2_sub_score4',\n",
    "       'score2_add_score4', 'score2_mul_score4', 'score3_sub_score4',\n",
    "       'score3_add_score4', 'score3_mul_score4', 'sim1_text_img',\n",
    "       'sim2_text_img', 'sim3_text_img', 'sim12_text', 'sim123_text',\n",
    "       'sim12_img', 'sim123_img', 'sim12_text_img', 'sim123_text_img',\n",
    "       'rank1_add_rank2_add_rank3', 'rank1_mul_rank2_mul_rank3',\n",
    "       'rank1_add_rank2_add_rank4', 'rank1_mul_rank2_mul_rank4',\n",
    "       'rank4_add_rank2_add_rank3', 'rank4_mul_rank2_mul_rank3',\n",
    "       'rank1_add_rank2_add_rank3_add_rank4',\n",
    "       'rank1_mul_rank2_mul_rank3_mul_rank4', 'score1_add_score2_add_score3',\n",
    "       'score1_mul_score2_mul_score3', 'score1_add_score2_add_score4',\n",
    "       'score1_mul_score2_mul_score4', 'score4_add_score2_add_score3',\n",
    "       'score4_mul_score2_mul_score3',\n",
    "       'score1_add_score2_add_score3_add_score4',\n",
    "       'score1_mul_score2_mul_score3_mul_score4']\n",
    "    df['fscore'] = 0\n",
    "    df['frank'] = 999\n",
    "    cat_cols = []\n",
    "    X_test = df[cols]\n",
    "    test_data = Pool(data=X_test, cat_features=cat_cols) \n",
    "    return test_data, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_model():\n",
    "    from catboost import Pool, CatBoostClassifier\n",
    "    params = {\n",
    "        'iterations': 20,\n",
    "        'learning_rate': 0.1,\n",
    "        'random_seed': 144,\n",
    "        'custom_metric': 'F1',\n",
    "        'loss_function': 'Logloss',\n",
    "        'class_weights': [1, 20],\n",
    "        }\n",
    "    print(params)\n",
    "    model = CatBoostClassifier(**params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 13:31:09.212 | INFO     | code.eda:load_click_data_per_phase:97 - phase: 0\n",
      "2020-06-14 13:31:10.614 | INFO     | code.eda:load_click_data_per_phase:108 - all_click: (261337, 3), click_test1_val: (1663, 3), test_qtime1: (1663, 2)\n",
      "2020-06-14 13:31:10.617 | INFO     | code.eda:load_click_data_per_phase:97 - phase: 1\n",
      "2020-06-14 13:31:11.898 | INFO     | code.eda:load_click_data_per_phase:108 - all_click: (264871, 3), click_test1_val: (1726, 3), test_qtime1: (1726, 2)\n",
      "2020-06-14 13:31:11.901 | INFO     | code.eda:load_click_data_per_phase:97 - phase: 2\n",
      "2020-06-14 13:31:13.105 | INFO     | code.eda:load_click_data_per_phase:108 - all_click: (264624, 3), click_test1_val: (1690, 3), test_qtime1: (1690, 2)\n",
      "2020-06-14 13:31:13.107 | INFO     | code.eda:load_click_data_per_phase:97 - phase: 3\n",
      "2020-06-14 13:31:14.674 | INFO     | code.eda:load_click_data_per_phase:108 - all_click: (286609, 3), click_test1_val: (1675, 3), test_qtime1: (1675, 2)\n",
      "2020-06-14 13:31:14.677 | INFO     | code.eda:load_click_data_per_phase:97 - phase: 4\n",
      "2020-06-14 13:31:16.042 | INFO     | code.eda:load_click_data_per_phase:108 - all_click: (291672, 3), click_test1_val: (1708, 3), test_qtime1: (1708, 2)\n",
      "2020-06-14 13:31:16.044 | INFO     | code.eda:load_click_data_per_phase:97 - phase: 5\n",
      "2020-06-14 13:31:17.568 | INFO     | code.eda:load_click_data_per_phase:108 - all_click: (313378, 3), click_test1_val: (1798, 3), test_qtime1: (1798, 2)\n",
      "2020-06-14 13:31:17.571 | INFO     | code.eda:load_click_data_per_phase:97 - phase: 6\n",
      "2020-06-14 13:31:19.174 | INFO     | code.eda:load_click_data_per_phase:108 - all_click: (337339, 3), click_test1_val: (1821, 3), test_qtime1: (1821, 2)\n",
      "2020-06-14 13:31:19.176 | INFO     | code.eda:load_click_data_per_phase:97 - phase: 7\n",
      "2020-06-14 13:31:20.614 | INFO     | code.eda:load_click_data_per_phase:108 - all_click: (299152, 3), click_test1_val: (1797, 3), test_qtime1: (1797, 2)\n",
      "2020-06-14 13:31:20.615 | INFO     | code.eda:load_click_data_per_phase:97 - phase: 8\n",
      "2020-06-14 13:31:22.038 | INFO     | code.eda:load_click_data_per_phase:108 - all_click: (292828, 3), click_test1_val: (1818, 3), test_qtime1: (1818, 2)\n",
      "2020-06-14 13:31:22.041 | INFO     | code.eda:load_click_data_per_phase:97 - phase: 9\n",
      "2020-06-14 13:31:23.252 | INFO     | code.eda:load_click_data_per_phase:108 - all_click: (281588, 3), click_test1_val: (1752, 3), test_qtime1: (1752, 2)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "if __name__ == '__main__':\n",
    "    match_num, phase = 500, [7,8,9]\n",
    "    all_click_df = load_click_data_per_phase(9)\n",
    "    t()\n",
    "    item_feat = get_item_data()\n",
    "    item_feat['text_vec'] = item_feat.iloc[:, 1:129].values.tolist()\n",
    "    item_feat['img_vec'] = item_feat.iloc[:, 129:257].values.tolist()\n",
    "\n",
    "    item_feat['text_vec'] = item_feat['text_vec'].map(np.array)\n",
    "\n",
    "    item_feat['img_vec'] = item_feat['img_vec'].map(np.array)\n",
    "\n",
    "    item_feat.set_index('item_id', inplace=True)\n",
    "    \n",
    "    r_itemcf, r_binn, r_itemcf_yl, r_list_w2v = load_match_items789()\n",
    "    \n",
    "    recall_list = [r_itemcf, r_binn, r_itemcf_yl, r_list_w2v]\n",
    "    df = data_preporcess(recall_list, match_num, phase, mode='predict')\n",
    "    dffp = fe(df)\n",
    "        \n",
    "    test_data, cat_cols = get_model_input(dffp, mode='predict')\n",
    "    \n",
    "\n",
    "    model = CatBoostClassifier()\n",
    "    model.load_model(os.path.join('../user_data/model_data/model.dump'))\n",
    "    dffp.loc[:, 'fscore'] = model.predict_proba(test_data)[:, 1]\n",
    "\n",
    "    dffp.loc[:, 'frank'] = dffp.groupby('user_id')['fscore'].rank(method='first', ascending=False)\n",
    "    df1 = dffp.sort_values(['user_id', 'fscore'], ascending=False)\n",
    "\n",
    "    sub = df1[df1.frank<51]\n",
    "\n",
    "    sub = sub.groupby('user_id')['item_id_pred'].agg(list)\n",
    "    sub1 = pd.DataFrame()\n",
    "\n",
    "    for i in range(50):\n",
    "        sub1[f'c{i}'] = sub.str.get(i)\n",
    "\n",
    "    sub1.applymap(int).to_csv('../prediction_result/result.csv', header=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
